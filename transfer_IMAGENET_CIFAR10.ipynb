{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1\n",
    "\n",
    "Use the feature extractor **output of pretrain model** to produce laten space, then concatenate with the original model\n",
    "Here we use the VGG pretrain model from CIFAR 10 and the target task is CIFAR10 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "with tf.device('/gpu:0'):\n",
    "    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "    c = tf.matmul(a, b)\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "print (session.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.losses import *\n",
    "from keras.callbacks import *\n",
    "from keras.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras. callbacks import *\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import *\n",
    "import time\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "def define_block(input_layer, nb_layers, nb_neurons, kernel_size=(3,3), batch_normalization=True, activation='relu', **kwargs):\n",
    "    index = None\n",
    "    if 'index' in kwargs:\n",
    "        index = kwargs['index']\n",
    "    \n",
    "    for i in range(nb_layers):\n",
    "        if i == 0:\n",
    "            b = input_layer\n",
    "            \n",
    "        b = Conv2D(nb_neurons, kernel_size=kernel_size, strides=1, padding='same', name='conv' + str(i) + '_block' + str(index))(b)\n",
    "        if batch_normalization:\n",
    "            b = BatchNormalization(name='batchnorm' + str(i) + '_block' + str(index))(b)\n",
    "        b =  Activation(activation, name=activation + str(i) + '_block' + str(index))(b)\n",
    "    b = MaxPooling2D(pool_size=(2,2), name='maxpooling_block' + str(index))(b)\n",
    "    return b\n",
    "\n",
    "def define_skipped_connection(src_layer, dst_layer):\n",
    "    src_shape = src_layer.output_shape\n",
    "    dst_shape = dst_layer.input_shape\n",
    "    \n",
    "\n",
    "def SkippedVGG(nb_blocks, input_shape, num_classes, nb_layers, nb_neurons, include_top=True, verbose=1, layer_mode=False):\n",
    "    concats = []\n",
    "    maxpooling_outputs = []\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        concats.append([])\n",
    "        \n",
    "    if type(nb_layers) is int:\n",
    "        temp = nb_layers\n",
    "        nb_layers = []\n",
    "        for i in range(nb_blocks):\n",
    "            nb_layers.append(temp)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Create DensedVGG model with ' + str(nb_blocks) + ' blocks, input shape = ' + str(input_shape))\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        if verbose:\n",
    "            print('Create block ' + str(i) + ':')\n",
    "        \n",
    "        if i == 0: # First block\n",
    "            b = Input(shape=input_shape)\n",
    "            inputs = b\n",
    "        \n",
    "        # check all layers before\n",
    "        if len(concats[i-1]) > 1:\n",
    "            if verbose:\n",
    "                print('Concatenate ', concats[i-1])\n",
    "            b = concatenate(concats[i-1])\n",
    "        elif len(concats[i-1]) == 1:\n",
    "            if verbose:\n",
    "                print('Get direct output from the previous block ', concats[i-1])\n",
    "            b = concats[i-1][0]\n",
    "            \n",
    "        # create main block\n",
    "        b = define_block(b, nb_layers[i], nb_neurons[i], index=i)\n",
    "        concats[i].append(b)\n",
    "        maxpooling_outputs.append(b)\n",
    "        \n",
    "        \n",
    "        # create cropping layer\n",
    "        for j in range(i+1, nb_blocks):\n",
    "            if verbose:\n",
    "                print('-- create skipped connection from block '+ str(i) + ' to block ' + str(j+1) + ' ...')\n",
    "                \n",
    "            src_shape = b.get_shape()\n",
    "            src_shape = (int(src_shape[1]), int(src_shape[2]), int(src_shape[3]))\n",
    "            dst_shape = (src_shape[0]//2**(j-i), src_shape[1]//2**(j-i), src_shape[2])\n",
    "            \n",
    "            print(src_shape, dst_shape)\n",
    "            h = src_shape[0] - dst_shape[0]\n",
    "            w = src_shape[1] - dst_shape[1]\n",
    "            if h % 2 == 0:\n",
    "                h_0 = h // 2\n",
    "                h_1 = h // 2\n",
    "            else:\n",
    "                h_0 = h // 2\n",
    "                h_1 = (h // 2) + 1\n",
    "            \n",
    "            if w % 2 == 0:\n",
    "                w_0 = w // 2\n",
    "                w_1 = w // 2\n",
    "            else:\n",
    "                w_0 = w // 2\n",
    "                w_1 = (w // 2) + 1\n",
    "                \n",
    "            concat_layer = Cropping2D(((h_0,h_1),(w_0,w_1)), name='cropping_block' + str(i) + '_block' + str(j+1))(b)\n",
    "            concats[j].append(concat_layer)\n",
    "            \n",
    "    # top model\n",
    "    if verbose:\n",
    "        print('Create top model')\n",
    "    \n",
    "    if len(concats[nb_blocks-1]) == 1:\n",
    "        outputs = concats[nb_blocks-1]\n",
    "    else:\n",
    "        outputs = concatenate(concats[nb_blocks-1])\n",
    "        \n",
    "    if layer_mode:\n",
    "        return outputs\n",
    "        \n",
    "    if include_top:\n",
    "        outputs = GlobalAveragePooling2D()(outputs)\n",
    "        outputs = Dense(num_classes)(outputs)\n",
    "        outputs = BatchNormalization()(outputs)\n",
    "        outputs = Activation('softmax')(outputs)\n",
    "        \n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    # compile\n",
    "#     model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "def SkippedVGG_layer(nb_blocks, input_layer, nb_layers, nb_neurons, verbose=1):\n",
    "    concats = []\n",
    "    for i in range(nb_blocks):\n",
    "        concats.append([])\n",
    "        \n",
    "    if type(nb_layers) is int:\n",
    "        temp = nb_layers\n",
    "        nb_layers = []\n",
    "        for i in range(nb_blocks):\n",
    "            nb_layers.append(temp)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Create DensedVGG model with ' + str(nb_blocks) + ' blocks')\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        if verbose:\n",
    "            print('Create block ' + str(i) + ':')\n",
    "        \n",
    "        if i == 0: # First block\n",
    "            b = input_layer\n",
    "            inputs = b\n",
    "        \n",
    "        # check all layers before\n",
    "        if len(concats[i-1]) > 1:\n",
    "            if verbose:\n",
    "                print('Concatenate ', concats[i-1])\n",
    "            b = concatenate(concats[i-1])\n",
    "        elif len(concats[i-1]) == 1:\n",
    "            if verbose:\n",
    "                print('Get direct output from the previous block ', concats[i-1])\n",
    "            b = concats[i-1][0]\n",
    "            \n",
    "        # create main block\n",
    "        b = define_block(b, nb_layers[i], nb_neurons[i], index=i)\n",
    "        concats[i].append(b)\n",
    "        \n",
    "        # create cropping layer\n",
    "        for j in range(i+1, nb_blocks):\n",
    "            if verbose:\n",
    "                print('-- create skipped connection from block '+ str(i) + ' to block ' + str(j+1) + ' ...')\n",
    "                \n",
    "            src_shape = b.get_shape()\n",
    "            src_shape = (int(src_shape[1]), int(src_shape[2]), int(src_shape[3]))\n",
    "            dst_shape = (src_shape[0]//2**(j-i), src_shape[1]//2**(j-i), src_shape[2])\n",
    "            \n",
    "            print(src_shape, dst_shape)\n",
    "            h = src_shape[0] - dst_shape[0]\n",
    "            w = src_shape[1] - dst_shape[1]\n",
    "            if h % 2 == 0:\n",
    "                h_0 = h // 2\n",
    "                h_1 = h // 2\n",
    "            else:\n",
    "                h_0 = h // 2\n",
    "                h_1 = (h // 2) + 1\n",
    "            \n",
    "            if w % 2 == 0:\n",
    "                w_0 = w // 2\n",
    "                w_1 = w // 2\n",
    "            else:\n",
    "                w_0 = w // 2\n",
    "                w_1 = (w // 2) + 1\n",
    "                \n",
    "            concat_layer = Cropping2D(((h_0,h_1),(w_0,w_1)), name='cropping_block' + str(i) + '_block' + str(j+1))(b)\n",
    "            concats[j].append(concat_layer)\n",
    "   \n",
    "    if len(concats[nb_blocks-1]) == 1:\n",
    "        outputs = concats[nb_blocks-1]\n",
    "    else:\n",
    "        outputs = concatenate(concats[nb_blocks-1])\n",
    "    \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DensedVGG model with 3 blocks\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_1/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Concatenate  [<tf.Tensor 'cropping_block0_block2_1/strided_slice:0' shape=(?, 8, 8, 64) dtype=float32>, <tf.Tensor 'maxpooling_block1_1/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  73856       maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 192)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    885248      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   multiple             14714688    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 512)          0           vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 704)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1216)         0           global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           12170       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 20,599,602\n",
      "Trainable params: 5,881,054\n",
      "Non-trainable params: 14,718,548\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input((32,32,3))\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "for layer in pretrain_vgg.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "vgg_out = pretrain_vgg(input_layer)\n",
    "\n",
    "skippedvgg_out = SkippedVGG_layer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512])\n",
    "\n",
    "vgg_out = GlobalAveragePooling2D()(vgg_out)\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "concat_layer = concatenate([vgg_out, skippedvgg_out])\n",
    "outputs = Dense(10)(concat_layer)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/data/Quan/datasets/cifar10/train_set.hdf5', 'rb') as dt:\n",
    "    train_set = pickle.load(dt)\n",
    "    \n",
    "with open('/data/Quan/datasets/cifar10/validation_set.hdf5', 'rb') as dt:\n",
    "    validation_set = pickle.load(dt)\n",
    "    \n",
    "with open('/data/Quan/datasets/cifar10/test_set.hdf5', 'rb') as dt:\n",
    "    test_set = pickle.load(dt)\n",
    "    \n",
    "train_images = train_set[0]\n",
    "train_labels = train_set[1]\n",
    "\n",
    "validation_images = validation_set[0]\n",
    "validation_labels = validation_set[1]\n",
    "    \n",
    "test_images = test_set[0]\n",
    "test_labels = test_set[1]\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_datagen = ImageDataGenerator(rotation_range=360,\n",
    "                                   rescale=1./255,\n",
    "                                   fill_mode='nearest')\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow(train_images, train_labels, batch_size = batch_size, shuffle=True)\n",
    "validation_generator = test_datagen.flow(validation_images, validation_labels, batch_size=batch_size, shuffle=False)\n",
    "test_generator = test_datagen.flow(test_images, test_labels, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0912 10:15:55.402318 140538123249408 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "546/546 [==============================] - 22s 41ms/step - loss: 1.5957 - acc: 0.4410 - val_loss: 2.0317 - val_acc: 0.3315\n",
      "Epoch 2/300\n",
      "546/546 [==============================] - 20s 37ms/step - loss: 1.3576 - acc: 0.5280 - val_loss: 2.3610 - val_acc: 0.2804\n",
      "Epoch 3/300\n",
      "546/546 [==============================] - 19s 35ms/step - loss: 1.2633 - acc: 0.5621 - val_loss: 1.4322 - val_acc: 0.4898\n",
      "Epoch 4/300\n",
      "546/546 [==============================] - 19s 35ms/step - loss: 1.1851 - acc: 0.5889 - val_loss: 1.1879 - val_acc: 0.5876\n",
      "Epoch 5/300\n",
      "546/546 [==============================] - 19s 35ms/step - loss: 1.1346 - acc: 0.6015 - val_loss: 1.4234 - val_acc: 0.4963\n",
      "Epoch 6/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0758 - acc: 0.6267 - val_loss: 1.2964 - val_acc: 0.5546\n",
      "Epoch 7/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0333 - acc: 0.6381 - val_loss: 1.3578 - val_acc: 0.5208\n",
      "Epoch 8/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.9948 - acc: 0.6514 - val_loss: 1.0507 - val_acc: 0.6302\n",
      "Epoch 9/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.9574 - acc: 0.6665 - val_loss: 1.1230 - val_acc: 0.6154\n",
      "Epoch 10/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.9185 - acc: 0.6797 - val_loss: 1.2824 - val_acc: 0.5434\n",
      "Epoch 11/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.8859 - acc: 0.6929 - val_loss: 1.3040 - val_acc: 0.5617\n",
      "Epoch 12/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.8583 - acc: 0.7023 - val_loss: 1.0618 - val_acc: 0.6296\n",
      "Epoch 13/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.8301 - acc: 0.7130 - val_loss: 0.9251 - val_acc: 0.6839\n",
      "Epoch 14/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.8051 - acc: 0.7183 - val_loss: 0.9243 - val_acc: 0.6827\n",
      "Epoch 15/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.7790 - acc: 0.7284 - val_loss: 1.1229 - val_acc: 0.6198\n",
      "Epoch 16/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.7544 - acc: 0.7381 - val_loss: 1.0457 - val_acc: 0.6553\n",
      "Epoch 17/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7294 - acc: 0.7465 - val_loss: 0.8655 - val_acc: 0.7108\n",
      "Epoch 18/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7155 - acc: 0.7513 - val_loss: 1.2225 - val_acc: 0.6103\n",
      "Epoch 19/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6881 - acc: 0.7598 - val_loss: 1.1346 - val_acc: 0.6319\n",
      "Epoch 20/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6741 - acc: 0.7645 - val_loss: 1.1431 - val_acc: 0.6317\n",
      "Epoch 21/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6527 - acc: 0.7744 - val_loss: 0.9684 - val_acc: 0.6833\n",
      "Epoch 22/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6385 - acc: 0.7777 - val_loss: 0.8456 - val_acc: 0.7146\n",
      "Epoch 23/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6232 - acc: 0.7832 - val_loss: 0.9570 - val_acc: 0.6778\n",
      "Epoch 24/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6049 - acc: 0.7899 - val_loss: 0.8804 - val_acc: 0.7076\n",
      "Epoch 25/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5877 - acc: 0.7982 - val_loss: 0.8129 - val_acc: 0.7264\n",
      "Epoch 26/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5799 - acc: 0.7979 - val_loss: 0.7677 - val_acc: 0.7395\n",
      "Epoch 27/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5519 - acc: 0.8074 - val_loss: 0.8396 - val_acc: 0.7211\n",
      "Epoch 28/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5484 - acc: 0.8095 - val_loss: 1.0136 - val_acc: 0.6782\n",
      "Epoch 29/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5318 - acc: 0.8154 - val_loss: 1.0127 - val_acc: 0.6795\n",
      "Epoch 30/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5101 - acc: 0.8220 - val_loss: 0.8390 - val_acc: 0.7274\n",
      "Epoch 31/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4987 - acc: 0.8274 - val_loss: 0.7179 - val_acc: 0.7553\n",
      "Epoch 32/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4888 - acc: 0.8317 - val_loss: 0.9281 - val_acc: 0.7016\n",
      "Epoch 33/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4751 - acc: 0.8337 - val_loss: 0.8831 - val_acc: 0.7226\n",
      "Epoch 34/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4511 - acc: 0.8448 - val_loss: 0.7888 - val_acc: 0.7371\n",
      "Epoch 35/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4410 - acc: 0.8466 - val_loss: 0.9209 - val_acc: 0.7045\n",
      "Epoch 36/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4284 - acc: 0.8516 - val_loss: 1.0547 - val_acc: 0.6800\n",
      "Epoch 37/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4167 - acc: 0.8565 - val_loss: 0.9389 - val_acc: 0.7177\n",
      "Epoch 38/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3995 - acc: 0.8623 - val_loss: 0.7943 - val_acc: 0.7444\n",
      "Epoch 39/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3979 - acc: 0.8629 - val_loss: 0.9182 - val_acc: 0.7173\n",
      "Epoch 40/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3781 - acc: 0.8678 - val_loss: 0.8843 - val_acc: 0.7311\n",
      "Epoch 41/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3712 - acc: 0.8711 - val_loss: 0.8604 - val_acc: 0.7361\n",
      "Epoch 42/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3622 - acc: 0.8745 - val_loss: 0.7805 - val_acc: 0.7544\n",
      "Epoch 43/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3443 - acc: 0.8826 - val_loss: 0.9221 - val_acc: 0.7313\n",
      "Epoch 44/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3455 - acc: 0.8796 - val_loss: 0.9648 - val_acc: 0.7206\n",
      "Epoch 45/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3185 - acc: 0.8903 - val_loss: 0.8780 - val_acc: 0.7334\n",
      "Epoch 46/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3123 - acc: 0.8926 - val_loss: 0.7516 - val_acc: 0.7699\n",
      "Epoch 47/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3024 - acc: 0.8953 - val_loss: 0.8289 - val_acc: 0.7623\n",
      "Epoch 48/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2937 - acc: 0.8993 - val_loss: 0.8147 - val_acc: 0.7582\n",
      "Epoch 49/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2881 - acc: 0.9001 - val_loss: 0.9662 - val_acc: 0.7250\n",
      "Epoch 50/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2730 - acc: 0.9051 - val_loss: 0.8059 - val_acc: 0.7600\n",
      "Epoch 51/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2670 - acc: 0.9087 - val_loss: 0.9215 - val_acc: 0.7428\n",
      "Epoch 52/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2577 - acc: 0.9114 - val_loss: 0.9046 - val_acc: 0.7365\n",
      "Epoch 53/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2514 - acc: 0.9132 - val_loss: 0.8896 - val_acc: 0.7438\n",
      "Epoch 54/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2418 - acc: 0.9171 - val_loss: 0.9340 - val_acc: 0.7442\n",
      "Epoch 55/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2364 - acc: 0.9199 - val_loss: 0.8353 - val_acc: 0.7586\n",
      "Epoch 56/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2261 - acc: 0.9237 - val_loss: 0.8860 - val_acc: 0.7477\n",
      "Epoch 57/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2237 - acc: 0.9236 - val_loss: 1.0391 - val_acc: 0.7142\n",
      "Epoch 58/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2116 - acc: 0.9283 - val_loss: 0.8701 - val_acc: 0.7546\n",
      "Epoch 59/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2055 - acc: 0.9308 - val_loss: 0.8452 - val_acc: 0.7565\n",
      "Epoch 60/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2043 - acc: 0.9303 - val_loss: 1.0592 - val_acc: 0.6999\n",
      "Epoch 61/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1996 - acc: 0.9334 - val_loss: 0.8374 - val_acc: 0.7640\n",
      "Epoch 62/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1884 - acc: 0.9354 - val_loss: 0.9605 - val_acc: 0.7398\n",
      "Epoch 63/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1853 - acc: 0.9370 - val_loss: 0.8707 - val_acc: 0.7578\n",
      "Epoch 64/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1773 - acc: 0.9402 - val_loss: 0.8815 - val_acc: 0.7525\n",
      "Epoch 65/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1725 - acc: 0.9426 - val_loss: 0.9729 - val_acc: 0.7351\n",
      "Epoch 66/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1734 - acc: 0.9414 - val_loss: 0.8705 - val_acc: 0.7611\n",
      "Epoch 67/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1637 - acc: 0.9446 - val_loss: 0.8120 - val_acc: 0.7725\n",
      "Epoch 68/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1607 - acc: 0.9452 - val_loss: 0.8860 - val_acc: 0.7624\n",
      "Epoch 69/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1532 - acc: 0.9493 - val_loss: 0.8435 - val_acc: 0.7695\n",
      "Epoch 70/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1549 - acc: 0.9485 - val_loss: 0.8789 - val_acc: 0.7627\n",
      "Epoch 71/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1545 - acc: 0.9476 - val_loss: 0.8592 - val_acc: 0.7658\n",
      "Epoch 72/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1462 - acc: 0.9512 - val_loss: 0.9885 - val_acc: 0.7477\n",
      "Epoch 73/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1415 - acc: 0.9530 - val_loss: 0.8852 - val_acc: 0.7685\n",
      "Epoch 74/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1343 - acc: 0.9550 - val_loss: 0.9868 - val_acc: 0.7550\n",
      "Epoch 75/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1330 - acc: 0.9553 - val_loss: 0.8992 - val_acc: 0.7657\n",
      "Epoch 76/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1305 - acc: 0.9567 - val_loss: 0.9363 - val_acc: 0.7564\n",
      "Epoch 77/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1277 - acc: 0.9579 - val_loss: 0.9506 - val_acc: 0.7580\n",
      "Epoch 78/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.1289 - acc: 0.9574 - val_loss: 0.9150 - val_acc: 0.7622\n",
      "Epoch 79/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1239 - acc: 0.9588 - val_loss: 1.0045 - val_acc: 0.7450\n",
      "Epoch 80/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1228 - acc: 0.9590 - val_loss: 0.9113 - val_acc: 0.7665\n",
      "Epoch 81/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1181 - acc: 0.9603 - val_loss: 1.0159 - val_acc: 0.7464\n",
      "Epoch 82/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1169 - acc: 0.9610 - val_loss: 0.9040 - val_acc: 0.7658\n",
      "Epoch 83/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1149 - acc: 0.9619 - val_loss: 0.9684 - val_acc: 0.7593\n",
      "Epoch 84/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1135 - acc: 0.9614 - val_loss: 1.0348 - val_acc: 0.7398\n",
      "Epoch 85/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1081 - acc: 0.9640 - val_loss: 0.9871 - val_acc: 0.7494\n",
      "Epoch 86/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1065 - acc: 0.9650 - val_loss: 0.8387 - val_acc: 0.7837\n",
      "Epoch 87/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1079 - acc: 0.9642 - val_loss: 1.0870 - val_acc: 0.7463\n",
      "Epoch 88/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0995 - acc: 0.9667 - val_loss: 0.9044 - val_acc: 0.7719\n",
      "Epoch 89/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1069 - acc: 0.9649 - val_loss: 0.9259 - val_acc: 0.7724\n",
      "Epoch 90/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0975 - acc: 0.9682 - val_loss: 0.9654 - val_acc: 0.7587\n",
      "Epoch 91/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0975 - acc: 0.9676 - val_loss: 0.9389 - val_acc: 0.7692\n",
      "Epoch 92/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0923 - acc: 0.9699 - val_loss: 0.9568 - val_acc: 0.7651\n",
      "Epoch 93/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0977 - acc: 0.9677 - val_loss: 0.9155 - val_acc: 0.7667\n",
      "Epoch 94/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0926 - acc: 0.9702 - val_loss: 0.9390 - val_acc: 0.7696\n",
      "Epoch 95/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0893 - acc: 0.9707 - val_loss: 0.9331 - val_acc: 0.7734\n",
      "Epoch 96/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0942 - acc: 0.9699 - val_loss: 1.1063 - val_acc: 0.7405\n",
      "Epoch 97/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0869 - acc: 0.9711 - val_loss: 0.9341 - val_acc: 0.7725\n",
      "Epoch 98/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0880 - acc: 0.9711 - val_loss: 0.8972 - val_acc: 0.7779\n",
      "Epoch 99/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0850 - acc: 0.9718 - val_loss: 1.0523 - val_acc: 0.7535\n",
      "Epoch 100/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0833 - acc: 0.9728 - val_loss: 1.0364 - val_acc: 0.7631\n",
      "Epoch 101/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0813 - acc: 0.9728 - val_loss: 1.0737 - val_acc: 0.7375\n",
      "Epoch 102/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0798 - acc: 0.9734 - val_loss: 0.9379 - val_acc: 0.7781\n",
      "Epoch 103/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0817 - acc: 0.9729 - val_loss: 1.1296 - val_acc: 0.7469\n",
      "Epoch 104/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0803 - acc: 0.9740 - val_loss: 1.0653 - val_acc: 0.7558\n",
      "Epoch 105/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0784 - acc: 0.9737 - val_loss: 1.0139 - val_acc: 0.7554\n",
      "Epoch 106/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0777 - acc: 0.9737 - val_loss: 1.0822 - val_acc: 0.7577\n",
      "Epoch 107/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0769 - acc: 0.9733 - val_loss: 1.0881 - val_acc: 0.7532\n",
      "Epoch 108/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0728 - acc: 0.9761 - val_loss: 0.9729 - val_acc: 0.7669\n",
      "Epoch 109/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0688 - acc: 0.9783 - val_loss: 1.0573 - val_acc: 0.7582\n",
      "Epoch 110/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0749 - acc: 0.9752 - val_loss: 1.0272 - val_acc: 0.7602\n",
      "Epoch 111/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0757 - acc: 0.9746 - val_loss: 1.0419 - val_acc: 0.7670\n",
      "Epoch 112/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0660 - acc: 0.9787 - val_loss: 1.1548 - val_acc: 0.7378\n",
      "Epoch 113/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0722 - acc: 0.9765 - val_loss: 1.1461 - val_acc: 0.7471\n",
      "Epoch 114/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0711 - acc: 0.9765 - val_loss: 0.9296 - val_acc: 0.7775\n",
      "Epoch 115/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0686 - acc: 0.9780 - val_loss: 0.9755 - val_acc: 0.7734\n",
      "Epoch 116/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0636 - acc: 0.9796 - val_loss: 1.1245 - val_acc: 0.7527\n",
      "Epoch 117/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0673 - acc: 0.9780 - val_loss: 1.1625 - val_acc: 0.7424\n",
      "Epoch 118/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0689 - acc: 0.9780 - val_loss: 0.9790 - val_acc: 0.7817\n",
      "Epoch 119/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0654 - acc: 0.9780 - val_loss: 1.0667 - val_acc: 0.7613\n",
      "Epoch 120/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0618 - acc: 0.9798 - val_loss: 1.0684 - val_acc: 0.7671\n",
      "Epoch 121/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0655 - acc: 0.9782 - val_loss: 0.9657 - val_acc: 0.7770\n",
      "Epoch 122/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0642 - acc: 0.9792 - val_loss: 0.9481 - val_acc: 0.7871\n",
      "Epoch 123/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0619 - acc: 0.9798 - val_loss: 1.0182 - val_acc: 0.7651\n",
      "Epoch 124/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0662 - acc: 0.9781 - val_loss: 1.1223 - val_acc: 0.7611\n",
      "Epoch 125/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0637 - acc: 0.9795 - val_loss: 0.9269 - val_acc: 0.7755\n",
      "Epoch 126/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0578 - acc: 0.9812 - val_loss: 1.0164 - val_acc: 0.7732\n",
      "Epoch 127/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0598 - acc: 0.9801 - val_loss: 1.0408 - val_acc: 0.7649\n",
      "Epoch 128/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0603 - acc: 0.9805 - val_loss: 1.0459 - val_acc: 0.7716\n",
      "Epoch 129/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0596 - acc: 0.9800 - val_loss: 1.0655 - val_acc: 0.7641\n",
      "Epoch 130/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0591 - acc: 0.9814 - val_loss: 1.0387 - val_acc: 0.7744\n",
      "Epoch 131/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0623 - acc: 0.9786 - val_loss: 1.1838 - val_acc: 0.7389\n",
      "Epoch 132/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0572 - acc: 0.9820 - val_loss: 1.0793 - val_acc: 0.7671\n",
      "Epoch 133/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0524 - acc: 0.9835 - val_loss: 1.0495 - val_acc: 0.7714\n",
      "Epoch 134/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0560 - acc: 0.9820 - val_loss: 0.9369 - val_acc: 0.7843\n",
      "Epoch 135/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0543 - acc: 0.9830 - val_loss: 1.0985 - val_acc: 0.7608\n",
      "Epoch 136/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0551 - acc: 0.9811 - val_loss: 1.0606 - val_acc: 0.7722\n",
      "Epoch 137/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0532 - acc: 0.9821 - val_loss: 1.0107 - val_acc: 0.7789\n",
      "Epoch 138/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0530 - acc: 0.9817 - val_loss: 1.0267 - val_acc: 0.7753\n",
      "Epoch 139/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0522 - acc: 0.9824 - val_loss: 1.0472 - val_acc: 0.7749\n",
      "Epoch 140/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0532 - acc: 0.9819 - val_loss: 1.0624 - val_acc: 0.7651\n",
      "Epoch 141/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0498 - acc: 0.9834 - val_loss: 1.0489 - val_acc: 0.7752\n",
      "Epoch 142/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0542 - acc: 0.9822 - val_loss: 1.0090 - val_acc: 0.7819\n",
      "Epoch 143/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0508 - acc: 0.9833 - val_loss: 1.1046 - val_acc: 0.7725\n",
      "Epoch 144/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0454 - acc: 0.9851 - val_loss: 1.2093 - val_acc: 0.7487\n",
      "Epoch 145/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0485 - acc: 0.9843 - val_loss: 1.0291 - val_acc: 0.7752\n",
      "Epoch 146/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0508 - acc: 0.9837 - val_loss: 1.0609 - val_acc: 0.7686\n",
      "Epoch 147/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0498 - acc: 0.9834 - val_loss: 1.0727 - val_acc: 0.7764\n",
      "Epoch 148/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0482 - acc: 0.9842 - val_loss: 1.1250 - val_acc: 0.7596\n",
      "Epoch 149/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0473 - acc: 0.9852 - val_loss: 1.1072 - val_acc: 0.7704\n",
      "Epoch 150/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0484 - acc: 0.9842 - val_loss: 1.0264 - val_acc: 0.7784\n",
      "Epoch 151/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0464 - acc: 0.9851 - val_loss: 1.0536 - val_acc: 0.7665\n",
      "Epoch 152/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0494 - acc: 0.9832 - val_loss: 1.0635 - val_acc: 0.7654\n",
      "Epoch 153/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0492 - acc: 0.9840 - val_loss: 1.0598 - val_acc: 0.7776\n",
      "Epoch 154/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0447 - acc: 0.9858 - val_loss: 1.0418 - val_acc: 0.7797\n",
      "Epoch 155/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0436 - acc: 0.9862 - val_loss: 1.0031 - val_acc: 0.7822\n",
      "Epoch 156/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0465 - acc: 0.9851 - val_loss: 1.1153 - val_acc: 0.7634\n",
      "Epoch 157/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0485 - acc: 0.9844 - val_loss: 1.0605 - val_acc: 0.7793\n",
      "Epoch 158/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0459 - acc: 0.9849 - val_loss: 1.0215 - val_acc: 0.7737\n",
      "Epoch 159/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0429 - acc: 0.9865 - val_loss: 1.0186 - val_acc: 0.7868\n",
      "Epoch 160/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0445 - acc: 0.9858 - val_loss: 1.0103 - val_acc: 0.7849\n",
      "Epoch 161/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0442 - acc: 0.9855 - val_loss: 1.0567 - val_acc: 0.7730\n",
      "Epoch 162/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0453 - acc: 0.9856 - val_loss: 1.0265 - val_acc: 0.7801\n",
      "Epoch 163/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0458 - acc: 0.9851 - val_loss: 1.0861 - val_acc: 0.7667\n",
      "Epoch 164/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0427 - acc: 0.9868 - val_loss: 0.9594 - val_acc: 0.7921\n",
      "Epoch 165/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0431 - acc: 0.9858 - val_loss: 1.1128 - val_acc: 0.7694\n",
      "Epoch 166/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0416 - acc: 0.9870 - val_loss: 1.0238 - val_acc: 0.7807\n",
      "Epoch 167/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0431 - acc: 0.9853 - val_loss: 1.0552 - val_acc: 0.7675\n",
      "Epoch 168/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0407 - acc: 0.9871 - val_loss: 0.9974 - val_acc: 0.7931\n",
      "Epoch 169/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0445 - acc: 0.9854 - val_loss: 1.1100 - val_acc: 0.7653\n",
      "Epoch 170/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0395 - acc: 0.9870 - val_loss: 1.1195 - val_acc: 0.7673\n",
      "Epoch 171/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0377 - acc: 0.9880 - val_loss: 1.2678 - val_acc: 0.7501\n",
      "Epoch 172/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0408 - acc: 0.9868 - val_loss: 1.0028 - val_acc: 0.7862\n",
      "Epoch 173/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0419 - acc: 0.9862 - val_loss: 1.2847 - val_acc: 0.7452\n",
      "Epoch 174/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0342 - acc: 0.9890 - val_loss: 1.1264 - val_acc: 0.7735\n",
      "Epoch 175/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0383 - acc: 0.9871 - val_loss: 1.0521 - val_acc: 0.7828\n",
      "Epoch 176/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0438 - acc: 0.9853 - val_loss: 1.1402 - val_acc: 0.7708\n",
      "Epoch 177/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0376 - acc: 0.9881 - val_loss: 0.9868 - val_acc: 0.7900\n",
      "Epoch 178/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0409 - acc: 0.9865 - val_loss: 1.0779 - val_acc: 0.7776\n",
      "Epoch 179/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0370 - acc: 0.9879 - val_loss: 1.0205 - val_acc: 0.7841\n",
      "Epoch 180/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0365 - acc: 0.9879 - val_loss: 1.0973 - val_acc: 0.7712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 181/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0367 - acc: 0.9888 - val_loss: 1.0955 - val_acc: 0.7786\n",
      "Epoch 182/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0391 - acc: 0.9877 - val_loss: 1.0751 - val_acc: 0.7800\n",
      "Epoch 183/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0370 - acc: 0.9877 - val_loss: 1.1315 - val_acc: 0.7696\n",
      "Epoch 184/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0368 - acc: 0.9877 - val_loss: 1.0547 - val_acc: 0.7840\n",
      "Epoch 185/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0348 - acc: 0.9889 - val_loss: 1.1652 - val_acc: 0.7680\n",
      "Epoch 186/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0390 - acc: 0.9868 - val_loss: 1.0166 - val_acc: 0.7911\n",
      "Epoch 187/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0319 - acc: 0.9898 - val_loss: 1.0106 - val_acc: 0.7882\n",
      "Epoch 188/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0371 - acc: 0.9879 - val_loss: 1.0717 - val_acc: 0.7797\n",
      "Epoch 189/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0378 - acc: 0.9877 - val_loss: 0.9870 - val_acc: 0.7876\n",
      "Epoch 190/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0320 - acc: 0.9899 - val_loss: 1.0558 - val_acc: 0.7904\n",
      "Epoch 191/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0393 - acc: 0.9866 - val_loss: 1.1326 - val_acc: 0.7702\n",
      "Epoch 192/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0340 - acc: 0.9887 - val_loss: 1.1461 - val_acc: 0.7739\n",
      "Epoch 193/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0347 - acc: 0.9882 - val_loss: 1.0535 - val_acc: 0.7907\n",
      "Epoch 194/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0331 - acc: 0.9895 - val_loss: 1.0953 - val_acc: 0.7756\n",
      "Epoch 195/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0315 - acc: 0.9896 - val_loss: 1.1131 - val_acc: 0.7762\n",
      "Epoch 196/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0371 - acc: 0.9875 - val_loss: 1.0351 - val_acc: 0.7791\n",
      "Epoch 197/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0325 - acc: 0.9894 - val_loss: 1.1192 - val_acc: 0.7768\n",
      "Epoch 198/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0370 - acc: 0.9875 - val_loss: 1.0933 - val_acc: 0.7775\n",
      "Epoch 199/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0328 - acc: 0.9892 - val_loss: 1.0845 - val_acc: 0.7831\n",
      "Epoch 200/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0353 - acc: 0.9884 - val_loss: 1.0864 - val_acc: 0.7742\n",
      "Epoch 201/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0346 - acc: 0.9881 - val_loss: 1.0954 - val_acc: 0.7799\n",
      "Epoch 202/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0313 - acc: 0.9894 - val_loss: 1.0484 - val_acc: 0.7852\n",
      "Epoch 208/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0347 - acc: 0.9886 - val_loss: 1.1284 - val_acc: 0.7752\n",
      "Epoch 209/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0335 - acc: 0.9887 - val_loss: 1.0443 - val_acc: 0.7872\n",
      "Epoch 210/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0286 - acc: 0.9912 - val_loss: 1.0984 - val_acc: 0.7839\n",
      "Epoch 211/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0330 - acc: 0.9890 - val_loss: 1.1474 - val_acc: 0.7785\n",
      "Epoch 212/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0303 - acc: 0.9901 - val_loss: 1.0783 - val_acc: 0.7847\n",
      "Epoch 213/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0315 - acc: 0.9898 - val_loss: 1.1510 - val_acc: 0.7715\n",
      "Epoch 214/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0319 - acc: 0.9895 - val_loss: 1.1432 - val_acc: 0.7821\n",
      "Epoch 215/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0292 - acc: 0.9904 - val_loss: 1.1024 - val_acc: 0.7818\n",
      "Epoch 216/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0304 - acc: 0.9895 - val_loss: 1.1158 - val_acc: 0.7740\n",
      "Epoch 217/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0334 - acc: 0.9889 - val_loss: 1.2010 - val_acc: 0.7651\n",
      "Epoch 218/300\n",
      "391/546 [====================>.........] - ETA: 4s - loss: 0.0305 - acc: 0.9898"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0253 - acc: 0.9920 - val_loss: 1.1827 - val_acc: 0.7680\n",
      "Epoch 242/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0257 - acc: 0.9921 - val_loss: 1.1088 - val_acc: 0.7851\n",
      "Epoch 243/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0250 - acc: 0.9912 - val_loss: 1.1495 - val_acc: 0.7742\n",
      "Epoch 244/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0266 - acc: 0.9920 - val_loss: 1.2114 - val_acc: 0.7713\n",
      "Epoch 245/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0235 - acc: 0.9924 - val_loss: 1.2157 - val_acc: 0.7731\n",
      "Epoch 246/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0292 - acc: 0.9906 - val_loss: 1.1371 - val_acc: 0.7756\n",
      "Epoch 247/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0273 - acc: 0.9910 - val_loss: 1.2345 - val_acc: 0.7566\n",
      "Epoch 248/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0294 - acc: 0.9898 - val_loss: 1.2023 - val_acc: 0.7702\n",
      "Epoch 249/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0250 - acc: 0.9916 - val_loss: 1.1293 - val_acc: 0.7847\n",
      "Epoch 250/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0269 - acc: 0.9902 - val_loss: 1.1287 - val_acc: 0.7760\n",
      "Epoch 251/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0245 - acc: 0.9917 - val_loss: 1.1096 - val_acc: 0.7818\n",
      "Epoch 252/300\n",
      "425/546 [======================>.......] - ETA: 3s - loss: 0.0247 - acc: 0.9922"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0219 - acc: 0.9931 - val_loss: 1.1155 - val_acc: 0.7879\n",
      "Epoch 276/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0217 - acc: 0.9934 - val_loss: 1.1498 - val_acc: 0.7841\n",
      "Epoch 277/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0218 - acc: 0.9930 - val_loss: 1.1779 - val_acc: 0.7807\n",
      "Epoch 278/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0241 - acc: 0.9921 - val_loss: 1.1920 - val_acc: 0.7786\n",
      "Epoch 279/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0215 - acc: 0.9932 - val_loss: 1.1832 - val_acc: 0.7743\n",
      "Epoch 280/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0239 - acc: 0.9922 - val_loss: 1.1679 - val_acc: 0.7760\n",
      "Epoch 281/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0214 - acc: 0.9933 - val_loss: 1.1848 - val_acc: 0.7818\n",
      "Epoch 282/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0240 - acc: 0.9920 - val_loss: 1.1712 - val_acc: 0.7772\n",
      "Epoch 283/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0229 - acc: 0.9931 - val_loss: 1.1952 - val_acc: 0.7804\n",
      "Epoch 284/300\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0240 - acc: 0.9920 - val_loss: 1.0774 - val_acc: 0.7902\n",
      "Epoch 285/300\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 0.0244 - acc: 0.9922 - val_loss: 1.0995 - val_acc: 0.7931\n",
      "Epoch 286/300\n",
      "433/546 [======================>.......] - ETA: 3s - loss: 0.0226 - acc: 0.9926"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea1_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea1_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5872.486154556274"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  73856       maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 8, 8, 192)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    885248      concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "vgg16 (Model)                   multiple             14714688    input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 512)          0           vgg16[1][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 704)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 1216)         0           global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           12170       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 20,599,602\n",
      "Trainable params: 5,881,054\n",
      "Non-trainable params: 14,718,548\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea1_model.hdf5')\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 83s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0228634945854813, 0.7924]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 05:40:30.765764 139959264085760 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1093/1093 [==============================] - 30s 27ms/step - loss: 1.6010 - acc: 0.4332 - val_loss: 2.0038 - val_acc: 0.3562\n",
      "Epoch 2/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.3899 - acc: 0.5103 - val_loss: 1.4360 - val_acc: 0.4880\n",
      "Epoch 3/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.3028 - acc: 0.5413 - val_loss: 1.5977 - val_acc: 0.4494\n",
      "Epoch 4/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.2395 - acc: 0.5619 - val_loss: 1.4075 - val_acc: 0.5114\n",
      "Epoch 5/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.1882 - acc: 0.5779 - val_loss: 1.3048 - val_acc: 0.5452\n",
      "Epoch 6/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.1478 - acc: 0.5945 - val_loss: 1.4221 - val_acc: 0.5207\n",
      "Epoch 7/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.1003 - acc: 0.6133 - val_loss: 1.2547 - val_acc: 0.5587\n",
      "Epoch 8/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0572 - acc: 0.6284 - val_loss: 1.2854 - val_acc: 0.5546\n",
      "Epoch 9/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0199 - acc: 0.6408 - val_loss: 1.2087 - val_acc: 0.5867\n",
      "Epoch 10/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9916 - acc: 0.6524 - val_loss: 1.0255 - val_acc: 0.6483\n",
      "Epoch 11/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9565 - acc: 0.6658 - val_loss: 1.0692 - val_acc: 0.6319\n",
      "Epoch 12/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9286 - acc: 0.6747 - val_loss: 1.2139 - val_acc: 0.5879\n",
      "Epoch 13/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9019 - acc: 0.6858 - val_loss: 1.1068 - val_acc: 0.6216\n",
      "Epoch 14/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8811 - acc: 0.6890 - val_loss: 0.9723 - val_acc: 0.6650\n",
      "Epoch 15/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8593 - acc: 0.6992 - val_loss: 1.0949 - val_acc: 0.6402\n",
      "Epoch 16/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8293 - acc: 0.7098 - val_loss: 0.8919 - val_acc: 0.6924\n",
      "Epoch 17/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8169 - acc: 0.7147 - val_loss: 1.0406 - val_acc: 0.6493\n",
      "Epoch 18/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7986 - acc: 0.7202 - val_loss: 0.9619 - val_acc: 0.6709\n",
      "Epoch 19/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7725 - acc: 0.7315 - val_loss: 0.8317 - val_acc: 0.7202\n",
      "Epoch 20/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7587 - acc: 0.7351 - val_loss: 0.9361 - val_acc: 0.6796\n",
      "Epoch 21/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7363 - acc: 0.7463 - val_loss: 0.8429 - val_acc: 0.7171\n",
      "Epoch 22/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7230 - acc: 0.7485 - val_loss: 0.9949 - val_acc: 0.6695\n",
      "Epoch 23/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7064 - acc: 0.7530 - val_loss: 0.9131 - val_acc: 0.6933\n",
      "Epoch 24/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6856 - acc: 0.7599 - val_loss: 0.8252 - val_acc: 0.7198\n",
      "Epoch 25/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6670 - acc: 0.7680 - val_loss: 0.8130 - val_acc: 0.7258\n",
      "Epoch 26/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6544 - acc: 0.7734 - val_loss: 0.8761 - val_acc: 0.6994\n",
      "Epoch 27/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6398 - acc: 0.7790 - val_loss: 0.9849 - val_acc: 0.6778\n",
      "Epoch 28/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6201 - acc: 0.7838 - val_loss: 0.8383 - val_acc: 0.7165\n",
      "Epoch 29/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6104 - acc: 0.7887 - val_loss: 0.9168 - val_acc: 0.6923\n",
      "Epoch 30/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5945 - acc: 0.7927 - val_loss: 0.9109 - val_acc: 0.7046\n",
      "Epoch 31/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5799 - acc: 0.7976 - val_loss: 0.8024 - val_acc: 0.7338\n",
      "Epoch 32/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5639 - acc: 0.8064 - val_loss: 0.7833 - val_acc: 0.7373\n",
      "Epoch 33/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5575 - acc: 0.8054 - val_loss: 0.7844 - val_acc: 0.7366\n",
      "Epoch 34/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5390 - acc: 0.8150 - val_loss: 0.8087 - val_acc: 0.7311\n",
      "Epoch 35/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5277 - acc: 0.8183 - val_loss: 0.7836 - val_acc: 0.7410\n",
      "Epoch 36/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5133 - acc: 0.8239 - val_loss: 0.8025 - val_acc: 0.7290\n",
      "Epoch 37/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5045 - acc: 0.8266 - val_loss: 0.8137 - val_acc: 0.7371\n",
      "Epoch 38/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4918 - acc: 0.8287 - val_loss: 0.7905 - val_acc: 0.7359\n",
      "Epoch 39/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4770 - acc: 0.8351 - val_loss: 0.7679 - val_acc: 0.7509\n",
      "Epoch 40/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4612 - acc: 0.8398 - val_loss: 0.9596 - val_acc: 0.6929\n",
      "Epoch 41/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4541 - acc: 0.8426 - val_loss: 0.8101 - val_acc: 0.7337\n",
      "Epoch 42/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4433 - acc: 0.8492 - val_loss: 0.7517 - val_acc: 0.7538\n",
      "Epoch 43/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4351 - acc: 0.8527 - val_loss: 0.9159 - val_acc: 0.7204\n",
      "Epoch 44/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4168 - acc: 0.8565 - val_loss: 0.8026 - val_acc: 0.7459\n",
      "Epoch 45/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4076 - acc: 0.8600 - val_loss: 0.7327 - val_acc: 0.7636\n",
      "Epoch 46/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4011 - acc: 0.8644 - val_loss: 0.7429 - val_acc: 0.7595\n",
      "Epoch 47/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3862 - acc: 0.8660 - val_loss: 0.8788 - val_acc: 0.7300\n",
      "Epoch 48/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3744 - acc: 0.8705 - val_loss: 0.7593 - val_acc: 0.7606\n",
      "Epoch 49/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3670 - acc: 0.8734 - val_loss: 0.8068 - val_acc: 0.7492\n",
      "Epoch 50/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3559 - acc: 0.8761 - val_loss: 0.7466 - val_acc: 0.7620\n",
      "Epoch 51/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3481 - acc: 0.8815 - val_loss: 0.7604 - val_acc: 0.7631\n",
      "Epoch 52/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3408 - acc: 0.8826 - val_loss: 0.7766 - val_acc: 0.7655\n",
      "Epoch 53/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3379 - acc: 0.8845 - val_loss: 0.7558 - val_acc: 0.7678\n",
      "Epoch 54/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3200 - acc: 0.8891 - val_loss: 0.8498 - val_acc: 0.7455\n",
      "Epoch 55/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3175 - acc: 0.8909 - val_loss: 0.7871 - val_acc: 0.7557\n",
      "Epoch 56/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3059 - acc: 0.8961 - val_loss: 0.7720 - val_acc: 0.7618\n",
      "Epoch 57/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2997 - acc: 0.8976 - val_loss: 0.7940 - val_acc: 0.7660\n",
      "Epoch 58/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2887 - acc: 0.9008 - val_loss: 0.9304 - val_acc: 0.7258\n",
      "Epoch 59/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2863 - acc: 0.9018 - val_loss: 0.8796 - val_acc: 0.7374\n",
      "Epoch 60/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2756 - acc: 0.9045 - val_loss: 0.7951 - val_acc: 0.7649\n",
      "Epoch 61/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2657 - acc: 0.9085 - val_loss: 0.9449 - val_acc: 0.7288\n",
      "Epoch 62/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2623 - acc: 0.9081 - val_loss: 0.8121 - val_acc: 0.7666\n",
      "Epoch 63/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2513 - acc: 0.9145 - val_loss: 0.9110 - val_acc: 0.7529\n",
      "Epoch 64/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2486 - acc: 0.9137 - val_loss: 0.8151 - val_acc: 0.7657\n",
      "Epoch 65/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2448 - acc: 0.9161 - val_loss: 0.8197 - val_acc: 0.7606\n",
      "Epoch 66/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2383 - acc: 0.9175 - val_loss: 0.8195 - val_acc: 0.7627\n",
      "Epoch 67/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2376 - acc: 0.9186 - val_loss: 0.7473 - val_acc: 0.7765\n",
      "Epoch 68/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2295 - acc: 0.9211 - val_loss: 0.9421 - val_acc: 0.7517\n",
      "Epoch 69/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2180 - acc: 0.9264 - val_loss: 0.8763 - val_acc: 0.7450\n",
      "Epoch 70/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2128 - acc: 0.9287 - val_loss: 0.9105 - val_acc: 0.7460\n",
      "Epoch 71/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2117 - acc: 0.9280 - val_loss: 0.8907 - val_acc: 0.7448\n",
      "Epoch 72/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2094 - acc: 0.9283 - val_loss: 0.8221 - val_acc: 0.7692\n",
      "Epoch 73/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1924 - acc: 0.9336 - val_loss: 0.7678 - val_acc: 0.7813\n",
      "Epoch 74/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1940 - acc: 0.9340 - val_loss: 0.8427 - val_acc: 0.7634\n",
      "Epoch 75/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1996 - acc: 0.9322 - val_loss: 0.8089 - val_acc: 0.7742\n",
      "Epoch 76/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1942 - acc: 0.9337 - val_loss: 0.9104 - val_acc: 0.7571\n",
      "Epoch 77/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1898 - acc: 0.9359 - val_loss: 0.8110 - val_acc: 0.7776\n",
      "Epoch 78/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1802 - acc: 0.9387 - val_loss: 0.8017 - val_acc: 0.7812\n",
      "Epoch 79/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1757 - acc: 0.9391 - val_loss: 0.8879 - val_acc: 0.7606\n",
      "Epoch 80/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1738 - acc: 0.9398 - val_loss: 0.8918 - val_acc: 0.7606\n",
      "Epoch 81/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1696 - acc: 0.9426 - val_loss: 0.9785 - val_acc: 0.7479\n",
      "Epoch 82/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1737 - acc: 0.9403 - val_loss: 0.8929 - val_acc: 0.7612\n",
      "Epoch 83/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1643 - acc: 0.9442 - val_loss: 0.9090 - val_acc: 0.7680\n",
      "Epoch 84/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1598 - acc: 0.9455 - val_loss: 0.8857 - val_acc: 0.7600\n",
      "Epoch 85/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1611 - acc: 0.9439 - val_loss: 0.8852 - val_acc: 0.7686\n",
      "Epoch 86/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1511 - acc: 0.9475 - val_loss: 0.8019 - val_acc: 0.7835\n",
      "Epoch 87/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1531 - acc: 0.9488 - val_loss: 0.8613 - val_acc: 0.7637\n",
      "Epoch 88/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1537 - acc: 0.9485 - val_loss: 0.9024 - val_acc: 0.7705\n",
      "Epoch 89/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1461 - acc: 0.9503 - val_loss: 0.8959 - val_acc: 0.7666\n",
      "Epoch 90/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1455 - acc: 0.9519 - val_loss: 0.8363 - val_acc: 0.7776\n",
      "Epoch 91/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1411 - acc: 0.9515 - val_loss: 0.9138 - val_acc: 0.7617\n",
      "Epoch 92/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1357 - acc: 0.9546 - val_loss: 0.8214 - val_acc: 0.7793\n",
      "Epoch 93/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1384 - acc: 0.9522 - val_loss: 0.8898 - val_acc: 0.7669\n",
      "Epoch 94/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1366 - acc: 0.9540 - val_loss: 0.9073 - val_acc: 0.7674\n",
      "Epoch 95/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1314 - acc: 0.9566 - val_loss: 0.9460 - val_acc: 0.7597\n",
      "Epoch 96/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1320 - acc: 0.9535 - val_loss: 0.9351 - val_acc: 0.7616\n",
      "Epoch 97/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1291 - acc: 0.9564 - val_loss: 0.9390 - val_acc: 0.7582\n",
      "Epoch 98/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1265 - acc: 0.9566 - val_loss: 0.9145 - val_acc: 0.7653\n",
      "Epoch 99/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1261 - acc: 0.9564 - val_loss: 0.8830 - val_acc: 0.7709\n",
      "Epoch 100/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1284 - acc: 0.9567 - val_loss: 0.9313 - val_acc: 0.7667\n",
      "Epoch 101/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1214 - acc: 0.9582 - val_loss: 0.9143 - val_acc: 0.7700\n",
      "Epoch 102/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1222 - acc: 0.9584 - val_loss: 0.9034 - val_acc: 0.7724\n",
      "Epoch 103/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1171 - acc: 0.9603 - val_loss: 0.9666 - val_acc: 0.7674\n",
      "Epoch 104/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1132 - acc: 0.9618 - val_loss: 0.8494 - val_acc: 0.7853\n",
      "Epoch 105/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1175 - acc: 0.9603 - val_loss: 0.8621 - val_acc: 0.7803\n",
      "Epoch 106/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1151 - acc: 0.9615 - val_loss: 0.8888 - val_acc: 0.7754\n",
      "Epoch 107/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1115 - acc: 0.9629 - val_loss: 0.9697 - val_acc: 0.7533\n",
      "Epoch 108/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1068 - acc: 0.9636 - val_loss: 0.8877 - val_acc: 0.7772\n",
      "Epoch 109/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1090 - acc: 0.9625 - val_loss: 0.8864 - val_acc: 0.7791\n",
      "Epoch 110/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1093 - acc: 0.9634 - val_loss: 0.8652 - val_acc: 0.7895\n",
      "Epoch 111/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1061 - acc: 0.9651 - val_loss: 0.9706 - val_acc: 0.7644\n",
      "Epoch 112/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1047 - acc: 0.9645 - val_loss: 0.8882 - val_acc: 0.7908\n",
      "Epoch 113/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1039 - acc: 0.9651 - val_loss: 0.9620 - val_acc: 0.7676\n",
      "Epoch 114/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0974 - acc: 0.9665 - val_loss: 1.0094 - val_acc: 0.7620\n",
      "Epoch 115/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1062 - acc: 0.9641 - val_loss: 0.8700 - val_acc: 0.7833\n",
      "Epoch 116/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0964 - acc: 0.9683 - val_loss: 0.9128 - val_acc: 0.7739\n",
      "Epoch 117/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0960 - acc: 0.9681 - val_loss: 0.9508 - val_acc: 0.7781\n",
      "Epoch 118/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0992 - acc: 0.9660 - val_loss: 0.9070 - val_acc: 0.7818\n",
      "Epoch 119/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0956 - acc: 0.9683 - val_loss: 0.8962 - val_acc: 0.7783\n",
      "Epoch 120/300\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0956 - acc: 0.9677 - val_loss: 0.8986 - val_acc: 0.7812\n",
      "Epoch 121/300\n",
      "1090/1093 [============================>.] - ETA: 0s - loss: 0.0946 - acc: 0.9677"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea1_32_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea1_32_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 86s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea1_32_model.hdf5')\n",
    "\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9464669375022655, 0.795]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2\n",
    "\n",
    "Use the feature extractor **from every output of convolutional layers** of pretrain model to produce laten space, then concatenate with the original model\n",
    "Here we use the VGG pretrain model from CIFAR 10 and the target task is CIFAR10 classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create DensedVGG model with 3 blocks\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_3/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Concatenate  [<tf.Tensor 'cropping_block0_block2_3/strided_slice:0' shape=(?, 8, 8, 64) dtype=float32>, <tf.Tensor 'maxpooling_block1_3/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  73856       maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 8, 8, 192)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    885248      concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 multiple             14714688    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 64)           0           model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_6 (Glo (None, 128)          0           model_3[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_7 (Glo (None, 256)          0           model_3[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_8 (Glo (None, 512)          0           model_3[1][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_9 (Glo (None, 512)          0           model_3[1][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_10 (Gl (None, 704)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 2176)         0           global_average_pooling2d_5[0][0] \n",
      "                                                                 global_average_pooling2d_6[0][0] \n",
      "                                                                 global_average_pooling2d_7[0][0] \n",
      "                                                                 global_average_pooling2d_8[0][0] \n",
      "                                                                 global_average_pooling2d_9[0][0] \n",
      "                                                                 global_average_pooling2d_10[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           21770       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10)           40          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10)           0           batch_normalization_3[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 20,609,202\n",
      "Trainable params: 5,890,654\n",
      "Non-trainable params: 14,718,548\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "for layer in pretrain_vgg.layers:\n",
    "    layer.trainable=False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs)\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "\n",
    "for layer in vgg_outs:\n",
    "    vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_layer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "vgg_gaps.append(skippedvgg_out)\n",
    "\n",
    "concat_layer = concatenate(vgg_gaps)\n",
    "outputs = Dense(10)(concat_layer)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 05:01:38.033998 140163647837952 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "546/546 [==============================] - 22s 41ms/step - loss: 1.4893 - acc: 0.4913 - val_loss: 1.8381 - val_acc: 0.3663\n",
      "Epoch 2/130\n",
      "546/546 [==============================] - 20s 37ms/step - loss: 1.2694 - acc: 0.5697 - val_loss: 1.5107 - val_acc: 0.4812\n",
      "Epoch 3/130\n",
      "546/546 [==============================] - 19s 35ms/step - loss: 1.1868 - acc: 0.5926 - val_loss: 1.3694 - val_acc: 0.5345\n",
      "Epoch 4/130\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.1409 - acc: 0.6071 - val_loss: 1.2462 - val_acc: 0.5837\n",
      "Epoch 5/130\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0971 - acc: 0.6217 - val_loss: 1.2673 - val_acc: 0.5593\n",
      "Epoch 6/130\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0602 - acc: 0.6310 - val_loss: 1.5290 - val_acc: 0.4772\n",
      "Epoch 7/130\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0414 - acc: 0.6362 - val_loss: 1.2553 - val_acc: 0.5512\n",
      "Epoch 8/130\n",
      "546/546 [==============================] - 19s 36ms/step - loss: 1.0146 - acc: 0.6424 - val_loss: 1.1665 - val_acc: 0.5974\n",
      "Epoch 9/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9973 - acc: 0.6526 - val_loss: 1.1532 - val_acc: 0.5969\n",
      "Epoch 10/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9706 - acc: 0.6582 - val_loss: 1.1160 - val_acc: 0.6100\n",
      "Epoch 11/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9576 - acc: 0.6644 - val_loss: 1.0803 - val_acc: 0.6246\n",
      "Epoch 12/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9323 - acc: 0.6739 - val_loss: 1.1332 - val_acc: 0.6017\n",
      "Epoch 13/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9123 - acc: 0.6820 - val_loss: 1.1154 - val_acc: 0.6154\n",
      "Epoch 14/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.9010 - acc: 0.6824 - val_loss: 1.1610 - val_acc: 0.5996\n",
      "Epoch 15/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.8809 - acc: 0.6940 - val_loss: 1.0217 - val_acc: 0.6437\n",
      "Epoch 16/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.8649 - acc: 0.6966 - val_loss: 0.9257 - val_acc: 0.6780\n",
      "Epoch 17/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.8465 - acc: 0.7029 - val_loss: 1.0468 - val_acc: 0.6381\n",
      "Epoch 18/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.8289 - acc: 0.7096 - val_loss: 1.0040 - val_acc: 0.6550\n",
      "Epoch 19/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.8152 - acc: 0.7133 - val_loss: 0.9634 - val_acc: 0.6755\n",
      "Epoch 20/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7984 - acc: 0.7213 - val_loss: 1.4613 - val_acc: 0.5645\n",
      "Epoch 21/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7761 - acc: 0.7277 - val_loss: 1.0672 - val_acc: 0.6456\n",
      "Epoch 22/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7634 - acc: 0.7346 - val_loss: 0.9214 - val_acc: 0.6856\n",
      "Epoch 23/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7439 - acc: 0.7376 - val_loss: 0.8969 - val_acc: 0.6925\n",
      "Epoch 24/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7383 - acc: 0.7407 - val_loss: 0.9232 - val_acc: 0.6861\n",
      "Epoch 25/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7187 - acc: 0.7488 - val_loss: 0.9032 - val_acc: 0.6936\n",
      "Epoch 26/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.7046 - acc: 0.7537 - val_loss: 1.0138 - val_acc: 0.6679\n",
      "Epoch 27/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6908 - acc: 0.7574 - val_loss: 0.9397 - val_acc: 0.6790\n",
      "Epoch 28/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6836 - acc: 0.7587 - val_loss: 0.9239 - val_acc: 0.6839\n",
      "Epoch 29/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6638 - acc: 0.7662 - val_loss: 1.0659 - val_acc: 0.6461\n",
      "Epoch 30/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6493 - acc: 0.7717 - val_loss: 1.0417 - val_acc: 0.6553\n",
      "Epoch 31/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6479 - acc: 0.7741 - val_loss: 0.9027 - val_acc: 0.6970\n",
      "Epoch 32/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6299 - acc: 0.7789 - val_loss: 1.0507 - val_acc: 0.6629\n",
      "Epoch 33/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.6119 - acc: 0.7852 - val_loss: 0.9711 - val_acc: 0.6846\n",
      "Epoch 34/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5959 - acc: 0.7897 - val_loss: 0.9390 - val_acc: 0.6973\n",
      "Epoch 35/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5850 - acc: 0.7938 - val_loss: 0.8713 - val_acc: 0.7120\n",
      "Epoch 36/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5741 - acc: 0.8006 - val_loss: 1.0093 - val_acc: 0.6780\n",
      "Epoch 37/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5630 - acc: 0.8030 - val_loss: 0.8296 - val_acc: 0.7269\n",
      "Epoch 38/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5511 - acc: 0.8050 - val_loss: 0.9313 - val_acc: 0.7041\n",
      "Epoch 39/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5326 - acc: 0.8164 - val_loss: 0.9872 - val_acc: 0.6838\n",
      "Epoch 40/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5239 - acc: 0.8169 - val_loss: 0.9061 - val_acc: 0.7008\n",
      "Epoch 41/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.5099 - acc: 0.8185 - val_loss: 0.9219 - val_acc: 0.7015\n",
      "Epoch 42/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4937 - acc: 0.8279 - val_loss: 0.9084 - val_acc: 0.7114\n",
      "Epoch 43/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4847 - acc: 0.8287 - val_loss: 0.9991 - val_acc: 0.6822\n",
      "Epoch 44/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4776 - acc: 0.8310 - val_loss: 0.9212 - val_acc: 0.7106\n",
      "Epoch 45/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4564 - acc: 0.8406 - val_loss: 0.9077 - val_acc: 0.7122\n",
      "Epoch 46/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4522 - acc: 0.8415 - val_loss: 0.8494 - val_acc: 0.7318\n",
      "Epoch 47/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4395 - acc: 0.8451 - val_loss: 0.9211 - val_acc: 0.7098\n",
      "Epoch 48/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4275 - acc: 0.8498 - val_loss: 0.8155 - val_acc: 0.7412\n",
      "Epoch 49/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4205 - acc: 0.8530 - val_loss: 0.9628 - val_acc: 0.7123\n",
      "Epoch 50/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.4118 - acc: 0.8569 - val_loss: 0.9941 - val_acc: 0.7001\n",
      "Epoch 51/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3994 - acc: 0.8598 - val_loss: 0.9687 - val_acc: 0.7092\n",
      "Epoch 52/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3825 - acc: 0.8655 - val_loss: 0.8500 - val_acc: 0.7392\n",
      "Epoch 53/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3757 - acc: 0.8686 - val_loss: 0.9683 - val_acc: 0.7106\n",
      "Epoch 54/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3670 - acc: 0.8699 - val_loss: 1.0036 - val_acc: 0.6993\n",
      "Epoch 55/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3517 - acc: 0.8772 - val_loss: 0.9740 - val_acc: 0.7006\n",
      "Epoch 56/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3536 - acc: 0.8746 - val_loss: 0.9988 - val_acc: 0.7067\n",
      "Epoch 57/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3361 - acc: 0.8825 - val_loss: 0.9828 - val_acc: 0.7126\n",
      "Epoch 58/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3252 - acc: 0.8873 - val_loss: 1.0681 - val_acc: 0.7072\n",
      "Epoch 59/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3245 - acc: 0.8878 - val_loss: 1.0254 - val_acc: 0.7186\n",
      "Epoch 60/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3080 - acc: 0.8938 - val_loss: 0.8543 - val_acc: 0.7416\n",
      "Epoch 61/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.3055 - acc: 0.8920 - val_loss: 1.1277 - val_acc: 0.7010\n",
      "Epoch 62/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2937 - acc: 0.8980 - val_loss: 0.9802 - val_acc: 0.7238\n",
      "Epoch 63/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2817 - acc: 0.9030 - val_loss: 1.0562 - val_acc: 0.7155\n",
      "Epoch 64/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2779 - acc: 0.9041 - val_loss: 0.9264 - val_acc: 0.7339\n",
      "Epoch 65/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2680 - acc: 0.9079 - val_loss: 1.0459 - val_acc: 0.7147\n",
      "Epoch 66/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2537 - acc: 0.9138 - val_loss: 0.9833 - val_acc: 0.7183\n",
      "Epoch 67/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2598 - acc: 0.9094 - val_loss: 1.0504 - val_acc: 0.7059\n",
      "Epoch 68/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2503 - acc: 0.9145 - val_loss: 0.9182 - val_acc: 0.7501\n",
      "Epoch 69/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2410 - acc: 0.9167 - val_loss: 0.9973 - val_acc: 0.7211\n",
      "Epoch 70/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2419 - acc: 0.9171 - val_loss: 1.0218 - val_acc: 0.7191\n",
      "Epoch 71/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2323 - acc: 0.9200 - val_loss: 1.0273 - val_acc: 0.7258\n",
      "Epoch 72/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2221 - acc: 0.9226 - val_loss: 1.0446 - val_acc: 0.7160\n",
      "Epoch 73/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2177 - acc: 0.9255 - val_loss: 0.9370 - val_acc: 0.7426\n",
      "Epoch 74/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2128 - acc: 0.9278 - val_loss: 1.0048 - val_acc: 0.7256\n",
      "Epoch 75/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.2092 - acc: 0.9271 - val_loss: 1.1907 - val_acc: 0.6958\n",
      "Epoch 76/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1990 - acc: 0.9307 - val_loss: 1.0901 - val_acc: 0.7158\n",
      "Epoch 77/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1938 - acc: 0.9334 - val_loss: 1.0539 - val_acc: 0.7297\n",
      "Epoch 78/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1829 - acc: 0.9373 - val_loss: 1.0625 - val_acc: 0.7197\n",
      "Epoch 79/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1894 - acc: 0.9343 - val_loss: 0.9778 - val_acc: 0.7395\n",
      "Epoch 80/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1818 - acc: 0.9366 - val_loss: 1.1534 - val_acc: 0.7092\n",
      "Epoch 81/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1728 - acc: 0.9425 - val_loss: 1.0723 - val_acc: 0.7278\n",
      "Epoch 82/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1780 - acc: 0.9387 - val_loss: 0.9489 - val_acc: 0.7422\n",
      "Epoch 83/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1662 - acc: 0.9428 - val_loss: 1.1307 - val_acc: 0.7205\n",
      "Epoch 84/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1603 - acc: 0.9456 - val_loss: 0.9776 - val_acc: 0.7445\n",
      "Epoch 85/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1590 - acc: 0.9461 - val_loss: 1.1494 - val_acc: 0.7114\n",
      "Epoch 86/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1558 - acc: 0.9463 - val_loss: 1.1527 - val_acc: 0.7163\n",
      "Epoch 87/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1500 - acc: 0.9487 - val_loss: 1.1434 - val_acc: 0.7025\n",
      "Epoch 88/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1502 - acc: 0.9499 - val_loss: 1.0508 - val_acc: 0.7250\n",
      "Epoch 89/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1469 - acc: 0.9492 - val_loss: 1.0451 - val_acc: 0.7440\n",
      "Epoch 90/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1448 - acc: 0.9513 - val_loss: 1.0374 - val_acc: 0.7325\n",
      "Epoch 91/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1453 - acc: 0.9501 - val_loss: 1.1635 - val_acc: 0.7159\n",
      "Epoch 92/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1367 - acc: 0.9539 - val_loss: 1.2181 - val_acc: 0.7173\n",
      "Epoch 93/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1363 - acc: 0.9533 - val_loss: 1.1889 - val_acc: 0.7036\n",
      "Epoch 94/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1344 - acc: 0.9550 - val_loss: 1.0468 - val_acc: 0.7354\n",
      "Epoch 95/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1321 - acc: 0.9548 - val_loss: 0.9861 - val_acc: 0.7519\n",
      "Epoch 96/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1304 - acc: 0.9563 - val_loss: 1.0276 - val_acc: 0.7460\n",
      "Epoch 97/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1221 - acc: 0.9593 - val_loss: 1.0333 - val_acc: 0.7458\n",
      "Epoch 98/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1260 - acc: 0.9583 - val_loss: 1.2095 - val_acc: 0.7118\n",
      "Epoch 99/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1203 - acc: 0.9590 - val_loss: 1.0590 - val_acc: 0.7414\n",
      "Epoch 100/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1207 - acc: 0.9601 - val_loss: 1.1320 - val_acc: 0.7363\n",
      "Epoch 101/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1150 - acc: 0.9620 - val_loss: 1.2784 - val_acc: 0.7159\n",
      "Epoch 102/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1163 - acc: 0.9615 - val_loss: 1.0226 - val_acc: 0.7506\n",
      "Epoch 103/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1160 - acc: 0.9603 - val_loss: 1.0599 - val_acc: 0.7432\n",
      "Epoch 104/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1132 - acc: 0.9618 - val_loss: 1.1707 - val_acc: 0.7187\n",
      "Epoch 105/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1045 - acc: 0.9648 - val_loss: 1.0687 - val_acc: 0.7370\n",
      "Epoch 106/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1057 - acc: 0.9649 - val_loss: 1.0898 - val_acc: 0.7404\n",
      "Epoch 107/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1103 - acc: 0.9632 - val_loss: 1.0675 - val_acc: 0.7377\n",
      "Epoch 108/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.1073 - acc: 0.9645 - val_loss: 1.2172 - val_acc: 0.7200\n",
      "Epoch 109/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0980 - acc: 0.9678 - val_loss: 1.0371 - val_acc: 0.7522\n",
      "Epoch 110/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0946 - acc: 0.9691 - val_loss: 1.2404 - val_acc: 0.7276\n",
      "Epoch 111/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0997 - acc: 0.9672 - val_loss: 1.1020 - val_acc: 0.7370\n",
      "Epoch 112/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0926 - acc: 0.9695 - val_loss: 1.2023 - val_acc: 0.7303\n",
      "Epoch 113/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0921 - acc: 0.9697 - val_loss: 1.0835 - val_acc: 0.7503\n",
      "Epoch 114/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0940 - acc: 0.9689 - val_loss: 1.1961 - val_acc: 0.7128\n",
      "Epoch 115/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0913 - acc: 0.9696 - val_loss: 1.2470 - val_acc: 0.7196\n",
      "Epoch 116/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0937 - acc: 0.9685 - val_loss: 1.0859 - val_acc: 0.7394\n",
      "Epoch 117/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0981 - acc: 0.9665 - val_loss: 1.2370 - val_acc: 0.7160\n",
      "Epoch 118/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0865 - acc: 0.9725 - val_loss: 1.1440 - val_acc: 0.7320\n",
      "Epoch 119/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0844 - acc: 0.9725 - val_loss: 1.1446 - val_acc: 0.7400\n",
      "Epoch 120/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0858 - acc: 0.9716 - val_loss: 1.1712 - val_acc: 0.7384\n",
      "Epoch 121/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0868 - acc: 0.9716 - val_loss: 1.1100 - val_acc: 0.7443\n",
      "Epoch 122/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0848 - acc: 0.9731 - val_loss: 1.2695 - val_acc: 0.7114\n",
      "Epoch 123/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0868 - acc: 0.9717 - val_loss: 1.1373 - val_acc: 0.7422\n",
      "Epoch 124/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0816 - acc: 0.9730 - val_loss: 1.1552 - val_acc: 0.7347\n",
      "Epoch 125/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0805 - acc: 0.9731 - val_loss: 1.3300 - val_acc: 0.7126\n",
      "Epoch 126/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0808 - acc: 0.9733 - val_loss: 1.1831 - val_acc: 0.7245\n",
      "Epoch 127/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0804 - acc: 0.9739 - val_loss: 1.1189 - val_acc: 0.7465\n",
      "Epoch 128/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0793 - acc: 0.9737 - val_loss: 1.2548 - val_acc: 0.7217\n",
      "Epoch 129/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0771 - acc: 0.9751 - val_loss: 1.1560 - val_acc: 0.7392\n",
      "Epoch 130/130\n",
      "546/546 [==============================] - 20s 36ms/step - loss: 0.0824 - acc: 0.9733 - val_loss: 1.1485 - val_acc: 0.7392\n",
      "exe time:  2565.0550076961517\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea12_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea12_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 83s 8ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0389897839899291, 0.7529]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea12_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 29s 27ms/step - loss: 1.4874 - acc: 0.4915 - val_loss: 1.4351 - val_acc: 0.4944\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.2891 - acc: 0.5540 - val_loss: 1.3448 - val_acc: 0.5311\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.2154 - acc: 0.5770 - val_loss: 1.2646 - val_acc: 0.5587\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.1785 - acc: 0.5865 - val_loss: 1.3030 - val_acc: 0.5452\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.1447 - acc: 0.5985 - val_loss: 1.2361 - val_acc: 0.5644\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.1160 - acc: 0.6080 - val_loss: 1.1761 - val_acc: 0.5963\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.0897 - acc: 0.6181 - val_loss: 1.1731 - val_acc: 0.5909\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.0714 - acc: 0.6213 - val_loss: 1.1948 - val_acc: 0.5898\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0479 - acc: 0.6347 - val_loss: 1.1876 - val_acc: 0.5987\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.0388 - acc: 0.6329 - val_loss: 1.2127 - val_acc: 0.5887\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.0219 - acc: 0.6407 - val_loss: 1.1002 - val_acc: 0.6145\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.9979 - acc: 0.6445 - val_loss: 1.0720 - val_acc: 0.6307\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.9852 - acc: 0.6525 - val_loss: 1.1405 - val_acc: 0.6048\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9636 - acc: 0.6615 - val_loss: 1.0537 - val_acc: 0.6426\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.9565 - acc: 0.6595 - val_loss: 1.0978 - val_acc: 0.6164\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.9422 - acc: 0.6690 - val_loss: 1.0342 - val_acc: 0.6462\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9211 - acc: 0.6758 - val_loss: 1.0622 - val_acc: 0.6431\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.8999 - acc: 0.6845 - val_loss: 1.0917 - val_acc: 0.6277\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8938 - acc: 0.6866 - val_loss: 1.0293 - val_acc: 0.6422\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.8728 - acc: 0.6942 - val_loss: 0.9815 - val_acc: 0.6601\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8596 - acc: 0.7004 - val_loss: 1.1467 - val_acc: 0.6178\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8435 - acc: 0.7024 - val_loss: 0.9175 - val_acc: 0.6825\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8309 - acc: 0.7098 - val_loss: 0.9038 - val_acc: 0.6845\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8183 - acc: 0.7144 - val_loss: 0.9701 - val_acc: 0.6676\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.8002 - acc: 0.7215 - val_loss: 0.9664 - val_acc: 0.6708\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7860 - acc: 0.7269 - val_loss: 0.9835 - val_acc: 0.6639\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.7683 - acc: 0.7313 - val_loss: 0.8967 - val_acc: 0.6941\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7639 - acc: 0.7303 - val_loss: 0.9343 - val_acc: 0.6887\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.7453 - acc: 0.7409 - val_loss: 0.9291 - val_acc: 0.6790\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7395 - acc: 0.7395 - val_loss: 0.8973 - val_acc: 0.6900\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.7274 - acc: 0.7472 - val_loss: 0.9973 - val_acc: 0.6698\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7168 - acc: 0.7513 - val_loss: 0.8986 - val_acc: 0.6975\n",
      "Epoch 33/130\n",
      " 466/1093 [===========>..................] - ETA: 13s - loss: 0.6958 - acc: 0.7539"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea12_32_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea12_32_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 89s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9581621086956785, 0.7734]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea12_32_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3\n",
    "\n",
    "Compare between the VGG pretrained model with the current SkippedVGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg = VGG16(include_top=False, input_shape=(32,32,3), weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, freeze all layers and train the defined top model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_11  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 10)                40        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 14,719,858\n",
      "Trainable params: 5,150\n",
      "Non-trainable params: 14,714,708\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "input_layer = Input((32,32,3))\n",
    "# Build the model\n",
    "vgg_out = vgg(input_layer)\n",
    "# Top model\n",
    "output_layer = GlobalAveragePooling2D()(vgg_out)\n",
    "output_layer = Dense(10)(output_layer)\n",
    "output_layer = BatchNormalization()(output_layer)\n",
    "output_layer = Activation('softmax')(output_layer)\n",
    "\n",
    "model3 = Model(inputs=input_layer, outputs=output_layer)\n",
    "model3.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0918 08:42:13.930425 139780496729856 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "546/546 [==============================] - 13s 23ms/step - loss: 1.8105 - acc: 0.3638 - val_loss: 1.7214 - val_acc: 0.3860\n",
      "Epoch 2/30\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.6276 - acc: 0.4325 - val_loss: 1.6062 - val_acc: 0.4355\n",
      "Epoch 3/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5784 - acc: 0.4462 - val_loss: 1.5761 - val_acc: 0.4485\n",
      "Epoch 4/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5549 - acc: 0.4520 - val_loss: 1.5469 - val_acc: 0.4589\n",
      "Epoch 5/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5440 - acc: 0.4554 - val_loss: 1.5426 - val_acc: 0.4565\n",
      "Epoch 6/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5278 - acc: 0.4643 - val_loss: 1.5184 - val_acc: 0.4651\n",
      "Epoch 7/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5264 - acc: 0.4621 - val_loss: 1.5032 - val_acc: 0.4738\n",
      "Epoch 8/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5236 - acc: 0.4598 - val_loss: 1.5340 - val_acc: 0.4606\n",
      "Epoch 9/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5157 - acc: 0.4655 - val_loss: 1.5069 - val_acc: 0.4711\n",
      "Epoch 10/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5066 - acc: 0.4667 - val_loss: 1.5071 - val_acc: 0.4730\n",
      "Epoch 11/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5059 - acc: 0.4690 - val_loss: 1.5234 - val_acc: 0.4675\n",
      "Epoch 12/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5071 - acc: 0.4669 - val_loss: 1.5437 - val_acc: 0.4585\n",
      "Epoch 13/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5114 - acc: 0.4652 - val_loss: 1.4993 - val_acc: 0.4730\n",
      "Epoch 14/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5073 - acc: 0.4687 - val_loss: 1.5057 - val_acc: 0.4688\n",
      "Epoch 15/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5055 - acc: 0.4655 - val_loss: 1.5123 - val_acc: 0.4679\n",
      "Epoch 16/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5066 - acc: 0.4673 - val_loss: 1.5232 - val_acc: 0.4645\n",
      "Epoch 17/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5008 - acc: 0.4688 - val_loss: 1.5163 - val_acc: 0.4681\n",
      "Epoch 18/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5006 - acc: 0.4708 - val_loss: 1.5243 - val_acc: 0.4682\n",
      "Epoch 19/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4959 - acc: 0.4689 - val_loss: 1.5166 - val_acc: 0.4683\n",
      "Epoch 20/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4960 - acc: 0.4714 - val_loss: 1.4925 - val_acc: 0.4786\n",
      "Epoch 21/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4964 - acc: 0.4699 - val_loss: 1.5076 - val_acc: 0.4743\n",
      "Epoch 22/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5004 - acc: 0.4697 - val_loss: 1.5005 - val_acc: 0.4776\n",
      "Epoch 23/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4959 - acc: 0.4726 - val_loss: 1.4902 - val_acc: 0.4784\n",
      "Epoch 24/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4995 - acc: 0.4714 - val_loss: 1.4985 - val_acc: 0.4751\n",
      "Epoch 25/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4988 - acc: 0.4694 - val_loss: 1.4997 - val_acc: 0.4760\n",
      "Epoch 26/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4971 - acc: 0.4692 - val_loss: 1.5021 - val_acc: 0.4717\n",
      "Epoch 27/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4966 - acc: 0.4707 - val_loss: 1.4885 - val_acc: 0.4792\n",
      "Epoch 28/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4974 - acc: 0.4696 - val_loss: 1.4953 - val_acc: 0.4776\n",
      "Epoch 29/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4924 - acc: 0.4725 - val_loss: 1.4875 - val_acc: 0.4785\n",
      "Epoch 30/30\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5000 - acc: 0.4692 - val_loss: 1.4999 - val_acc: 0.4743\n",
      "exe time:  335.13596415519714\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea13_freezed_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model3.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea12_freezed_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('/data/Quan/datasets/cifar10/transfer_idea13_freezed_model.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model3.layers:\n",
    "    layer.trainable=True\n",
    "    \n",
    "for layer in model3.layers[1].layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "546/546 [==============================] - 12s 21ms/step - loss: 1.4947 - acc: 0.4723 - val_loss: 1.4977 - val_acc: 0.4760\n",
      "Epoch 2/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4949 - acc: 0.4706 - val_loss: 1.5224 - val_acc: 0.4657\n",
      "Epoch 3/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4912 - acc: 0.4715 - val_loss: 1.4947 - val_acc: 0.4770\n",
      "Epoch 4/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4927 - acc: 0.4725 - val_loss: 1.5160 - val_acc: 0.4649\n",
      "Epoch 5/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4951 - acc: 0.4739 - val_loss: 1.5026 - val_acc: 0.4743\n",
      "Epoch 6/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4912 - acc: 0.4737 - val_loss: 1.4806 - val_acc: 0.4825\n",
      "Epoch 7/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4867 - acc: 0.4751 - val_loss: 1.5052 - val_acc: 0.4740\n",
      "Epoch 8/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4932 - acc: 0.4712 - val_loss: 1.4988 - val_acc: 0.4762\n",
      "Epoch 9/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4945 - acc: 0.4728 - val_loss: 1.5064 - val_acc: 0.4705\n",
      "Epoch 10/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4906 - acc: 0.4719 - val_loss: 1.4993 - val_acc: 0.4771\n",
      "Epoch 11/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4889 - acc: 0.4717 - val_loss: 1.4968 - val_acc: 0.4762\n",
      "Epoch 12/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4940 - acc: 0.4736 - val_loss: 1.5073 - val_acc: 0.4715\n",
      "Epoch 13/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4924 - acc: 0.4723 - val_loss: 1.4984 - val_acc: 0.4784\n",
      "Epoch 14/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.5045 - acc: 0.4660 - val_loss: 1.5012 - val_acc: 0.4758\n",
      "Epoch 15/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4926 - acc: 0.4725 - val_loss: 1.4995 - val_acc: 0.4754\n",
      "Epoch 16/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4904 - acc: 0.4742 - val_loss: 1.4993 - val_acc: 0.4752\n",
      "Epoch 17/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4902 - acc: 0.4725 - val_loss: 1.4933 - val_acc: 0.4769\n",
      "Epoch 18/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4983 - acc: 0.4697 - val_loss: 1.4963 - val_acc: 0.4762\n",
      "Epoch 19/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4931 - acc: 0.4734 - val_loss: 1.4913 - val_acc: 0.4784\n",
      "Epoch 20/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4919 - acc: 0.4719 - val_loss: 1.5063 - val_acc: 0.4757\n",
      "Epoch 21/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4887 - acc: 0.4768 - val_loss: 1.4865 - val_acc: 0.4852\n",
      "Epoch 22/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4953 - acc: 0.4730 - val_loss: 1.4980 - val_acc: 0.4760\n",
      "Epoch 23/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4920 - acc: 0.4752 - val_loss: 1.4882 - val_acc: 0.4825\n",
      "Epoch 24/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4901 - acc: 0.4729 - val_loss: 1.4766 - val_acc: 0.4827\n",
      "Epoch 25/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4927 - acc: 0.4734 - val_loss: 1.4957 - val_acc: 0.4776\n",
      "Epoch 26/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4878 - acc: 0.4702 - val_loss: 1.4819 - val_acc: 0.4833\n",
      "Epoch 27/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4920 - acc: 0.4733 - val_loss: 1.4965 - val_acc: 0.4772\n",
      "Epoch 28/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4863 - acc: 0.4752 - val_loss: 1.4963 - val_acc: 0.4770\n",
      "Epoch 29/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4907 - acc: 0.4764 - val_loss: 1.5003 - val_acc: 0.4765\n",
      "Epoch 30/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4898 - acc: 0.4726 - val_loss: 1.4856 - val_acc: 0.4811\n",
      "Epoch 31/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4940 - acc: 0.4733 - val_loss: 1.4971 - val_acc: 0.4768\n",
      "Epoch 32/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4904 - acc: 0.4759 - val_loss: 1.4996 - val_acc: 0.4770\n",
      "Epoch 33/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4863 - acc: 0.4784 - val_loss: 1.4933 - val_acc: 0.4778\n",
      "Epoch 34/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4931 - acc: 0.4726 - val_loss: 1.4918 - val_acc: 0.4807\n",
      "Epoch 35/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4883 - acc: 0.4732 - val_loss: 1.5084 - val_acc: 0.4707\n",
      "Epoch 36/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4839 - acc: 0.4769 - val_loss: 1.4906 - val_acc: 0.4798\n",
      "Epoch 37/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4826 - acc: 0.4772 - val_loss: 1.4910 - val_acc: 0.4803\n",
      "Epoch 38/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4888 - acc: 0.4717 - val_loss: 1.4975 - val_acc: 0.4778\n",
      "Epoch 39/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4917 - acc: 0.4734 - val_loss: 1.4808 - val_acc: 0.4852\n",
      "Epoch 40/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4894 - acc: 0.4750 - val_loss: 1.5071 - val_acc: 0.4725\n",
      "Epoch 41/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4944 - acc: 0.4722 - val_loss: 1.4929 - val_acc: 0.4809\n",
      "Epoch 42/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4874 - acc: 0.4738 - val_loss: 1.4891 - val_acc: 0.4794\n",
      "Epoch 43/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4849 - acc: 0.4745 - val_loss: 1.4964 - val_acc: 0.4785\n",
      "Epoch 44/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4921 - acc: 0.4713 - val_loss: 1.5011 - val_acc: 0.4732\n",
      "Epoch 45/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4850 - acc: 0.4744 - val_loss: 1.4918 - val_acc: 0.4775\n",
      "Epoch 46/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4829 - acc: 0.4769 - val_loss: 1.4898 - val_acc: 0.4804\n",
      "Epoch 47/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4861 - acc: 0.4759 - val_loss: 1.4953 - val_acc: 0.4774\n",
      "Epoch 48/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4876 - acc: 0.4750 - val_loss: 1.4986 - val_acc: 0.4759\n",
      "Epoch 49/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4854 - acc: 0.4728 - val_loss: 1.4941 - val_acc: 0.4784\n",
      "Epoch 50/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4908 - acc: 0.4718 - val_loss: 1.5002 - val_acc: 0.4760\n",
      "Epoch 51/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4896 - acc: 0.4747 - val_loss: 1.4935 - val_acc: 0.4762\n",
      "Epoch 52/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4861 - acc: 0.4734 - val_loss: 1.4861 - val_acc: 0.4797\n",
      "Epoch 53/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4812 - acc: 0.4789 - val_loss: 1.4859 - val_acc: 0.4787\n",
      "Epoch 54/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4849 - acc: 0.4756 - val_loss: 1.4904 - val_acc: 0.4807\n",
      "Epoch 55/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4844 - acc: 0.4762 - val_loss: 1.4880 - val_acc: 0.4783\n",
      "Epoch 56/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4845 - acc: 0.4769 - val_loss: 1.4994 - val_acc: 0.4732\n",
      "Epoch 57/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4867 - acc: 0.4765 - val_loss: 1.4948 - val_acc: 0.4770\n",
      "Epoch 58/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4891 - acc: 0.4760 - val_loss: 1.4919 - val_acc: 0.4752\n",
      "Epoch 59/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4824 - acc: 0.4756 - val_loss: 1.4937 - val_acc: 0.4782\n",
      "Epoch 60/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4877 - acc: 0.4740 - val_loss: 1.5084 - val_acc: 0.4742\n",
      "Epoch 61/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4795 - acc: 0.4760 - val_loss: 1.4931 - val_acc: 0.4760\n",
      "Epoch 62/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4857 - acc: 0.4746 - val_loss: 1.4910 - val_acc: 0.4810\n",
      "Epoch 63/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4815 - acc: 0.4773 - val_loss: 1.4905 - val_acc: 0.4786\n",
      "Epoch 64/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4900 - acc: 0.4726 - val_loss: 1.4928 - val_acc: 0.4779\n",
      "Epoch 65/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4891 - acc: 0.4709 - val_loss: 1.4965 - val_acc: 0.4767\n",
      "Epoch 66/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4820 - acc: 0.4772 - val_loss: 1.4904 - val_acc: 0.4758\n",
      "Epoch 67/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4857 - acc: 0.4751 - val_loss: 1.4893 - val_acc: 0.4787\n",
      "Epoch 68/130\n",
      "546/546 [==============================] - 11s 21ms/step - loss: 1.4788 - acc: 0.4804 - val_loss: 1.4808 - val_acc: 0.4817\n",
      "Epoch 69/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4828 - acc: 0.4758 - val_loss: 1.4905 - val_acc: 0.4783\n",
      "Epoch 70/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4845 - acc: 0.4743 - val_loss: 1.4940 - val_acc: 0.4761\n",
      "Epoch 71/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4867 - acc: 0.4755 - val_loss: 1.5001 - val_acc: 0.4766\n",
      "Epoch 72/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4820 - acc: 0.4777 - val_loss: 1.5011 - val_acc: 0.4775\n",
      "Epoch 73/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4847 - acc: 0.4764 - val_loss: 1.4852 - val_acc: 0.4813\n",
      "Epoch 74/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4819 - acc: 0.4778 - val_loss: 1.4829 - val_acc: 0.4839\n",
      "Epoch 75/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4859 - acc: 0.4769 - val_loss: 1.4953 - val_acc: 0.4796\n",
      "Epoch 76/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4858 - acc: 0.4750 - val_loss: 1.4855 - val_acc: 0.4819\n",
      "Epoch 77/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4774 - acc: 0.4786 - val_loss: 1.4888 - val_acc: 0.4813\n",
      "Epoch 78/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4858 - acc: 0.4730 - val_loss: 1.4921 - val_acc: 0.4773\n",
      "Epoch 79/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4828 - acc: 0.4760 - val_loss: 1.4956 - val_acc: 0.4779\n",
      "Epoch 80/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4801 - acc: 0.4773 - val_loss: 1.4950 - val_acc: 0.4790\n",
      "Epoch 81/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4848 - acc: 0.4764 - val_loss: 1.5022 - val_acc: 0.4762\n",
      "Epoch 82/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4843 - acc: 0.4759 - val_loss: 1.4889 - val_acc: 0.4805\n",
      "Epoch 83/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4802 - acc: 0.4780 - val_loss: 1.4958 - val_acc: 0.4768\n",
      "Epoch 84/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4835 - acc: 0.4770 - val_loss: 1.4966 - val_acc: 0.4750\n",
      "Epoch 85/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4871 - acc: 0.4747 - val_loss: 1.4819 - val_acc: 0.4838\n",
      "Epoch 86/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4745 - acc: 0.4776 - val_loss: 1.4894 - val_acc: 0.4784\n",
      "Epoch 87/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4878 - acc: 0.4767 - val_loss: 1.5103 - val_acc: 0.4749\n",
      "Epoch 88/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4816 - acc: 0.4770 - val_loss: 1.4796 - val_acc: 0.4834\n",
      "Epoch 89/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4770 - acc: 0.4789 - val_loss: 1.4975 - val_acc: 0.4796\n",
      "Epoch 90/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4795 - acc: 0.4741 - val_loss: 1.4989 - val_acc: 0.4766\n",
      "Epoch 91/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4871 - acc: 0.4731 - val_loss: 1.5032 - val_acc: 0.4733\n",
      "Epoch 92/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4825 - acc: 0.4781 - val_loss: 1.4950 - val_acc: 0.4798\n",
      "Epoch 93/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4869 - acc: 0.4749 - val_loss: 1.4887 - val_acc: 0.4819\n",
      "Epoch 94/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4868 - acc: 0.4749 - val_loss: 1.4917 - val_acc: 0.4819\n",
      "Epoch 95/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4784 - acc: 0.4783 - val_loss: 1.4917 - val_acc: 0.4790\n",
      "Epoch 96/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4824 - acc: 0.4744 - val_loss: 1.4963 - val_acc: 0.4759\n",
      "Epoch 97/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4881 - acc: 0.4742 - val_loss: 1.4987 - val_acc: 0.4755\n",
      "Epoch 98/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4870 - acc: 0.4748 - val_loss: 1.4963 - val_acc: 0.4786\n",
      "Epoch 99/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4835 - acc: 0.4736 - val_loss: 1.4922 - val_acc: 0.4778\n",
      "Epoch 100/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4851 - acc: 0.4746 - val_loss: 1.4859 - val_acc: 0.4824\n",
      "Epoch 101/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4822 - acc: 0.4764 - val_loss: 1.5122 - val_acc: 0.4736\n",
      "Epoch 102/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4850 - acc: 0.4721 - val_loss: 1.4947 - val_acc: 0.4793\n",
      "Epoch 103/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4804 - acc: 0.4779 - val_loss: 1.4926 - val_acc: 0.4789\n",
      "Epoch 104/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4914 - acc: 0.4722 - val_loss: 1.4882 - val_acc: 0.4805\n",
      "Epoch 105/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4807 - acc: 0.4776 - val_loss: 1.4874 - val_acc: 0.4807\n",
      "Epoch 106/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4838 - acc: 0.4759 - val_loss: 1.4920 - val_acc: 0.4794\n",
      "Epoch 107/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4789 - acc: 0.4784 - val_loss: 1.4866 - val_acc: 0.4802\n",
      "Epoch 108/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4764 - acc: 0.4777 - val_loss: 1.4887 - val_acc: 0.4794\n",
      "Epoch 109/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4774 - acc: 0.4816 - val_loss: 1.4939 - val_acc: 0.4779\n",
      "Epoch 110/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4901 - acc: 0.4686 - val_loss: 1.4919 - val_acc: 0.4781\n",
      "Epoch 111/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4797 - acc: 0.4774 - val_loss: 1.4774 - val_acc: 0.4832\n",
      "Epoch 112/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4864 - acc: 0.4764 - val_loss: 1.4970 - val_acc: 0.4748\n",
      "Epoch 113/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4790 - acc: 0.4786 - val_loss: 1.4808 - val_acc: 0.4835\n",
      "Epoch 114/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4827 - acc: 0.4774 - val_loss: 1.4890 - val_acc: 0.4782\n",
      "Epoch 115/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4844 - acc: 0.4727 - val_loss: 1.4964 - val_acc: 0.4750\n",
      "Epoch 116/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4855 - acc: 0.4771 - val_loss: 1.5082 - val_acc: 0.4720\n",
      "Epoch 117/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4833 - acc: 0.4749 - val_loss: 1.4956 - val_acc: 0.4784\n",
      "Epoch 118/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4824 - acc: 0.4775 - val_loss: 1.4935 - val_acc: 0.4762\n",
      "Epoch 119/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4849 - acc: 0.4731 - val_loss: 1.5000 - val_acc: 0.4742\n",
      "Epoch 120/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4811 - acc: 0.4766 - val_loss: 1.5028 - val_acc: 0.4756\n",
      "Epoch 121/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4844 - acc: 0.4758 - val_loss: 1.4951 - val_acc: 0.4780\n",
      "Epoch 122/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4871 - acc: 0.4724 - val_loss: 1.4898 - val_acc: 0.4809\n",
      "Epoch 123/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4833 - acc: 0.4769 - val_loss: 1.4917 - val_acc: 0.4777\n",
      "Epoch 124/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4864 - acc: 0.4762 - val_loss: 1.5040 - val_acc: 0.4736\n",
      "Epoch 125/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4810 - acc: 0.4759 - val_loss: 1.4979 - val_acc: 0.4782\n",
      "Epoch 126/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4852 - acc: 0.4758 - val_loss: 1.5022 - val_acc: 0.4750\n",
      "Epoch 127/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4794 - acc: 0.4767 - val_loss: 1.4897 - val_acc: 0.4787\n",
      "Epoch 128/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4879 - acc: 0.4737 - val_loss: 1.4922 - val_acc: 0.4796\n",
      "Epoch 129/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4786 - acc: 0.4781 - val_loss: 1.4841 - val_acc: 0.4825\n",
      "Epoch 130/130\n",
      "546/546 [==============================] - 11s 20ms/step - loss: 1.4880 - acc: 0.4753 - val_loss: 1.4891 - val_acc: 0.4783\n",
      "exe time:  1442.692500591278\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea13_finetuning_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model3.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea12_finetuning_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 64s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.497175493601896, 0.4761]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea13_finetuning_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.7731 - acc: 0.3716 - val_loss: 1.6663 - val_acc: 0.4135\n",
      "Epoch 2/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.6267 - acc: 0.4263 - val_loss: 1.5717 - val_acc: 0.4460\n",
      "Epoch 3/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5850 - acc: 0.4412 - val_loss: 1.5471 - val_acc: 0.4560\n",
      "Epoch 4/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5677 - acc: 0.4455 - val_loss: 1.5426 - val_acc: 0.4636\n",
      "Epoch 5/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5482 - acc: 0.4508 - val_loss: 1.5250 - val_acc: 0.4657\n",
      "Epoch 6/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5498 - acc: 0.4506 - val_loss: 1.5122 - val_acc: 0.4675\n",
      "Epoch 7/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5452 - acc: 0.4533 - val_loss: 1.5201 - val_acc: 0.4663\n",
      "Epoch 8/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5383 - acc: 0.4535 - val_loss: 1.5198 - val_acc: 0.4652\n",
      "Epoch 9/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5366 - acc: 0.4514 - val_loss: 1.5187 - val_acc: 0.4644\n",
      "Epoch 10/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5368 - acc: 0.4545 - val_loss: 1.5414 - val_acc: 0.4540\n",
      "Epoch 11/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5319 - acc: 0.4575 - val_loss: 1.5074 - val_acc: 0.4666\n",
      "Epoch 12/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5318 - acc: 0.4588 - val_loss: 1.5031 - val_acc: 0.4727\n",
      "Epoch 13/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5228 - acc: 0.4595 - val_loss: 1.5114 - val_acc: 0.4666\n",
      "Epoch 14/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5275 - acc: 0.4616 - val_loss: 1.5238 - val_acc: 0.4611\n",
      "Epoch 15/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5209 - acc: 0.4607 - val_loss: 1.4995 - val_acc: 0.4770\n",
      "Epoch 16/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5304 - acc: 0.4570 - val_loss: 1.5016 - val_acc: 0.4746\n",
      "Epoch 17/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5247 - acc: 0.4594 - val_loss: 1.5041 - val_acc: 0.4740\n",
      "Epoch 18/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5203 - acc: 0.4602 - val_loss: 1.4918 - val_acc: 0.4811\n",
      "Epoch 19/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5288 - acc: 0.4557 - val_loss: 1.5225 - val_acc: 0.4655\n",
      "Epoch 20/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5240 - acc: 0.4613 - val_loss: 1.5354 - val_acc: 0.4588\n",
      "Epoch 21/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5156 - acc: 0.4613 - val_loss: 1.4864 - val_acc: 0.4810\n",
      "Epoch 22/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5243 - acc: 0.4588 - val_loss: 1.5051 - val_acc: 0.4663\n",
      "Epoch 23/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5161 - acc: 0.4606 - val_loss: 1.4946 - val_acc: 0.4751\n",
      "Epoch 24/30\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5176 - acc: 0.4631 - val_loss: 1.5099 - val_acc: 0.4713\n",
      "Epoch 25/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5152 - acc: 0.4627 - val_loss: 1.5027 - val_acc: 0.4713\n",
      "Epoch 26/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5189 - acc: 0.4625 - val_loss: 1.4905 - val_acc: 0.4784\n",
      "Epoch 27/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5199 - acc: 0.4609 - val_loss: 1.4938 - val_acc: 0.4757\n",
      "Epoch 28/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5172 - acc: 0.4626 - val_loss: 1.5085 - val_acc: 0.4728\n",
      "Epoch 29/30\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5091 - acc: 0.4675 - val_loss: 1.4994 - val_acc: 0.4763\n",
      "Epoch 30/30\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5228 - acc: 0.4588 - val_loss: 1.5192 - val_acc: 0.4667\n",
      "exe time:  415.1235899925232\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea13_32_freezed_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model3.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea13_32_freezed_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_model('/data/Quan/datasets/cifar10/transfer_idea13_32_freezed_model.hdf5')\n",
    "\n",
    "for layer in model3.layers:\n",
    "    layer.trainable=True\n",
    "    \n",
    "for layer in model3.layers[1].layers:\n",
    "    layer.trainable=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.5222 - acc: 0.4599 - val_loss: 1.5082 - val_acc: 0.4741\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5263 - acc: 0.4577 - val_loss: 1.5052 - val_acc: 0.4701\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5202 - acc: 0.4614 - val_loss: 1.4996 - val_acc: 0.4733\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5217 - acc: 0.4591 - val_loss: 1.5137 - val_acc: 0.4691\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5205 - acc: 0.4601 - val_loss: 1.5042 - val_acc: 0.4740\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5153 - acc: 0.4617 - val_loss: 1.4985 - val_acc: 0.4767\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5153 - acc: 0.4628 - val_loss: 1.5056 - val_acc: 0.4707\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5168 - acc: 0.4641 - val_loss: 1.5077 - val_acc: 0.4714\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5159 - acc: 0.4610 - val_loss: 1.4936 - val_acc: 0.4757\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5189 - acc: 0.4619 - val_loss: 1.5053 - val_acc: 0.4738\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5201 - acc: 0.4640 - val_loss: 1.4994 - val_acc: 0.4745\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5191 - acc: 0.4625 - val_loss: 1.5000 - val_acc: 0.4743\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5176 - acc: 0.4606 - val_loss: 1.5139 - val_acc: 0.4647\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5128 - acc: 0.4644 - val_loss: 1.5098 - val_acc: 0.4725\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5129 - acc: 0.4623 - val_loss: 1.4912 - val_acc: 0.4774\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5187 - acc: 0.4611 - val_loss: 1.4891 - val_acc: 0.4782\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5142 - acc: 0.4633 - val_loss: 1.4974 - val_acc: 0.4729\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5123 - acc: 0.4642 - val_loss: 1.5064 - val_acc: 0.4697\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5153 - acc: 0.4685 - val_loss: 1.4915 - val_acc: 0.4757\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5119 - acc: 0.4663 - val_loss: 1.5134 - val_acc: 0.4708\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5158 - acc: 0.4633 - val_loss: 1.5024 - val_acc: 0.4727\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5140 - acc: 0.4656 - val_loss: 1.5009 - val_acc: 0.4767\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5167 - acc: 0.4638 - val_loss: 1.4939 - val_acc: 0.4759\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5150 - acc: 0.4650 - val_loss: 1.4983 - val_acc: 0.4749\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5166 - acc: 0.4588 - val_loss: 1.4924 - val_acc: 0.4753\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5110 - acc: 0.4630 - val_loss: 1.4928 - val_acc: 0.4768\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5127 - acc: 0.4661 - val_loss: 1.4906 - val_acc: 0.4762\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5115 - acc: 0.4621 - val_loss: 1.4969 - val_acc: 0.4745\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5162 - acc: 0.4651 - val_loss: 1.4957 - val_acc: 0.4713\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5098 - acc: 0.4695 - val_loss: 1.4967 - val_acc: 0.4735\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5102 - acc: 0.4661 - val_loss: 1.4970 - val_acc: 0.4792\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5117 - acc: 0.4618 - val_loss: 1.4869 - val_acc: 0.4830\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5149 - acc: 0.4627 - val_loss: 1.4878 - val_acc: 0.4793\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5079 - acc: 0.4660 - val_loss: 1.4987 - val_acc: 0.4747\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5133 - acc: 0.4665 - val_loss: 1.4958 - val_acc: 0.4768\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5099 - acc: 0.4661 - val_loss: 1.5037 - val_acc: 0.4745\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5115 - acc: 0.4662 - val_loss: 1.4941 - val_acc: 0.4757\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5095 - acc: 0.4659 - val_loss: 1.4985 - val_acc: 0.4749\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.5116 - acc: 0.4642 - val_loss: 1.4957 - val_acc: 0.4774\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5068 - acc: 0.4658 - val_loss: 1.4945 - val_acc: 0.4808\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5124 - acc: 0.4626 - val_loss: 1.4917 - val_acc: 0.4788\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5047 - acc: 0.4680 - val_loss: 1.4920 - val_acc: 0.4774\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5137 - acc: 0.4645 - val_loss: 1.4987 - val_acc: 0.4742\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5095 - acc: 0.4697 - val_loss: 1.4967 - val_acc: 0.4788\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5110 - acc: 0.4658 - val_loss: 1.4952 - val_acc: 0.4772\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5073 - acc: 0.4696 - val_loss: 1.4940 - val_acc: 0.4776\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5029 - acc: 0.4668 - val_loss: 1.4888 - val_acc: 0.4791\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5078 - acc: 0.4715 - val_loss: 1.5004 - val_acc: 0.4765\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5098 - acc: 0.4624 - val_loss: 1.4960 - val_acc: 0.4784\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5122 - acc: 0.4670 - val_loss: 1.4966 - val_acc: 0.4755\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5035 - acc: 0.4646 - val_loss: 1.5007 - val_acc: 0.4778\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5064 - acc: 0.4660 - val_loss: 1.4945 - val_acc: 0.4753\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5032 - acc: 0.4685 - val_loss: 1.4935 - val_acc: 0.4789\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5046 - acc: 0.4690 - val_loss: 1.4952 - val_acc: 0.4779\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5142 - acc: 0.4672 - val_loss: 1.4976 - val_acc: 0.4762\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5060 - acc: 0.4676 - val_loss: 1.4920 - val_acc: 0.4774\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5064 - acc: 0.4680 - val_loss: 1.4842 - val_acc: 0.4792\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5090 - acc: 0.4677 - val_loss: 1.4908 - val_acc: 0.4811\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5009 - acc: 0.4709 - val_loss: 1.5022 - val_acc: 0.4735\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5141 - acc: 0.4651 - val_loss: 1.4896 - val_acc: 0.4794\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5063 - acc: 0.4664 - val_loss: 1.4882 - val_acc: 0.4767\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5031 - acc: 0.4682 - val_loss: 1.4955 - val_acc: 0.4741\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5045 - acc: 0.4667 - val_loss: 1.4908 - val_acc: 0.4780\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5087 - acc: 0.4660 - val_loss: 1.4954 - val_acc: 0.4790\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5073 - acc: 0.4682 - val_loss: 1.4814 - val_acc: 0.4822\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5087 - acc: 0.4661 - val_loss: 1.4959 - val_acc: 0.4798\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5088 - acc: 0.4666 - val_loss: 1.4824 - val_acc: 0.4825\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5096 - acc: 0.4671 - val_loss: 1.4790 - val_acc: 0.4833\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5053 - acc: 0.4685 - val_loss: 1.4935 - val_acc: 0.4779\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5065 - acc: 0.4641 - val_loss: 1.4902 - val_acc: 0.4820\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5082 - acc: 0.4670 - val_loss: 1.4835 - val_acc: 0.4831\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5099 - acc: 0.4657 - val_loss: 1.5033 - val_acc: 0.4749\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5039 - acc: 0.4701 - val_loss: 1.4997 - val_acc: 0.4762\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.4985 - acc: 0.4702 - val_loss: 1.4957 - val_acc: 0.4764\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5077 - acc: 0.4677 - val_loss: 1.4931 - val_acc: 0.4768\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.4991 - acc: 0.4703 - val_loss: 1.4902 - val_acc: 0.4794\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5092 - acc: 0.4660 - val_loss: 1.4847 - val_acc: 0.4808\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5073 - acc: 0.4688 - val_loss: 1.4946 - val_acc: 0.4762\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5093 - acc: 0.4684 - val_loss: 1.4958 - val_acc: 0.4759\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5059 - acc: 0.4679 - val_loss: 1.4895 - val_acc: 0.4785\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5051 - acc: 0.4690 - val_loss: 1.4832 - val_acc: 0.4838\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5069 - acc: 0.4699 - val_loss: 1.4924 - val_acc: 0.4778\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5033 - acc: 0.4681 - val_loss: 1.4959 - val_acc: 0.4759\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5092 - acc: 0.4652 - val_loss: 1.4956 - val_acc: 0.4773\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5061 - acc: 0.4674 - val_loss: 1.4857 - val_acc: 0.4788\n",
      "Epoch 86/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5024 - acc: 0.4682 - val_loss: 1.4895 - val_acc: 0.4782\n",
      "Epoch 87/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5112 - acc: 0.4649 - val_loss: 1.4814 - val_acc: 0.4826\n",
      "Epoch 88/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5043 - acc: 0.4656 - val_loss: 1.4905 - val_acc: 0.4788\n",
      "Epoch 89/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5126 - acc: 0.4668 - val_loss: 1.4962 - val_acc: 0.4782\n",
      "Epoch 90/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5083 - acc: 0.4635 - val_loss: 1.4990 - val_acc: 0.4763\n",
      "Epoch 91/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5049 - acc: 0.4689 - val_loss: 1.4913 - val_acc: 0.4780\n",
      "Epoch 92/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5084 - acc: 0.4682 - val_loss: 1.4891 - val_acc: 0.4763\n",
      "Epoch 93/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5041 - acc: 0.4684 - val_loss: 1.4871 - val_acc: 0.4788\n",
      "Epoch 94/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5060 - acc: 0.4676 - val_loss: 1.4928 - val_acc: 0.4800\n",
      "Epoch 95/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5043 - acc: 0.4692 - val_loss: 1.4942 - val_acc: 0.4816\n",
      "Epoch 96/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5035 - acc: 0.4668 - val_loss: 1.4936 - val_acc: 0.4789\n",
      "Epoch 97/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5073 - acc: 0.4699 - val_loss: 1.4932 - val_acc: 0.4794\n",
      "Epoch 98/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5000 - acc: 0.4689 - val_loss: 1.4930 - val_acc: 0.4797\n",
      "Epoch 99/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5087 - acc: 0.4647 - val_loss: 1.4915 - val_acc: 0.4807\n",
      "Epoch 100/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5064 - acc: 0.4661 - val_loss: 1.4840 - val_acc: 0.4809\n",
      "Epoch 101/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.4992 - acc: 0.4692 - val_loss: 1.5057 - val_acc: 0.4705\n",
      "Epoch 102/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5017 - acc: 0.4680 - val_loss: 1.4884 - val_acc: 0.4806\n",
      "Epoch 103/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5038 - acc: 0.4688 - val_loss: 1.4897 - val_acc: 0.4800\n",
      "Epoch 104/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5092 - acc: 0.4664 - val_loss: 1.4865 - val_acc: 0.4793\n",
      "Epoch 105/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5029 - acc: 0.4682 - val_loss: 1.4907 - val_acc: 0.4784\n",
      "Epoch 106/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5033 - acc: 0.4678 - val_loss: 1.4826 - val_acc: 0.4833\n",
      "Epoch 107/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5039 - acc: 0.4701 - val_loss: 1.4900 - val_acc: 0.4780\n",
      "Epoch 108/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5047 - acc: 0.4660 - val_loss: 1.4979 - val_acc: 0.4767\n",
      "Epoch 109/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5061 - acc: 0.4666 - val_loss: 1.4971 - val_acc: 0.4759\n",
      "Epoch 110/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5049 - acc: 0.4689 - val_loss: 1.4924 - val_acc: 0.4786\n",
      "Epoch 111/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5020 - acc: 0.4675 - val_loss: 1.4973 - val_acc: 0.4771\n",
      "Epoch 112/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.4970 - acc: 0.4701 - val_loss: 1.4910 - val_acc: 0.4798\n",
      "Epoch 113/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5024 - acc: 0.4696 - val_loss: 1.4972 - val_acc: 0.4772\n",
      "Epoch 114/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5079 - acc: 0.4669 - val_loss: 1.4876 - val_acc: 0.4794\n",
      "Epoch 115/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5078 - acc: 0.4656 - val_loss: 1.4869 - val_acc: 0.4792\n",
      "Epoch 116/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5006 - acc: 0.4664 - val_loss: 1.4896 - val_acc: 0.4800\n",
      "Epoch 117/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5080 - acc: 0.4671 - val_loss: 1.4978 - val_acc: 0.4754\n",
      "Epoch 118/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5019 - acc: 0.4666 - val_loss: 1.4931 - val_acc: 0.4773\n",
      "Epoch 119/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5002 - acc: 0.4716 - val_loss: 1.4836 - val_acc: 0.4828\n",
      "Epoch 120/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5047 - acc: 0.4661 - val_loss: 1.4901 - val_acc: 0.4782\n",
      "Epoch 121/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5038 - acc: 0.4689 - val_loss: 1.4972 - val_acc: 0.4765\n",
      "Epoch 122/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5108 - acc: 0.4675 - val_loss: 1.4940 - val_acc: 0.4775\n",
      "Epoch 123/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5084 - acc: 0.4665 - val_loss: 1.4933 - val_acc: 0.4796\n",
      "Epoch 124/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5037 - acc: 0.4683 - val_loss: 1.5020 - val_acc: 0.4741\n",
      "Epoch 125/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5028 - acc: 0.4690 - val_loss: 1.4975 - val_acc: 0.4777\n",
      "Epoch 126/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.4976 - acc: 0.4706 - val_loss: 1.4957 - val_acc: 0.4757\n",
      "Epoch 127/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5035 - acc: 0.4677 - val_loss: 1.4940 - val_acc: 0.4754\n",
      "Epoch 128/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5051 - acc: 0.4682 - val_loss: 1.4963 - val_acc: 0.4759\n",
      "Epoch 129/130\n",
      "1093/1093 [==============================] - 14s 12ms/step - loss: 1.5066 - acc: 0.4667 - val_loss: 1.4875 - val_acc: 0.4790\n",
      "Epoch 130/130\n",
      "1093/1093 [==============================] - 13s 12ms/step - loss: 1.5054 - acc: 0.4688 - val_loss: 1.5007 - val_acc: 0.4747\n",
      "exe time:  1752.4261074066162\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "test_generator.reset()\n",
    "validation_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea13_32_finetuning_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model3.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea12_32_finetuning_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/saving.py:327: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 69s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4958488912238042, 0.4778]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea13_32_finetuning_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 4\n",
    "\n",
    "Use extracted features from output of each conv block and feed to corresponding conv blocks of new model (concatenate with the current ones). The pre-train model can be trainable or non-trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SkippedVGG_for_transfer(nb_blocks, input_layer, nb_layers, nb_neurons, extra_inputs, verbose=1):\n",
    "    concats = []\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        concats.append([])\n",
    "        \n",
    "    if type(nb_layers) is int:\n",
    "        temp = nb_layers\n",
    "        nb_layers = []\n",
    "        for i in range(nb_blocks):\n",
    "            nb_layers.append(temp)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Create DensedVGG model with ' + str(nb_blocks) + ' blocks, input layer = ', input_layer)\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        if verbose:\n",
    "            print('Create block ' + str(i) + ':')\n",
    "        \n",
    "        if i == 0: # First block\n",
    "            b = input_layer\n",
    "            inputs = b\n",
    "        \n",
    "        # check all layers before\n",
    "        if len(concats[i-1]) > 1:\n",
    "            if (i-1) < len(extra_inputs):\n",
    "                print('Add extra inputs ', extra_inputs[i-1])\n",
    "                temp_concats = concats[i-1]\n",
    "                temp_concats.append(extra_inputs[i-1])\n",
    "                b = concatenate(temp_concats)\n",
    "            else:   \n",
    "                print('No extra inputs at block ', i)\n",
    "                b = concatenate(concats[i-1])\n",
    "        elif len(concats[i-1]) == 1:\n",
    "            if verbose:\n",
    "                print('Get direct output from the previous block ', concats[i-1])\n",
    "#             b = concats[i-1][0]\n",
    "            if (i-1) < len(extra_inputs):\n",
    "                print('Add extra inputs ', extra_inputs[i-1])\n",
    "                temp_concats = concats[i-1]\n",
    "                temp_concats.append(extra_inputs[i-1])\n",
    "                b = concatenate(temp_concats)\n",
    "            else:   \n",
    "                print('No extra inputs at block ', i)\n",
    "                b = concatenate(concats[i-1][0])\n",
    "            \n",
    "        # create main block\n",
    "        b = define_block(b, nb_layers[i], nb_neurons[i], index=i)\n",
    "        concats[i].append(b)\n",
    "        \n",
    "        \n",
    "        # create cropping layer\n",
    "        for j in range(i+1, nb_blocks):\n",
    "            if verbose:\n",
    "                print('-- create skipped connection from block '+ str(i) + ' to block ' + str(j+1) + ' ...')\n",
    "                \n",
    "            src_shape = b.get_shape()\n",
    "            src_shape = (int(src_shape[1]), int(src_shape[2]), int(src_shape[3]))\n",
    "            dst_shape = (src_shape[0]//2**(j-i), src_shape[1]//2**(j-i), src_shape[2])\n",
    "            \n",
    "            print(src_shape, dst_shape)\n",
    "            h = src_shape[0] - dst_shape[0]\n",
    "            w = src_shape[1] - dst_shape[1]\n",
    "            if h % 2 == 0:\n",
    "                h_0 = h // 2\n",
    "                h_1 = h // 2\n",
    "            else:\n",
    "                h_0 = h // 2\n",
    "                h_1 = (h // 2) + 1\n",
    "            \n",
    "            if w % 2 == 0:\n",
    "                w_0 = w // 2\n",
    "                w_1 = w // 2\n",
    "            else:\n",
    "                w_0 = w // 2\n",
    "                w_1 = (w // 2) + 1\n",
    "                \n",
    "            concat_layer = Cropping2D(((h_0,h_1),(w_0,w_1)), name='cropping_block' + str(i) + '_block' + str(j+1))(b)\n",
    "            concats[j].append(concat_layer)\n",
    "            \n",
    "    if len(concats[nb_blocks-1]) == 1:\n",
    "        outputs = concats[nb_blocks-1]\n",
    "    else:\n",
    "        outputs = concatenate(concats[nb_blocks-1])\n",
    "        \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_28/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_28/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_44:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_19/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_28/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_28/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_44 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_28 (Model)                multiple             260160      input_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_45 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_28[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_46 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_28[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_47 (Concatenate)    (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_46 (Gl (None, 704)          0           concatenate_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 10)           7050        global_average_pooling2d_46[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 10)           40          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 10)           0           batch_normalization_10[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 6,803,506\n",
      "Trainable params: 6,799,646\n",
      "Non-trainable params: 3,860\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 32s 29ms/step - loss: 1.5928 - acc: 0.4375 - val_loss: 1.7211 - val_acc: 0.3948\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.3523 - acc: 0.5264 - val_loss: 1.6106 - val_acc: 0.4675\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.2308 - acc: 0.5701 - val_loss: 1.2443 - val_acc: 0.5704\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.1528 - acc: 0.5985 - val_loss: 1.2667 - val_acc: 0.5607\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0872 - acc: 0.6218 - val_loss: 1.3130 - val_acc: 0.5583\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0274 - acc: 0.6395 - val_loss: 1.1442 - val_acc: 0.6069\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.9757 - acc: 0.6605 - val_loss: 1.2506 - val_acc: 0.5706\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9326 - acc: 0.6743 - val_loss: 1.0801 - val_acc: 0.6384\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9014 - acc: 0.6858 - val_loss: 0.9901 - val_acc: 0.6588\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8655 - acc: 0.6984 - val_loss: 0.9616 - val_acc: 0.6684\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8310 - acc: 0.7119 - val_loss: 0.8743 - val_acc: 0.6998\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7996 - acc: 0.7232 - val_loss: 0.8387 - val_acc: 0.7085\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7674 - acc: 0.7344 - val_loss: 0.9994 - val_acc: 0.6637\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7468 - acc: 0.7430 - val_loss: 1.1310 - val_acc: 0.6292\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.7273 - acc: 0.7465 - val_loss: 0.8953 - val_acc: 0.6989\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7068 - acc: 0.7543 - val_loss: 0.8610 - val_acc: 0.7104\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6837 - acc: 0.7607 - val_loss: 0.9191 - val_acc: 0.6925\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6630 - acc: 0.7723 - val_loss: 0.7626 - val_acc: 0.7479\n",
      "Epoch 19/130\n",
      "1024/1093 [===========================>..] - ETA: 1s - loss: 0.6419 - acc: 0.7785"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea14_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea14_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exe time:  3568.3500208854675\n"
     ]
    }
   ],
   "source": [
    "print('exe time: ', exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 72s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.698373294539816, 0.8193]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea14_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4.2\n",
    "Using more than 3 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_42/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_42/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>, <tf.Tensor 'model_42/block3_pool/MaxPool:0' shape=(?, 4, 4, 256) dtype=float32>, <tf.Tensor 'model_42/block4_pool/MaxPool:0' shape=(?, 2, 2, 512) dtype=float32>]\n",
      "Create DensedVGG model with 5 blocks, input layer =  Tensor(\"input_64:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "-- create skipped connection from block 0 to block 4 ...\n",
      "(16, 16, 64) (2, 2, 64)\n",
      "-- create skipped connection from block 0 to block 5 ...\n",
      "(16, 16, 64) (1, 1, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_32/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_42/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "-- create skipped connection from block 1 to block 4 ...\n",
      "(8, 8, 128) (2, 2, 128)\n",
      "-- create skipped connection from block 1 to block 5 ...\n",
      "(8, 8, 128) (1, 1, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_42/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "-- create skipped connection from block 2 to block 4 ...\n",
      "(4, 4, 512) (2, 2, 512)\n",
      "-- create skipped connection from block 2 to block 5 ...\n",
      "(4, 4, 512) (1, 1, 512)\n",
      "Create block 3:\n",
      "Add extra inputs  Tensor(\"model_42/block3_pool/MaxPool:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      "-- create skipped connection from block 3 to block 5 ...\n",
      "(2, 2, 512) (1, 1, 512)\n",
      "Create block 4:\n",
      "Add extra inputs  Tensor(\"model_42/block4_pool/MaxPool:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_64 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_42 (Model)                multiple             7635264     input_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_85 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_42[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_86 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_42[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_87 (Concatenate)    (None, 4, 4, 960)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "                                                                 model_42[1][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block3 (Conv2D)           (None, 4, 4, 512)    4424192     concatenate_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block3 (BatchNormali (None, 4, 4, 512)    2048        conv0_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm0_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block3 (Conv2D)           (None, 4, 4, 512)    2359808     relu0_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block3 (BatchNormali (None, 4, 4, 512)    2048        conv1_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm1_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3 (Conv2D)           (None, 4, 4, 512)    2359808     relu1_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block3 (BatchNormali (None, 4, 4, 512)    2048        conv2_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm2_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block3 (MaxPooling2D (None, 2, 2, 512)    0           relu2_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block4 (Croppin (None, 2, 2, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block4 (Croppin (None, 2, 2, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block2_block4 (Croppin (None, 2, 2, 512)    0           maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_88 (Concatenate)    (None, 2, 2, 1728)   0           cropping_block0_block4[0][0]     \n",
      "                                                                 cropping_block1_block4[0][0]     \n",
      "                                                                 cropping_block2_block4[0][0]     \n",
      "                                                                 maxpooling_block3[0][0]          \n",
      "                                                                 model_42[1][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block4 (Conv2D)           (None, 2, 2, 512)    7963136     concatenate_88[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block4 (BatchNormali (None, 2, 2, 512)    2048        conv0_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm0_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block4 (Conv2D)           (None, 2, 2, 512)    2359808     relu0_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block4 (BatchNormali (None, 2, 2, 512)    2048        conv1_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm1_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4 (Conv2D)           (None, 2, 2, 512)    2359808     relu1_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block4 (BatchNormali (None, 2, 2, 512)    2048        conv2_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm2_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block5 (Croppin (None, 1, 1, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block5 (Croppin (None, 1, 1, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block2_block5 (Croppin (None, 1, 1, 512)    0           maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block3_block5 (Croppin (None, 1, 1, 512)    0           maxpooling_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block4 (MaxPooling2D (None, 1, 1, 512)    0           relu2_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_89 (Concatenate)    (None, 1, 1, 1728)   0           cropping_block0_block5[0][0]     \n",
      "                                                                 cropping_block1_block5[0][0]     \n",
      "                                                                 cropping_block2_block5[0][0]     \n",
      "                                                                 cropping_block3_block5[0][0]     \n",
      "                                                                 maxpooling_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_71 (Gl (None, 1728)         0           concatenate_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 10)           17290       global_average_pooling2d_71[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 10)           40          dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 10)           0           batch_normalization_14[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 36,027,698\n",
      "Trainable params: 36,017,694\n",
      "Non-trainable params: 10,004\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:4])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=5, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3,3,3], nb_neurons=[64,128,512,512,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 87s 80ms/step - loss: 1.6047 - acc: 0.4306 - val_loss: 2.1076 - val_acc: 0.3281\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 1.3459 - acc: 0.5239 - val_loss: 1.4224 - val_acc: 0.5111\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 1.2328 - acc: 0.5678 - val_loss: 1.2323 - val_acc: 0.5680\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 1.1512 - acc: 0.5996 - val_loss: 1.4612 - val_acc: 0.5098\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 1.0822 - acc: 0.6235 - val_loss: 1.1208 - val_acc: 0.6140\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 1.0269 - acc: 0.6435 - val_loss: 1.1049 - val_acc: 0.6158\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.9726 - acc: 0.6630 - val_loss: 1.4017 - val_acc: 0.5520\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.9309 - acc: 0.6757 - val_loss: 1.1100 - val_acc: 0.6259\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.8982 - acc: 0.6902 - val_loss: 0.9477 - val_acc: 0.6733\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.8547 - acc: 0.7040 - val_loss: 0.9266 - val_acc: 0.6885\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.8211 - acc: 0.7180 - val_loss: 1.0337 - val_acc: 0.6536\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.7928 - acc: 0.7298 - val_loss: 0.8539 - val_acc: 0.7104\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.7684 - acc: 0.7335 - val_loss: 0.9311 - val_acc: 0.6833\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.7338 - acc: 0.7452 - val_loss: 1.0535 - val_acc: 0.6537\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.7136 - acc: 0.7568 - val_loss: 0.8121 - val_acc: 0.7253\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.6909 - acc: 0.7615 - val_loss: 0.7728 - val_acc: 0.7355\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.6657 - acc: 0.7716 - val_loss: 0.8588 - val_acc: 0.7115\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.6490 - acc: 0.7754 - val_loss: 0.7986 - val_acc: 0.7322\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.6278 - acc: 0.7844 - val_loss: 0.7793 - val_acc: 0.7348\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.6075 - acc: 0.7921 - val_loss: 0.7693 - val_acc: 0.7400\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.5879 - acc: 0.7997 - val_loss: 0.8108 - val_acc: 0.7277\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.5702 - acc: 0.8045 - val_loss: 0.7135 - val_acc: 0.7614\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.5560 - acc: 0.8109 - val_loss: 0.8598 - val_acc: 0.7175\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.5414 - acc: 0.8148 - val_loss: 0.7593 - val_acc: 0.7478\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.5225 - acc: 0.8234 - val_loss: 0.7326 - val_acc: 0.7546\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.5115 - acc: 0.8269 - val_loss: 0.7148 - val_acc: 0.7628\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.4959 - acc: 0.8316 - val_loss: 0.7360 - val_acc: 0.7576\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.4841 - acc: 0.8362 - val_loss: 0.7815 - val_acc: 0.7485\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.4683 - acc: 0.8414 - val_loss: 0.7075 - val_acc: 0.7713\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.4532 - acc: 0.8440 - val_loss: 0.6958 - val_acc: 0.7724\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.4419 - acc: 0.8510 - val_loss: 0.6961 - val_acc: 0.7751\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.4308 - acc: 0.8537 - val_loss: 0.7978 - val_acc: 0.7484\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.4249 - acc: 0.8560 - val_loss: 0.7182 - val_acc: 0.7714\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.4038 - acc: 0.8639 - val_loss: 0.7213 - val_acc: 0.7762\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3954 - acc: 0.8660 - val_loss: 0.6875 - val_acc: 0.7828\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3860 - acc: 0.8686 - val_loss: 0.6943 - val_acc: 0.7813\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.3807 - acc: 0.8709 - val_loss: 0.7521 - val_acc: 0.7658\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3674 - acc: 0.8751 - val_loss: 0.7302 - val_acc: 0.7709\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.3644 - acc: 0.8762 - val_loss: 0.7110 - val_acc: 0.7782\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.3531 - acc: 0.8806 - val_loss: 0.7214 - val_acc: 0.7722\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.3394 - acc: 0.8852 - val_loss: 0.7503 - val_acc: 0.7666\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3216 - acc: 0.8922 - val_loss: 0.6895 - val_acc: 0.7851\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3217 - acc: 0.8927 - val_loss: 0.8307 - val_acc: 0.7545:\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3141 - acc: 0.8928 - val_loss: 0.7157 - val_acc: 0.7837\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.3047 - acc: 0.8959 - val_loss: 0.7691 - val_acc: 0.7675\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.3030 - acc: 0.8978 - val_loss: 0.7164 - val_acc: 0.7812\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2886 - acc: 0.9030 - val_loss: 0.7151 - val_acc: 0.7809\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2773 - acc: 0.9073 - val_loss: 0.7969 - val_acc: 0.7658\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2761 - acc: 0.9073 - val_loss: 0.6889 - val_acc: 0.7984\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2718 - acc: 0.9094 - val_loss: 0.7480 - val_acc: 0.7796\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.2639 - acc: 0.9117 - val_loss: 0.7828 - val_acc: 0.7708\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2592 - acc: 0.9121 - val_loss: 0.7738 - val_acc: 0.7677\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2490 - acc: 0.9155 - val_loss: 0.7526 - val_acc: 0.7833\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2380 - acc: 0.9200 - val_loss: 0.7450 - val_acc: 0.7841\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 79s 73ms/step - loss: 0.2349 - acc: 0.9210 - val_loss: 0.7471 - val_acc: 0.7820\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2326 - acc: 0.9225 - val_loss: 0.7881 - val_acc: 0.7773\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2234 - acc: 0.9251 - val_loss: 0.8144 - val_acc: 0.7716\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 80s 73ms/step - loss: 0.2255 - acc: 0.9240 - val_loss: 0.7877 - val_acc: 0.7761\n",
      "Epoch 59/130\n",
      " 835/1093 [=====================>........] - ETA: 17s - loss: 0.2181 - acc: 0.9288"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea14.2_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea14.2_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10431.521920204163"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 122s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.877768070437959, 0.7949]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea14.2_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4.3\n",
    "\n",
    "freeze all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_46/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_46/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_68:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_35/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_46/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_46/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_68 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_46 (Model)                multiple             260160      input_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_93 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_46[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_93[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_94 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_46[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_94[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_95 (Concatenate)    (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_73 (Gl (None, 704)          0           concatenate_95[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 10)           7050        global_average_pooling2d_73[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 10)           40          dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 10)           0           batch_normalization_16[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 6,803,506\n",
      "Trainable params: 6,539,486\n",
      "Non-trainable params: 264,020\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in pretrain_vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 34s 31ms/step - loss: 1.5558 - acc: 0.4526 - val_loss: 1.6641 - val_acc: 0.4422\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 1.3389 - acc: 0.5320 - val_loss: 1.3510 - val_acc: 0.5196\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.2315 - acc: 0.5678 - val_loss: 1.4610 - val_acc: 0.4868\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 1.1466 - acc: 0.5950 - val_loss: 1.3095 - val_acc: 0.5605\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 1.0895 - acc: 0.6165 - val_loss: 1.0113 - val_acc: 0.6426\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0395 - acc: 0.6337 - val_loss: 1.1144 - val_acc: 0.6211\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0003 - acc: 0.6490 - val_loss: 1.0834 - val_acc: 0.6269\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9616 - acc: 0.6629 - val_loss: 0.9622 - val_acc: 0.6618\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.9309 - acc: 0.6738 - val_loss: 0.9494 - val_acc: 0.6716\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9030 - acc: 0.6851 - val_loss: 0.9964 - val_acc: 0.6593\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8724 - acc: 0.6966 - val_loss: 0.8973 - val_acc: 0.6871\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.8468 - acc: 0.7064 - val_loss: 0.8955 - val_acc: 0.6901\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8291 - acc: 0.7116 - val_loss: 0.8770 - val_acc: 0.7054\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7972 - acc: 0.7246 - val_loss: 0.9633 - val_acc: 0.6705\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.7783 - acc: 0.7286 - val_loss: 0.9597 - val_acc: 0.6659\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.7579 - acc: 0.7365 - val_loss: 1.2670 - val_acc: 0.5985\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7422 - acc: 0.7425 - val_loss: 0.7966 - val_acc: 0.7239\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7163 - acc: 0.7520 - val_loss: 0.8344 - val_acc: 0.7105\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7029 - acc: 0.7562 - val_loss: 0.9058 - val_acc: 0.6934\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6817 - acc: 0.7609 - val_loss: 0.8493 - val_acc: 0.7161\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6601 - acc: 0.7694 - val_loss: 0.9472 - val_acc: 0.6906\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.6467 - acc: 0.7742 - val_loss: 0.8845 - val_acc: 0.6993\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6253 - acc: 0.7849 - val_loss: 1.0758 - val_acc: 0.6595\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6175 - acc: 0.7846 - val_loss: 0.8060 - val_acc: 0.7286\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5985 - acc: 0.7935 - val_loss: 0.7718 - val_acc: 0.7393\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5824 - acc: 0.7968 - val_loss: 0.8043 - val_acc: 0.7304\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5685 - acc: 0.8038 - val_loss: 0.8246 - val_acc: 0.7270\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5497 - acc: 0.8102 - val_loss: 0.8445 - val_acc: 0.7212\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.5399 - acc: 0.8150 - val_loss: 0.8736 - val_acc: 0.7106\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.5240 - acc: 0.8197 - val_loss: 0.8218 - val_acc: 0.7292\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5080 - acc: 0.8233 - val_loss: 0.7233 - val_acc: 0.7575\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4926 - acc: 0.8297 - val_loss: 0.7746 - val_acc: 0.7449\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.4859 - acc: 0.8318 - val_loss: 0.7944 - val_acc: 0.7422\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4765 - acc: 0.8326 - val_loss: 0.7302 - val_acc: 0.7567\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4523 - acc: 0.8441 - val_loss: 0.8593 - val_acc: 0.7278\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4514 - acc: 0.8435 - val_loss: 0.8299 - val_acc: 0.7350\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.4321 - acc: 0.8506 - val_loss: 0.8219 - val_acc: 0.7427\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4212 - acc: 0.8538 - val_loss: 0.8216 - val_acc: 0.7409\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4055 - acc: 0.8593 - val_loss: 0.8672 - val_acc: 0.7277\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4004 - acc: 0.8627 - val_loss: 0.7744 - val_acc: 0.7523\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3963 - acc: 0.8623 - val_loss: 0.8232 - val_acc: 0.7358\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3765 - acc: 0.8706 - val_loss: 0.7331 - val_acc: 0.7677\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3715 - acc: 0.8718 - val_loss: 0.7689 - val_acc: 0.7509\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3633 - acc: 0.8755 - val_loss: 0.7952 - val_acc: 0.7498\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 27s 24ms/step - loss: 0.3456 - acc: 0.8829 - val_loss: 0.7694 - val_acc: 0.7579\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3377 - acc: 0.8851 - val_loss: 0.8064 - val_acc: 0.7498\n",
      "Epoch 47/130\n",
      " 352/1093 [========>.....................] - ETA: 15s - loss: 0.3218 - acc: 0.8900"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea14.3_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea14.3_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3528.7938346862793"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 50s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.9533610496851498, 0.7807]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea14.3_model.hdf5')\n",
    "test_generator.reset()\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4.4\n",
    "freeze all layers and use 5 blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_48/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_48/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>, <tf.Tensor 'model_48/block3_pool/MaxPool:0' shape=(?, 4, 4, 256) dtype=float32>, <tf.Tensor 'model_48/block4_pool/MaxPool:0' shape=(?, 2, 2, 512) dtype=float32>]\n",
      "Create DensedVGG model with 5 blocks, input layer =  Tensor(\"input_70:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "-- create skipped connection from block 0 to block 4 ...\n",
      "(16, 16, 64) (2, 2, 64)\n",
      "-- create skipped connection from block 0 to block 5 ...\n",
      "(16, 16, 64) (1, 1, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_37/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_48/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "-- create skipped connection from block 1 to block 4 ...\n",
      "(8, 8, 128) (2, 2, 128)\n",
      "-- create skipped connection from block 1 to block 5 ...\n",
      "(8, 8, 128) (1, 1, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_48/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "-- create skipped connection from block 2 to block 4 ...\n",
      "(4, 4, 512) (2, 2, 512)\n",
      "-- create skipped connection from block 2 to block 5 ...\n",
      "(4, 4, 512) (1, 1, 512)\n",
      "Create block 3:\n",
      "Add extra inputs  Tensor(\"model_48/block3_pool/MaxPool:0\", shape=(?, 4, 4, 256), dtype=float32)\n",
      "-- create skipped connection from block 3 to block 5 ...\n",
      "(2, 2, 512) (1, 1, 512)\n",
      "Create block 4:\n",
      "Add extra inputs  Tensor(\"model_48/block4_pool/MaxPool:0\", shape=(?, 2, 2, 512), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_70 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_48 (Model)                multiple             7635264     input_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_96 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_48[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_96[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_97 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_48[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_97[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_98 (Concatenate)    (None, 4, 4, 960)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "                                                                 model_48[1][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block3 (Conv2D)           (None, 4, 4, 512)    4424192     concatenate_98[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block3 (BatchNormali (None, 4, 4, 512)    2048        conv0_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm0_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block3 (Conv2D)           (None, 4, 4, 512)    2359808     relu0_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block3 (BatchNormali (None, 4, 4, 512)    2048        conv1_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm1_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3 (Conv2D)           (None, 4, 4, 512)    2359808     relu1_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block3 (BatchNormali (None, 4, 4, 512)    2048        conv2_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block3 (Activation)       (None, 4, 4, 512)    0           batchnorm2_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block3 (MaxPooling2D (None, 2, 2, 512)    0           relu2_block3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block4 (Croppin (None, 2, 2, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block4 (Croppin (None, 2, 2, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block2_block4 (Croppin (None, 2, 2, 512)    0           maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_99 (Concatenate)    (None, 2, 2, 1728)   0           cropping_block0_block4[0][0]     \n",
      "                                                                 cropping_block1_block4[0][0]     \n",
      "                                                                 cropping_block2_block4[0][0]     \n",
      "                                                                 maxpooling_block3[0][0]          \n",
      "                                                                 model_48[1][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block4 (Conv2D)           (None, 2, 2, 512)    7963136     concatenate_99[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block4 (BatchNormali (None, 2, 2, 512)    2048        conv0_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm0_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block4 (Conv2D)           (None, 2, 2, 512)    2359808     relu0_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block4 (BatchNormali (None, 2, 2, 512)    2048        conv1_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm1_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block4 (Conv2D)           (None, 2, 2, 512)    2359808     relu1_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block4 (BatchNormali (None, 2, 2, 512)    2048        conv2_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block4 (Activation)       (None, 2, 2, 512)    0           batchnorm2_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block5 (Croppin (None, 1, 1, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block5 (Croppin (None, 1, 1, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block2_block5 (Croppin (None, 1, 1, 512)    0           maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block3_block5 (Croppin (None, 1, 1, 512)    0           maxpooling_block3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block4 (MaxPooling2D (None, 1, 1, 512)    0           relu2_block4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_100 (Concatenate)   (None, 1, 1, 1728)   0           cropping_block0_block5[0][0]     \n",
      "                                                                 cropping_block1_block5[0][0]     \n",
      "                                                                 cropping_block2_block5[0][0]     \n",
      "                                                                 cropping_block3_block5[0][0]     \n",
      "                                                                 maxpooling_block4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_74 (Gl (None, 1728)         0           concatenate_100[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 10)           17290       global_average_pooling2d_74[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 10)           40          dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 10)           0           batch_normalization_17[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 36,027,698\n",
      "Trainable params: 28,382,430\n",
      "Non-trainable params: 7,645,268\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in pretrain_vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:4])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=5, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3,3,3], nb_neurons=[64,128,512,512,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 77s 71ms/step - loss: 1.4318 - acc: 0.5020 - val_loss: 1.4810 - val_acc: 0.4943\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 1.2335 - acc: 0.5709 - val_loss: 1.2646 - val_acc: 0.5509\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 1.1517 - acc: 0.5982 - val_loss: 1.2733 - val_acc: 0.5616\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 1.0897 - acc: 0.6206 - val_loss: 1.1574 - val_acc: 0.5985\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 1.0366 - acc: 0.6370 - val_loss: 0.9580 - val_acc: 0.6669\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 1.0068 - acc: 0.6502 - val_loss: 1.0506 - val_acc: 0.6315\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.9546 - acc: 0.6685 - val_loss: 0.9184 - val_acc: 0.6793\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.9260 - acc: 0.6777 - val_loss: 0.9654 - val_acc: 0.6666\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.8916 - acc: 0.6904 - val_loss: 0.9503 - val_acc: 0.6658\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.8651 - acc: 0.6975 - val_loss: 0.9218 - val_acc: 0.6855\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.8331 - acc: 0.7098 - val_loss: 0.8545 - val_acc: 0.7075\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.8107 - acc: 0.7163 - val_loss: 0.9361 - val_acc: 0.6855\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.7865 - acc: 0.7291 - val_loss: 0.8927 - val_acc: 0.6979\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.7558 - acc: 0.7378 - val_loss: 0.8947 - val_acc: 0.6970\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.7355 - acc: 0.7466 - val_loss: 0.8005 - val_acc: 0.7264\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.7166 - acc: 0.7511 - val_loss: 0.9219 - val_acc: 0.6881\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.6923 - acc: 0.7621 - val_loss: 0.8287 - val_acc: 0.7187\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.6651 - acc: 0.7704 - val_loss: 0.7371 - val_acc: 0.7488\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.6492 - acc: 0.7778 - val_loss: 0.7415 - val_acc: 0.7467\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.6288 - acc: 0.7850 - val_loss: 0.7582 - val_acc: 0.7454\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.6065 - acc: 0.7889 - val_loss: 0.8694 - val_acc: 0.7095\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.5958 - acc: 0.7945 - val_loss: 0.7291 - val_acc: 0.7539\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.5725 - acc: 0.8021 - val_loss: 0.8280 - val_acc: 0.7283\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.5511 - acc: 0.8120 - val_loss: 0.7868 - val_acc: 0.7377\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.5348 - acc: 0.8166 - val_loss: 0.7378 - val_acc: 0.7573\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.5129 - acc: 0.8216 - val_loss: 0.7591 - val_acc: 0.7498\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.4969 - acc: 0.8305 - val_loss: 0.7510 - val_acc: 0.7529\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.4825 - acc: 0.8353 - val_loss: 0.7451 - val_acc: 0.7544\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.4615 - acc: 0.8421 - val_loss: 0.7947 - val_acc: 0.7450\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.4474 - acc: 0.8494 - val_loss: 0.7272 - val_acc: 0.7679\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.4218 - acc: 0.8587 - val_loss: 0.8130 - val_acc: 0.7429\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.4055 - acc: 0.8620 - val_loss: 0.7994 - val_acc: 0.7507\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.3909 - acc: 0.8661 - val_loss: 0.8250 - val_acc: 0.7445\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.3803 - acc: 0.8710 - val_loss: 0.8503 - val_acc: 0.7350\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.3663 - acc: 0.8753 - val_loss: 0.7485 - val_acc: 0.7646\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.3506 - acc: 0.8810 - val_loss: 0.7807 - val_acc: 0.7628\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.3328 - acc: 0.8878 - val_loss: 0.7760 - val_acc: 0.7576\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.3201 - acc: 0.8938 - val_loss: 0.7638 - val_acc: 0.7669\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.3144 - acc: 0.8930 - val_loss: 0.8598 - val_acc: 0.7469\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.2983 - acc: 0.8984 - val_loss: 0.8382 - val_acc: 0.7535\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2859 - acc: 0.9031 - val_loss: 0.8126 - val_acc: 0.7610\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.2779 - acc: 0.9066 - val_loss: 0.8218 - val_acc: 0.7573\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2667 - acc: 0.9080 - val_loss: 0.8225 - val_acc: 0.7622\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.2542 - acc: 0.9136 - val_loss: 0.8428 - val_acc: 0.7581\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.2457 - acc: 0.9170 - val_loss: 0.8733 - val_acc: 0.7516\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2367 - acc: 0.9196 - val_loss: 0.8545 - val_acc: 0.7605\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2250 - acc: 0.9245 - val_loss: 0.9364 - val_acc: 0.7425\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2253 - acc: 0.9250 - val_loss: 0.8667 - val_acc: 0.7530\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.2158 - acc: 0.9273 - val_loss: 0.8878 - val_acc: 0.7565\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.2034 - acc: 0.9317 - val_loss: 0.9277 - val_acc: 0.7509\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1957 - acc: 0.9345 - val_loss: 0.8572 - val_acc: 0.7689\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1962 - acc: 0.9356 - val_loss: 0.9016 - val_acc: 0.7584\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.1852 - acc: 0.9382 - val_loss: 0.8839 - val_acc: 0.7634\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1779 - acc: 0.9398 - val_loss: 0.9061 - val_acc: 0.7589\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1750 - acc: 0.9396 - val_loss: 0.8804 - val_acc: 0.7610\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1694 - acc: 0.9435 - val_loss: 0.8696 - val_acc: 0.7672\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1575 - acc: 0.9468 - val_loss: 0.8801 - val_acc: 0.7661\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.1575 - acc: 0.9470 - val_loss: 0.9397 - val_acc: 0.7588\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1551 - acc: 0.9474 - val_loss: 0.9102 - val_acc: 0.7650\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.1496 - acc: 0.9490 - val_loss: 0.9091 - val_acc: 0.7656\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1432 - acc: 0.9513 - val_loss: 0.9814 - val_acc: 0.7567\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1382 - acc: 0.9521 - val_loss: 0.9129 - val_acc: 0.7620\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1414 - acc: 0.9517 - val_loss: 0.8794 - val_acc: 0.7751\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 69s 63ms/step - loss: 0.1351 - acc: 0.9548 - val_loss: 0.9261 - val_acc: 0.7652\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 68s 63ms/step - loss: 0.1333 - acc: 0.9556 - val_loss: 0.9263 - val_acc: 0.7688\n",
      "Epoch 67/130\n",
      " 740/1093 [===================>..........] - ETA: 19s - loss: 0.1195 - acc: 0.9601"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea14.4_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea14.4_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8984.064810276031"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exe_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 115s 11ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0620492207547167, 0.7787]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(test_model)\n",
    "\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea14.4_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 4.5\n",
    "same as 4 but use smaller learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_21/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_21/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_21:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_10/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_21/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_21/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_21 (Model)                multiple             260160      input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_33 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_21[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_33[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_34 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_21[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_34[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_35 (Concatenate)    (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_25 (Gl (None, 704)          0           concatenate_35[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 10)           7050        global_average_pooling2d_25[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 10)           40          dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 10)           0           batch_normalization_31[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 6,803,506\n",
      "Trainable params: 6,799,646\n",
      "Non-trainable params: 3,860\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "del(model)\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "vgg.compile(loss=categorical_crossentropy, optimizer=Adam(1e-5), metrics=['accuracy'])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(1e-4), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 32s 29ms/step - loss: 1.5550 - acc: 0.4655 - val_loss: 1.6195 - val_acc: 0.4173\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.3462 - acc: 0.5547 - val_loss: 1.4837 - val_acc: 0.4989\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.2445 - acc: 0.5931 - val_loss: 1.1981 - val_acc: 0.6013\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.1599 - acc: 0.6274 - val_loss: 1.3115 - val_acc: 0.5777\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0944 - acc: 0.6498 - val_loss: 1.2138 - val_acc: 0.5850\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.0471 - acc: 0.6626 - val_loss: 1.0528 - val_acc: 0.6549\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 1.0096 - acc: 0.6757 - val_loss: 0.9619 - val_acc: 0.6927\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9638 - acc: 0.6894 - val_loss: 1.0013 - val_acc: 0.6722\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9282 - acc: 0.7023 - val_loss: 1.0299 - val_acc: 0.6467\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.9025 - acc: 0.7109 - val_loss: 0.9935 - val_acc: 0.6617\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8803 - acc: 0.7161 - val_loss: 1.0294 - val_acc: 0.6535\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8475 - acc: 0.7274 - val_loss: 0.8609 - val_acc: 0.7241\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.8227 - acc: 0.7380 - val_loss: 0.8805 - val_acc: 0.7183\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7987 - acc: 0.7417 - val_loss: 0.9375 - val_acc: 0.6920\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7821 - acc: 0.7457 - val_loss: 0.8268 - val_acc: 0.7344\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.7645 - acc: 0.7519 - val_loss: 0.8204 - val_acc: 0.7326\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7476 - acc: 0.7584 - val_loss: 0.7692 - val_acc: 0.7521\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7255 - acc: 0.7648 - val_loss: 0.7919 - val_acc: 0.7386\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.7103 - acc: 0.7689 - val_loss: 0.8709 - val_acc: 0.7157\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6905 - acc: 0.7776 - val_loss: 0.8279 - val_acc: 0.7260\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6800 - acc: 0.7790 - val_loss: 0.7475 - val_acc: 0.7556\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6599 - acc: 0.7883 - val_loss: 0.7928 - val_acc: 0.7400\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6445 - acc: 0.7897 - val_loss: 0.7286 - val_acc: 0.7575\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6266 - acc: 0.7988 - val_loss: 0.7493 - val_acc: 0.7535\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.6209 - acc: 0.7966 - val_loss: 0.7506 - val_acc: 0.7543\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5985 - acc: 0.8072 - val_loss: 0.7232 - val_acc: 0.7666\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5898 - acc: 0.8106 - val_loss: 0.7270 - val_acc: 0.7604\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5770 - acc: 0.8147 - val_loss: 0.7405 - val_acc: 0.7605\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5701 - acc: 0.8156 - val_loss: 0.6951 - val_acc: 0.7696\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5587 - acc: 0.8197 - val_loss: 0.7538 - val_acc: 0.7575\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5434 - acc: 0.8262 - val_loss: 0.7366 - val_acc: 0.76261s - loss: 0.54\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5342 - acc: 0.8266 - val_loss: 0.7807 - val_acc: 0.7438\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5251 - acc: 0.8331 - val_loss: 0.8022 - val_acc: 0.7414\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.5132 - acc: 0.8335 - val_loss: 0.6825 - val_acc: 0.7795\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4957 - acc: 0.8421 - val_loss: 0.6628 - val_acc: 0.7805\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.4913 - acc: 0.8423 - val_loss: 0.6763 - val_acc: 0.7794\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4850 - acc: 0.8419 - val_loss: 0.6783 - val_acc: 0.7792\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4691 - acc: 0.8486 - val_loss: 0.7078 - val_acc: 0.7650\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.4649 - acc: 0.8504 - val_loss: 0.6838 - val_acc: 0.7758\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4555 - acc: 0.8524 - val_loss: 0.6701 - val_acc: 0.7828\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4423 - acc: 0.8603 - val_loss: 0.6140 - val_acc: 0.7992\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4343 - acc: 0.8609 - val_loss: 0.6195 - val_acc: 0.7952\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4257 - acc: 0.8638 - val_loss: 0.6406 - val_acc: 0.7903\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4152 - acc: 0.8670 - val_loss: 0.6491 - val_acc: 0.7904\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.4086 - acc: 0.8702 - val_loss: 0.7017 - val_acc: 0.7722\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.4000 - acc: 0.8727 - val_loss: 0.6491 - val_acc: 0.7875\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3879 - acc: 0.8782 - val_loss: 0.6533 - val_acc: 0.7892\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3814 - acc: 0.8797 - val_loss: 0.6293 - val_acc: 0.7952\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3744 - acc: 0.8841 - val_loss: 0.6698 - val_acc: 0.7782\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3654 - acc: 0.8858 - val_loss: 0.5961 - val_acc: 0.8038\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3610 - acc: 0.8878 - val_loss: 0.6478 - val_acc: 0.7946\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.3469 - acc: 0.8913 - val_loss: 0.6879 - val_acc: 0.7820\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3447 - acc: 0.8909 - val_loss: 0.6490 - val_acc: 0.7921\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3377 - acc: 0.8945 - val_loss: 0.6489 - val_acc: 0.7867\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3318 - acc: 0.8967 - val_loss: 0.7098 - val_acc: 0.7739\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3213 - acc: 0.9000 - val_loss: 0.6421 - val_acc: 0.7913\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3148 - acc: 0.9014 - val_loss: 0.6191 - val_acc: 0.8032\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3093 - acc: 0.9035 - val_loss: 0.6746 - val_acc: 0.7909\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.3005 - acc: 0.9070 - val_loss: 0.7349 - val_acc: 0.7626\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2989 - acc: 0.9065 - val_loss: 0.6536 - val_acc: 0.7952\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2930 - acc: 0.9123 - val_loss: 0.6367 - val_acc: 0.7980\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.2910 - acc: 0.9111 - val_loss: 0.6064 - val_acc: 0.8042\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2762 - acc: 0.9161 - val_loss: 0.7118 - val_acc: 0.7773\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2705 - acc: 0.9176 - val_loss: 0.6297 - val_acc: 0.7979\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2670 - acc: 0.9183 - val_loss: 0.6432 - val_acc: 0.7910\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2636 - acc: 0.9179 - val_loss: 0.6728 - val_acc: 0.7900\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.2531 - acc: 0.9225 - val_loss: 0.6465 - val_acc: 0.7954\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2534 - acc: 0.9244 - val_loss: 0.6695 - val_acc: 0.7943\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.2430 - acc: 0.9266 - val_loss: 0.6552 - val_acc: 0.7978\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.2409 - acc: 0.9272 - val_loss: 0.7210 - val_acc: 0.7784\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2372 - acc: 0.9278 - val_loss: 0.6735 - val_acc: 0.7887\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2310 - acc: 0.9307 - val_loss: 0.6539 - val_acc: 0.7956\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2220 - acc: 0.9323 - val_loss: 0.6384 - val_acc: 0.7955\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.2223 - acc: 0.9336 - val_loss: 0.6728 - val_acc: 0.7914\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2221 - acc: 0.9321 - val_loss: 0.6647 - val_acc: 0.7946\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2197 - acc: 0.9320 - val_loss: 0.6202 - val_acc: 0.8059\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2081 - acc: 0.9380 - val_loss: 0.6839 - val_acc: 0.7924\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.2046 - acc: 0.9399 - val_loss: 0.6721 - val_acc: 0.7919\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1979 - acc: 0.9423 - val_loss: 0.8080 - val_acc: 0.7576\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1938 - acc: 0.9438 - val_loss: 0.6636 - val_acc: 0.8010\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1942 - acc: 0.9420 - val_loss: 0.6801 - val_acc: 0.7950\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1915 - acc: 0.9433 - val_loss: 0.6709 - val_acc: 0.7982\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1886 - acc: 0.9434 - val_loss: 0.6257 - val_acc: 0.8116\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1829 - acc: 0.9456 - val_loss: 0.6095 - val_acc: 0.8123\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1789 - acc: 0.9469 - val_loss: 0.6377 - val_acc: 0.8091\n",
      "Epoch 86/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1822 - acc: 0.9463 - val_loss: 0.6655 - val_acc: 0.7994\n",
      "Epoch 87/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1761 - acc: 0.9477 - val_loss: 0.7150 - val_acc: 0.7912ETA: 0s - loss: 0.1761 -\n",
      "Epoch 88/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1713 - acc: 0.9505 - val_loss: 0.6730 - val_acc: 0.7968\n",
      "Epoch 89/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1669 - acc: 0.9520 - val_loss: 0.5981 - val_acc: 0.8148\n",
      "Epoch 90/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1707 - acc: 0.9496 - val_loss: 0.6875 - val_acc: 0.7982\n",
      "Epoch 91/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1616 - acc: 0.9523 - val_loss: 0.6292 - val_acc: 0.8128\n",
      "Epoch 92/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1581 - acc: 0.9540 - val_loss: 0.6475 - val_acc: 0.8038\n",
      "Epoch 93/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1574 - acc: 0.9548 - val_loss: 0.7709 - val_acc: 0.7766\n",
      "Epoch 94/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1506 - acc: 0.9571 - val_loss: 0.6461 - val_acc: 0.8059\n",
      "Epoch 95/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1508 - acc: 0.9562 - val_loss: 0.7164 - val_acc: 0.7867\n",
      "Epoch 96/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1432 - acc: 0.9591 - val_loss: 0.6209 - val_acc: 0.8147\n",
      "Epoch 97/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1478 - acc: 0.9573 - val_loss: 0.6634 - val_acc: 0.8048\n",
      "Epoch 98/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1456 - acc: 0.9572 - val_loss: 0.6694 - val_acc: 0.8027\n",
      "Epoch 99/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1402 - acc: 0.9596 - val_loss: 0.6452 - val_acc: 0.8093\n",
      "Epoch 100/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1386 - acc: 0.9594 - val_loss: 0.6588 - val_acc: 0.8057\n",
      "Epoch 101/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1364 - acc: 0.9611 - val_loss: 0.6524 - val_acc: 0.8058\n",
      "Epoch 102/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1347 - acc: 0.9614 - val_loss: 0.6826 - val_acc: 0.8002\n",
      "Epoch 103/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1358 - acc: 0.9607 - val_loss: 0.6385 - val_acc: 0.8111\n",
      "Epoch 104/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1279 - acc: 0.9637 - val_loss: 0.6760 - val_acc: 0.8061\n",
      "Epoch 105/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1278 - acc: 0.9648 - val_loss: 0.6120 - val_acc: 0.8151\n",
      "Epoch 106/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1281 - acc: 0.9632 - val_loss: 0.6998 - val_acc: 0.8002\n",
      "Epoch 107/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1285 - acc: 0.9633 - val_loss: 0.6762 - val_acc: 0.8026\n",
      "Epoch 108/130\n",
      "1093/1093 [==============================] - 29s 26ms/step - loss: 0.1205 - acc: 0.9660 - val_loss: 0.7452 - val_acc: 0.7892\n",
      "Epoch 109/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1216 - acc: 0.9657 - val_loss: 0.7031 - val_acc: 0.8026\n",
      "Epoch 110/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1201 - acc: 0.9658 - val_loss: 0.6572 - val_acc: 0.8072\n",
      "Epoch 111/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1186 - acc: 0.9667 - val_loss: 0.6477 - val_acc: 0.8092\n",
      "Epoch 112/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1172 - acc: 0.9660 - val_loss: 0.7234 - val_acc: 0.7944\n",
      "Epoch 113/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1135 - acc: 0.9680 - val_loss: 0.6558 - val_acc: 0.8090\n",
      "Epoch 114/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1113 - acc: 0.9694 - val_loss: 0.6396 - val_acc: 0.8104\n",
      "Epoch 115/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1130 - acc: 0.9672 - val_loss: 0.6446 - val_acc: 0.8121\n",
      "Epoch 116/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1108 - acc: 0.9686 - val_loss: 0.6690 - val_acc: 0.8057\n",
      "Epoch 117/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1078 - acc: 0.9698 - val_loss: 0.6555 - val_acc: 0.8127\n",
      "Epoch 118/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1049 - acc: 0.9707 - val_loss: 0.7674 - val_acc: 0.7881\n",
      "Epoch 119/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.1071 - acc: 0.9699 - val_loss: 0.6841 - val_acc: 0.8040\n",
      "Epoch 120/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1015 - acc: 0.9717 - val_loss: 0.6652 - val_acc: 0.8135\n",
      "Epoch 121/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1015 - acc: 0.9721 - val_loss: 0.6523 - val_acc: 0.8093\n",
      "Epoch 122/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.1016 - acc: 0.9719 - val_loss: 0.6513 - val_acc: 0.8139\n",
      "Epoch 123/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0999 - acc: 0.9715 - val_loss: 0.7009 - val_acc: 0.8064\n",
      "Epoch 124/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0990 - acc: 0.9728 - val_loss: 0.6925 - val_acc: 0.8067\n",
      "Epoch 125/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 0.0974 - acc: 0.9735 - val_loss: 0.6855 - val_acc: 0.8059\n",
      "Epoch 126/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0953 - acc: 0.9729 - val_loss: 0.7166 - val_acc: 0.7972\n",
      "Epoch 127/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0992 - acc: 0.9725 - val_loss: 0.6726 - val_acc: 0.8103\n",
      "Epoch 128/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.0966 - acc: 0.9725 - val_loss: 0.6972 - val_acc: 0.8034\n",
      "Epoch 129/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0892 - acc: 0.9754 - val_loss: 0.7014 - val_acc: 0.8066\n",
      "Epoch 130/130\n",
      "1093/1093 [==============================] - 27s 25ms/step - loss: 0.0923 - acc: 0.9736 - val_loss: 0.6758 - val_acc: 0.8097\n",
      "exe time:  3577.822538614273\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea14.5_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea14.5_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 46s 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6463626317824214, 0.8109]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(test_model)\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea14.5_model.hdf5')\n",
    "test_generator.reset()\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5\n",
    "\n",
    "Same as **Experiment 4** but keep all rest of conv blocks and apply GAP, then concatenate with the output of new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_37/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_37/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>, <tf.Tensor 'model_37/block3_pool/MaxPool:0' shape=(?, 4, 4, 256) dtype=float32>, <tf.Tensor 'model_37/block4_pool/MaxPool:0' shape=(?, 2, 2, 512) dtype=float32>, <tf.Tensor 'model_37/block5_pool/MaxPool:0' shape=(?, 1, 1, 512) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_58:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_28/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_37/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_37/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_58 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_37 (Model)                multiple             14714688    input_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_71 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_37[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_71[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_72 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_37[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_73 (Concatenate)    (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_66 (Gl (None, 256)          0           model_37[1][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_67 (Gl (None, 512)          0           model_37[1][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_68 (Gl (None, 512)          0           model_37[1][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_69 (Gl (None, 704)          0           concatenate_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_74 (Concatenate)    (None, 1984)         0           global_average_pooling2d_66[0][0]\n",
      "                                                                 global_average_pooling2d_67[0][0]\n",
      "                                                                 global_average_pooling2d_68[0][0]\n",
      "                                                                 global_average_pooling2d_69[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 10)           19850       concatenate_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 10)           40          dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 10)           0           batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 21,270,834\n",
      "Trainable params: 21,266,974\n",
      "Non-trainable params: 3,860\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs)\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "for layer in vgg_outs[2:]:\n",
    "    vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs[:2])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "vgg_gaps.append(skippedvgg_out)\n",
    "outputs = concatenate(vgg_gaps)\n",
    "\n",
    "outputs = Dense(10)(outputs)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 58s 53ms/step - loss: 1.8820 - acc: 0.2761 - val_loss: 1.7512 - val_acc: 0.3214\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 1.6437 - acc: 0.3900 - val_loss: 2.0209 - val_acc: 0.3237\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.4718 - acc: 0.4592 - val_loss: 1.7065 - val_acc: 0.3774\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.3668 - acc: 0.4988 - val_loss: 1.6042 - val_acc: 0.4634\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 52s 48ms/step - loss: 1.2831 - acc: 0.5411 - val_loss: 1.2207 - val_acc: 0.5541\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 1.2132 - acc: 0.5665 - val_loss: 1.2823 - val_acc: 0.5635\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.1627 - acc: 0.5874 - val_loss: 1.1595 - val_acc: 0.5919\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.1087 - acc: 0.6100 - val_loss: 1.3733 - val_acc: 0.5468\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.0624 - acc: 0.6274 - val_loss: 1.1276 - val_acc: 0.6152\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 1.0337 - acc: 0.6381 - val_loss: 0.9989 - val_acc: 0.6497\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 1.0003 - acc: 0.6509 - val_loss: 1.0393 - val_acc: 0.6435\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 52s 48ms/step - loss: 0.9685 - acc: 0.6602 - val_loss: 1.1215 - val_acc: 0.6102\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.9444 - acc: 0.6735 - val_loss: 1.1505 - val_acc: 0.6181\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.9202 - acc: 0.6797 - val_loss: 1.0274 - val_acc: 0.6470\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.8911 - acc: 0.6902 - val_loss: 0.9418 - val_acc: 0.6796\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 0.8744 - acc: 0.6984 - val_loss: 0.8905 - val_acc: 0.6972\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.8569 - acc: 0.7035 - val_loss: 0.9063 - val_acc: 0.6900\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.8389 - acc: 0.7100 - val_loss: 0.8369 - val_acc: 0.7158\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.8130 - acc: 0.7197 - val_loss: 0.9282 - val_acc: 0.6873\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7985 - acc: 0.7253 - val_loss: 0.9121 - val_acc: 0.6837\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7813 - acc: 0.7310 - val_loss: 0.9494 - val_acc: 0.6825\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 0.7673 - acc: 0.7370 - val_loss: 1.0989 - val_acc: 0.6589\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7559 - acc: 0.7420 - val_loss: 0.8467 - val_acc: 0.7164\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7362 - acc: 0.7469 - val_loss: 0.9113 - val_acc: 0.7014\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7204 - acc: 0.7548 - val_loss: 1.0680 - val_acc: 0.6716\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7117 - acc: 0.7567 - val_loss: 0.7939 - val_acc: 0.7326\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.7023 - acc: 0.7606 - val_loss: 0.8360 - val_acc: 0.7183\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6878 - acc: 0.7655 - val_loss: 0.7652 - val_acc: 0.7401\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6732 - acc: 0.7705 - val_loss: 0.8989 - val_acc: 0.7034\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6603 - acc: 0.7754 - val_loss: 0.9557 - val_acc: 0.6927\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6516 - acc: 0.7797 - val_loss: 0.7871 - val_acc: 0.7379\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6459 - acc: 0.7795 - val_loss: 0.8448 - val_acc: 0.7216\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 52s 48ms/step - loss: 0.6292 - acc: 0.7880 - val_loss: 0.7635 - val_acc: 0.7433\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6263 - acc: 0.7848 - val_loss: 0.7457 - val_acc: 0.7511\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6121 - acc: 0.7932 - val_loss: 0.7356 - val_acc: 0.7518\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6050 - acc: 0.7923 - val_loss: 0.7937 - val_acc: 0.7399\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 51s 47ms/step - loss: 0.6042 - acc: 0.7940 - val_loss: 0.7258 - val_acc: 0.7566\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 52s 47ms/step - loss: 0.5883 - acc: 0.8008 - val_loss: 0.7372 - val_acc: 0.7565\n",
      "Epoch 39/130\n",
      " 727/1093 [==================>...........] - ETA: 15s - loss: 0.5891 - acc: 0.8009"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea15_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea15_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exe time:  6736.114166021347\n"
     ]
    }
   ],
   "source": [
    "print('exe time: ', exe_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 72s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.765161869376994, 0.7932]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea15_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 5.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1002 05:28:10.060826 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W1002 05:28:10.061772 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W1002 05:28:10.064476 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W1002 05:28:10.083518 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W1002 05:28:10.304884 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W1002 05:28:10.305469 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W1002 05:28:10.647969 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_1/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_1/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>, <tf.Tensor 'model_1/block3_pool/MaxPool:0' shape=(?, 4, 4, 256) dtype=float32>, <tf.Tensor 'model_1/block4_pool/MaxPool:0' shape=(?, 2, 2, 512) dtype=float32>, <tf.Tensor 'model_1/block5_pool/MaxPool:0' shape=(?, 1, 1, 512) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_1:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_1/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_1/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 05:28:11.180184 140535202330368 deprecation_wrapper.py:119] From /usr/local/lib/python3.5/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_1 (Model)                 multiple             14714688    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_1[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_1[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 256)          0           model_1[1][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 512)          0           model_1[1][3]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_3 (Glo (None, 512)          0           model_1[1][4]                    \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 704)          0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1984)         0           global_average_pooling2d_1[0][0] \n",
      "                                                                 global_average_pooling2d_2[0][0] \n",
      "                                                                 global_average_pooling2d_3[0][0] \n",
      "                                                                 global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 10)           19850       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 10)           40          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 10)           0           batch_normalization_1[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 21,270,834\n",
      "Trainable params: 6,812,446\n",
      "Non-trainable params: 14,458,388\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "# del(model)\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in pretrain_vgg.layers[7:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs)\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "for layer in vgg_outs[2:]:\n",
    "    vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs[:2])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "vgg_gaps.append(skippedvgg_out)\n",
    "outputs = concatenate(vgg_gaps)\n",
    "\n",
    "outputs = Dense(10)(outputs)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1002 05:28:18.475156 140535202330368 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 38s 35ms/step - loss: 1.6226 - acc: 0.4232 - val_loss: 1.6144 - val_acc: 0.4272\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 1.2908 - acc: 0.5544 - val_loss: 1.3326 - val_acc: 0.5198\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 1.1636 - acc: 0.5964 - val_loss: 1.2274 - val_acc: 0.5831\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 1.0895 - acc: 0.6207 - val_loss: 1.1582 - val_acc: 0.5983\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 1.0346 - acc: 0.6377 - val_loss: 1.3172 - val_acc: 0.5622\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.9928 - acc: 0.6539 - val_loss: 1.0620 - val_acc: 0.6283\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.9546 - acc: 0.6649 - val_loss: 1.0134 - val_acc: 0.6507\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.9288 - acc: 0.6750 - val_loss: 1.0605 - val_acc: 0.6384\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.9017 - acc: 0.6830 - val_loss: 1.0489 - val_acc: 0.6383\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.8757 - acc: 0.6932 - val_loss: 0.9348 - val_acc: 0.6809\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.8492 - acc: 0.7015 - val_loss: 0.8487 - val_acc: 0.7043\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.8372 - acc: 0.7062 - val_loss: 1.0279 - val_acc: 0.6513\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.8205 - acc: 0.7113 - val_loss: 0.8171 - val_acc: 0.7174\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7935 - acc: 0.7244 - val_loss: 0.8394 - val_acc: 0.7107\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7755 - acc: 0.7274 - val_loss: 0.8523 - val_acc: 0.7145\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7601 - acc: 0.7345 - val_loss: 0.8203 - val_acc: 0.7182\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7432 - acc: 0.7402 - val_loss: 0.8740 - val_acc: 0.7006\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7325 - acc: 0.7426 - val_loss: 0.8846 - val_acc: 0.6978\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7153 - acc: 0.7527 - val_loss: 0.7853 - val_acc: 0.7308\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.7062 - acc: 0.7508 - val_loss: 0.7879 - val_acc: 0.7278\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6856 - acc: 0.7621 - val_loss: 0.7625 - val_acc: 0.7336\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6709 - acc: 0.7661 - val_loss: 0.7633 - val_acc: 0.7370\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6557 - acc: 0.7698 - val_loss: 0.7791 - val_acc: 0.7342\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6424 - acc: 0.7766 - val_loss: 0.8426 - val_acc: 0.7159\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6339 - acc: 0.7813 - val_loss: 0.7614 - val_acc: 0.7409\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6122 - acc: 0.7870 - val_loss: 0.7261 - val_acc: 0.7471\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.6102 - acc: 0.7866 - val_loss: 0.7108 - val_acc: 0.7599\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5963 - acc: 0.7923 - val_loss: 0.7631 - val_acc: 0.7452\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5825 - acc: 0.7970 - val_loss: 0.6665 - val_acc: 0.7720\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5759 - acc: 0.8002 - val_loss: 0.8064 - val_acc: 0.7347\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5608 - acc: 0.8073 - val_loss: 0.7267 - val_acc: 0.7548\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5518 - acc: 0.8076 - val_loss: 0.7914 - val_acc: 0.7388\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5318 - acc: 0.8171 - val_loss: 0.6963 - val_acc: 0.7647\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5180 - acc: 0.8203 - val_loss: 0.7446 - val_acc: 0.7513\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5084 - acc: 0.8248 - val_loss: 0.6597 - val_acc: 0.7779\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.5055 - acc: 0.8237 - val_loss: 0.6707 - val_acc: 0.7747\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4860 - acc: 0.8302 - val_loss: 0.7189 - val_acc: 0.7609\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4835 - acc: 0.8334 - val_loss: 0.7182 - val_acc: 0.7561\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4737 - acc: 0.8360 - val_loss: 0.6762 - val_acc: 0.7775\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4611 - acc: 0.8404 - val_loss: 0.6875 - val_acc: 0.7705\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4530 - acc: 0.8432 - val_loss: 0.7033 - val_acc: 0.7674\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4414 - acc: 0.8484 - val_loss: 0.6879 - val_acc: 0.7750\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4300 - acc: 0.8489 - val_loss: 0.7186 - val_acc: 0.7696\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4204 - acc: 0.8548 - val_loss: 0.6827 - val_acc: 0.7753\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4163 - acc: 0.8551 - val_loss: 0.6789 - val_acc: 0.7777\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.4013 - acc: 0.8620 - val_loss: 0.6965 - val_acc: 0.7715\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3953 - acc: 0.8641 - val_loss: 0.6477 - val_acc: 0.7889\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3885 - acc: 0.8665 - val_loss: 0.6099 - val_acc: 0.8009\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3733 - acc: 0.8721 - val_loss: 0.7114 - val_acc: 0.7682\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3678 - acc: 0.8724 - val_loss: 0.6469 - val_acc: 0.7904\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3658 - acc: 0.8743 - val_loss: 0.6578 - val_acc: 0.7885\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3511 - acc: 0.8790 - val_loss: 0.6763 - val_acc: 0.7857\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3439 - acc: 0.8804 - val_loss: 0.7643 - val_acc: 0.7637\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3425 - acc: 0.8823 - val_loss: 0.7194 - val_acc: 0.7739\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3306 - acc: 0.8863 - val_loss: 0.6882 - val_acc: 0.7842\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3290 - acc: 0.8875 - val_loss: 0.7297 - val_acc: 0.7698\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3128 - acc: 0.8936 - val_loss: 0.8165 - val_acc: 0.7527\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3118 - acc: 0.8921 - val_loss: 0.6726 - val_acc: 0.7877\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.3052 - acc: 0.8945 - val_loss: 0.6895 - val_acc: 0.7853\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2987 - acc: 0.8961 - val_loss: 0.6695 - val_acc: 0.7875\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2886 - acc: 0.9012 - val_loss: 0.6914 - val_acc: 0.7885\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2824 - acc: 0.9030 - val_loss: 0.6976 - val_acc: 0.7846\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2741 - acc: 0.9049 - val_loss: 0.7051 - val_acc: 0.7866\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2702 - acc: 0.9076 - val_loss: 0.7695 - val_acc: 0.7656\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2644 - acc: 0.9098 - val_loss: 0.6722 - val_acc: 0.7972\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2616 - acc: 0.9094 - val_loss: 0.6908 - val_acc: 0.7892\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2510 - acc: 0.9152 - val_loss: 0.7278 - val_acc: 0.7807\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2475 - acc: 0.9144 - val_loss: 0.8049 - val_acc: 0.7703\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2450 - acc: 0.9154 - val_loss: 0.8315 - val_acc: 0.7557\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2414 - acc: 0.9167 - val_loss: 0.7200 - val_acc: 0.7827\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2312 - acc: 0.9202 - val_loss: 0.7558 - val_acc: 0.7780\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2281 - acc: 0.9213 - val_loss: 0.7150 - val_acc: 0.7901\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2234 - acc: 0.9230 - val_loss: 0.6908 - val_acc: 0.7940\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2230 - acc: 0.9232 - val_loss: 0.7401 - val_acc: 0.7869\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2163 - acc: 0.9250 - val_loss: 0.6706 - val_acc: 0.8044\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2132 - acc: 0.9268 - val_loss: 0.7914 - val_acc: 0.7708\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2060 - acc: 0.9300 - val_loss: 0.8682 - val_acc: 0.7594\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.2024 - acc: 0.9309 - val_loss: 0.7683 - val_acc: 0.7813\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1998 - acc: 0.9311 - val_loss: 0.8258 - val_acc: 0.7642\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1995 - acc: 0.9315 - val_loss: 0.7179 - val_acc: 0.7953\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1928 - acc: 0.9341 - val_loss: 0.8446 - val_acc: 0.7676\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1942 - acc: 0.9323 - val_loss: 0.7560 - val_acc: 0.7860\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1913 - acc: 0.9353 - val_loss: 0.7523 - val_acc: 0.7793\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1899 - acc: 0.9354 - val_loss: 0.8316 - val_acc: 0.7767\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1799 - acc: 0.9401 - val_loss: 0.7258 - val_acc: 0.7962\n",
      "Epoch 86/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1717 - acc: 0.9410 - val_loss: 0.8028 - val_acc: 0.7730\n",
      "Epoch 87/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1813 - acc: 0.9392 - val_loss: 0.7457 - val_acc: 0.7861\n",
      "Epoch 88/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1741 - acc: 0.9406 - val_loss: 0.7653 - val_acc: 0.7865\n",
      "Epoch 89/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1688 - acc: 0.9414 - val_loss: 0.8521 - val_acc: 0.7775\n",
      "Epoch 90/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1680 - acc: 0.9435 - val_loss: 0.8254 - val_acc: 0.7791\n",
      "Epoch 91/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1601 - acc: 0.9455 - val_loss: 0.8060 - val_acc: 0.7804\n",
      "Epoch 92/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1646 - acc: 0.9431 - val_loss: 0.7950 - val_acc: 0.7831\n",
      "Epoch 93/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1567 - acc: 0.9460 - val_loss: 0.7893 - val_acc: 0.7869\n",
      "Epoch 94/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1562 - acc: 0.9465 - val_loss: 0.8170 - val_acc: 0.7824\n",
      "Epoch 95/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1582 - acc: 0.9459 - val_loss: 0.8246 - val_acc: 0.7800\n",
      "Epoch 96/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1525 - acc: 0.9489 - val_loss: 0.8416 - val_acc: 0.7680\n",
      "Epoch 97/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1517 - acc: 0.9490 - val_loss: 0.7786 - val_acc: 0.7898\n",
      "Epoch 98/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1457 - acc: 0.9499 - val_loss: 0.8081 - val_acc: 0.7815\n",
      "Epoch 99/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1472 - acc: 0.9509 - val_loss: 0.7104 - val_acc: 0.8057\n",
      "Epoch 100/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1430 - acc: 0.9507 - val_loss: 0.7617 - val_acc: 0.7875\n",
      "Epoch 101/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1418 - acc: 0.9515 - val_loss: 0.7175 - val_acc: 0.8065\n",
      "Epoch 102/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1379 - acc: 0.9537 - val_loss: 0.7983 - val_acc: 0.7802\n",
      "Epoch 103/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1410 - acc: 0.9514 - val_loss: 0.7808 - val_acc: 0.7931\n",
      "Epoch 104/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1407 - acc: 0.9514 - val_loss: 0.8284 - val_acc: 0.7837\n",
      "Epoch 105/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1356 - acc: 0.9556 - val_loss: 0.7481 - val_acc: 0.8022\n",
      "Epoch 106/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1304 - acc: 0.9557 - val_loss: 0.7569 - val_acc: 0.7959\n",
      "Epoch 107/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1305 - acc: 0.9559 - val_loss: 0.8130 - val_acc: 0.7910\n",
      "Epoch 108/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1314 - acc: 0.9571 - val_loss: 0.8153 - val_acc: 0.7827\n",
      "Epoch 109/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1221 - acc: 0.9585 - val_loss: 0.8475 - val_acc: 0.7829\n",
      "Epoch 110/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1283 - acc: 0.9555 - val_loss: 0.8175 - val_acc: 0.7897\n",
      "Epoch 111/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1239 - acc: 0.9574 - val_loss: 0.7912 - val_acc: 0.7924\n",
      "Epoch 112/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1209 - acc: 0.9585 - val_loss: 0.8011 - val_acc: 0.7924\n",
      "Epoch 113/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1242 - acc: 0.9579 - val_loss: 0.8405 - val_acc: 0.7877\n",
      "Epoch 114/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1148 - acc: 0.9621 - val_loss: 0.7986 - val_acc: 0.7938\n",
      "Epoch 115/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1246 - acc: 0.9570 - val_loss: 0.9918 - val_acc: 0.7644\n",
      "Epoch 116/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1153 - acc: 0.9601 - val_loss: 0.7415 - val_acc: 0.8036\n",
      "Epoch 117/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1188 - acc: 0.9607 - val_loss: 0.7629 - val_acc: 0.8046\n",
      "Epoch 118/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1147 - acc: 0.9605 - val_loss: 0.8109 - val_acc: 0.7914\n",
      "Epoch 119/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1117 - acc: 0.9619 - val_loss: 0.7306 - val_acc: 0.8067\n",
      "Epoch 120/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1136 - acc: 0.9605 - val_loss: 0.8670 - val_acc: 0.7881\n",
      "Epoch 121/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1129 - acc: 0.9632 - val_loss: 0.7665 - val_acc: 0.8015\n",
      "Epoch 122/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1097 - acc: 0.9626 - val_loss: 0.8236 - val_acc: 0.7861\n",
      "Epoch 123/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1054 - acc: 0.9646 - val_loss: 0.9060 - val_acc: 0.7793\n",
      "Epoch 124/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1073 - acc: 0.9639 - val_loss: 0.7734 - val_acc: 0.8027\n",
      "Epoch 125/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1047 - acc: 0.9660 - val_loss: 0.8193 - val_acc: 0.7950\n",
      "Epoch 126/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1066 - acc: 0.9647 - val_loss: 0.9626 - val_acc: 0.7666\n",
      "Epoch 127/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1047 - acc: 0.9640 - val_loss: 0.8073 - val_acc: 0.7929\n",
      "Epoch 128/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1029 - acc: 0.9652 - val_loss: 0.9093 - val_acc: 0.7856\n",
      "Epoch 129/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.1007 - acc: 0.9657 - val_loss: 0.7812 - val_acc: 0.8036\n",
      "Epoch 130/130\n",
      "1093/1093 [==============================] - 36s 33ms/step - loss: 0.0994 - acc: 0.9663 - val_loss: 0.8112 - val_acc: 0.7990\n",
      "exe time:  4672.700249195099\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea15.2_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea15.2_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 65s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7668102605718647, 0.7975]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea15.2_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_13/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_13/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>, <tf.Tensor 'model_13/block5_pool/MaxPool:0' shape=(?, 1, 1, 512) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_13:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_7/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_13/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_13/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "model_13 (Model)                multiple             14714688    input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_24 (Concatenate)    (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_13[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_25 (Concatenate)    (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_13[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_26 (Concatenate)    (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 512)          0           model_13[1][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_13 (Gl (None, 704)          0           concatenate_26[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 10)           5130        flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 10)           7050        global_average_pooling2d_13[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 10)           40          dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 10)           40          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 10)           0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 10)           0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_27 (Concatenate)    (None, 20)           0           activation_15[0][0]              \n",
      "                                                                 activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 10)           210         concatenate_27[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 10)           40          dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 10)           0           batch_normalization_17[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 21,263,454\n",
      "Trainable params: 6,805,026\n",
      "Non-trainable params: 14,458,428\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,18]\n",
    "# vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in pretrain_vgg.layers[7:]:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs)\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "\n",
    "# top model for pretrain_vgg\n",
    "vgg_top = Flatten()(vgg_outs[-1])\n",
    "vgg_top = Dense(10)(vgg_top)\n",
    "vgg_top = BatchNormalization()(vgg_top)\n",
    "vgg_top = Activation('softmax')(vgg_top)\n",
    "\n",
    "\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs[2:]:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs[:2])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "# Ensamble\n",
    "final_output = concatenate([vgg_top, outputs])\n",
    "final_output = Dense(10)(final_output)\n",
    "final_output = BatchNormalization()(final_output)\n",
    "final_output = Activation('softmax')(final_output)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=final_output)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'model_9/block5_pool/MaxPool:0' shape=(?, 1, 1, 512) dtype=float32>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_outs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 44s 40ms/step - loss: 1.7231 - acc: 0.3848 - val_loss: 1.9341 - val_acc: 0.3413\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 1.4676 - acc: 0.4820 - val_loss: 1.5523 - val_acc: 0.4667\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 1.3469 - acc: 0.5242 - val_loss: 1.3226 - val_acc: 0.5408\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 1.2619 - acc: 0.5555 - val_loss: 1.2820 - val_acc: 0.5438\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 1.2003 - acc: 0.5794 - val_loss: 1.2585 - val_acc: 0.5553\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 1.1412 - acc: 0.5983 - val_loss: 1.3935 - val_acc: 0.5132\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 1.0990 - acc: 0.6163 - val_loss: 1.5525 - val_acc: 0.4840\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 1.0607 - acc: 0.6294 - val_loss: 1.2360 - val_acc: 0.5700\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 1.0194 - acc: 0.6431 - val_loss: 1.1028 - val_acc: 0.6164\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.9848 - acc: 0.6571 - val_loss: 1.2260 - val_acc: 0.5752\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.9573 - acc: 0.6664 - val_loss: 1.2986 - val_acc: 0.5850\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.9296 - acc: 0.6751 - val_loss: 0.9614 - val_acc: 0.6632\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.8917 - acc: 0.6903 - val_loss: 1.0943 - val_acc: 0.6351\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.8694 - acc: 0.6988 - val_loss: 1.1355 - val_acc: 0.6260\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.8444 - acc: 0.7065 - val_loss: 0.9446 - val_acc: 0.6690\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 40s 37ms/step - loss: 0.8180 - acc: 0.7178 - val_loss: 0.9015 - val_acc: 0.6911\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.7990 - acc: 0.7213 - val_loss: 0.8670 - val_acc: 0.6949\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.7796 - acc: 0.7299 - val_loss: 0.9090 - val_acc: 0.6909\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 40s 37ms/step - loss: 0.7573 - acc: 0.7395 - val_loss: 0.7780 - val_acc: 0.7400\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.7293 - acc: 0.7487 - val_loss: 1.0173 - val_acc: 0.6567\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.7131 - acc: 0.7556 - val_loss: 0.7718 - val_acc: 0.7350\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.6876 - acc: 0.7611 - val_loss: 0.8998 - val_acc: 0.6960\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.6759 - acc: 0.7682 - val_loss: 0.8414 - val_acc: 0.7135\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.6593 - acc: 0.7706 - val_loss: 1.0257 - val_acc: 0.6618\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.6403 - acc: 0.7797 - val_loss: 0.7147 - val_acc: 0.7538\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.6222 - acc: 0.7864 - val_loss: 0.7655 - val_acc: 0.7435\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.6100 - acc: 0.7895 - val_loss: 0.6916 - val_acc: 0.7636\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.5903 - acc: 0.7957 - val_loss: 0.7453 - val_acc: 0.7518\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.5778 - acc: 0.8032 - val_loss: 0.7715 - val_acc: 0.7443\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.5591 - acc: 0.8085 - val_loss: 0.7848 - val_acc: 0.7339\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.5528 - acc: 0.8109 - val_loss: 0.6952 - val_acc: 0.7636\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.5352 - acc: 0.8172 - val_loss: 0.7911 - val_acc: 0.7292\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.5277 - acc: 0.8175 - val_loss: 0.7527 - val_acc: 0.7453\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.5162 - acc: 0.8232 - val_loss: 0.6531 - val_acc: 0.7773\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.4993 - acc: 0.8290 - val_loss: 0.8057 - val_acc: 0.7309\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4905 - acc: 0.8310 - val_loss: 0.7808 - val_acc: 0.7456\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.4776 - acc: 0.8370 - val_loss: 0.7884 - val_acc: 0.7431\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4612 - acc: 0.8432 - val_loss: 0.6895 - val_acc: 0.7718\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4561 - acc: 0.8423 - val_loss: 0.7841 - val_acc: 0.7461\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4475 - acc: 0.8466 - val_loss: 0.6847 - val_acc: 0.7708\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.4333 - acc: 0.8512 - val_loss: 0.7841 - val_acc: 0.7507\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.4201 - acc: 0.8545 - val_loss: 0.6561 - val_acc: 0.7845\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4147 - acc: 0.8584 - val_loss: 0.6929 - val_acc: 0.7702\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.4015 - acc: 0.8611 - val_loss: 0.7853 - val_acc: 0.7412\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3963 - acc: 0.8634 - val_loss: 0.6401 - val_acc: 0.7909\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.3810 - acc: 0.8691 - val_loss: 0.7295 - val_acc: 0.7705\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3779 - acc: 0.8699 - val_loss: 0.7099 - val_acc: 0.7668\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3628 - acc: 0.8759 - val_loss: 0.6412 - val_acc: 0.7922\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.3609 - acc: 0.8768 - val_loss: 0.6878 - val_acc: 0.7743\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3547 - acc: 0.8800 - val_loss: 0.7549 - val_acc: 0.7607\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.3446 - acc: 0.8833 - val_loss: 0.7362 - val_acc: 0.7633\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3347 - acc: 0.8867 - val_loss: 0.6903 - val_acc: 0.7809\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.3245 - acc: 0.8884 - val_loss: 0.6830 - val_acc: 0.7810\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3238 - acc: 0.8903 - val_loss: 0.7159 - val_acc: 0.7694\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3171 - acc: 0.8917 - val_loss: 0.6452 - val_acc: 0.7936\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.3040 - acc: 0.8957 - val_loss: 0.7950 - val_acc: 0.7563\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2995 - acc: 0.8974 - val_loss: 0.7955 - val_acc: 0.7529\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2933 - acc: 0.9001 - val_loss: 0.7544 - val_acc: 0.7720\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2898 - acc: 0.9017 - val_loss: 0.6600 - val_acc: 0.7957\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.2748 - acc: 0.9060 - val_loss: 0.7207 - val_acc: 0.7745\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2754 - acc: 0.9061 - val_loss: 0.7147 - val_acc: 0.7829\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2689 - acc: 0.9093 - val_loss: 0.6977 - val_acc: 0.7910\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2661 - acc: 0.9090 - val_loss: 0.6843 - val_acc: 0.7916\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2542 - acc: 0.9147 - val_loss: 0.6714 - val_acc: 0.7947\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2524 - acc: 0.9133 - val_loss: 0.8062 - val_acc: 0.7626\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.2504 - acc: 0.9160 - val_loss: 0.8207 - val_acc: 0.7551\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2426 - acc: 0.9169 - val_loss: 0.7545 - val_acc: 0.7745\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2390 - acc: 0.9187 - val_loss: 0.7161 - val_acc: 0.7857\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2376 - acc: 0.9203 - val_loss: 0.7000 - val_acc: 0.7845\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2267 - acc: 0.9223 - val_loss: 0.7584 - val_acc: 0.7768\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2244 - acc: 0.9245 - val_loss: 0.7081 - val_acc: 0.7912\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.2253 - acc: 0.9233 - val_loss: 0.6776 - val_acc: 0.8007\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.2157 - acc: 0.9277 - val_loss: 0.8324 - val_acc: 0.7578\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2152 - acc: 0.9254 - val_loss: 0.6803 - val_acc: 0.8016\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2100 - acc: 0.9261 - val_loss: 0.6690 - val_acc: 0.7999\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2051 - acc: 0.9303 - val_loss: 0.7428 - val_acc: 0.7783\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.2025 - acc: 0.9309 - val_loss: 0.9532 - val_acc: 0.7433\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1961 - acc: 0.9327 - val_loss: 0.8043 - val_acc: 0.7759\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1906 - acc: 0.9361 - val_loss: 0.7316 - val_acc: 0.7897\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1915 - acc: 0.9344 - val_loss: 0.7715 - val_acc: 0.7801\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1871 - acc: 0.9373 - val_loss: 0.6856 - val_acc: 0.8044\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1812 - acc: 0.9390 - val_loss: 0.7529 - val_acc: 0.7870\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1779 - acc: 0.9387 - val_loss: 0.6865 - val_acc: 0.8020\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1732 - acc: 0.9401 - val_loss: 0.8164 - val_acc: 0.7772\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 40s 37ms/step - loss: 0.1732 - acc: 0.9418 - val_loss: 0.7257 - val_acc: 0.7985\n",
      "Epoch 86/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1670 - acc: 0.9425 - val_loss: 0.7455 - val_acc: 0.7979\n",
      "Epoch 87/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1666 - acc: 0.9429 - val_loss: 0.7404 - val_acc: 0.7902\n",
      "Epoch 88/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1634 - acc: 0.9452 - val_loss: 0.6968 - val_acc: 0.8029\n",
      "Epoch 89/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1629 - acc: 0.9441 - val_loss: 0.7302 - val_acc: 0.8025\n",
      "Epoch 90/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1648 - acc: 0.9443 - val_loss: 0.8645 - val_acc: 0.7694\n",
      "Epoch 91/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1551 - acc: 0.9481 - val_loss: 0.9367 - val_acc: 0.7553\n",
      "Epoch 92/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1527 - acc: 0.9488 - val_loss: 0.8042 - val_acc: 0.7835\n",
      "Epoch 93/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1539 - acc: 0.9475 - val_loss: 0.8052 - val_acc: 0.7824\n",
      "Epoch 94/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1502 - acc: 0.9495 - val_loss: 0.7893 - val_acc: 0.7838\n",
      "Epoch 95/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1478 - acc: 0.9495 - val_loss: 0.7883 - val_acc: 0.7860\n",
      "Epoch 96/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1391 - acc: 0.9531 - val_loss: 0.8019 - val_acc: 0.7871\n",
      "Epoch 97/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1459 - acc: 0.9500 - val_loss: 0.7477 - val_acc: 0.7987\n",
      "Epoch 98/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1435 - acc: 0.9506 - val_loss: 0.9130 - val_acc: 0.7541\n",
      "Epoch 99/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1374 - acc: 0.9535 - val_loss: 0.7768 - val_acc: 0.7930\n",
      "Epoch 100/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1360 - acc: 0.9539 - val_loss: 0.8152 - val_acc: 0.7811\n",
      "Epoch 101/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1430 - acc: 0.9504 - val_loss: 0.7812 - val_acc: 0.7888\n",
      "Epoch 102/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1301 - acc: 0.9572 - val_loss: 0.7299 - val_acc: 0.8038\n",
      "Epoch 103/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1286 - acc: 0.9567 - val_loss: 0.8957 - val_acc: 0.7822\n",
      "Epoch 104/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1312 - acc: 0.9557 - val_loss: 0.7460 - val_acc: 0.7990\n",
      "Epoch 105/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1293 - acc: 0.9566 - val_loss: 0.7860 - val_acc: 0.7976\n",
      "Epoch 106/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1278 - acc: 0.9565 - val_loss: 0.7923 - val_acc: 0.7948\n",
      "Epoch 107/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1236 - acc: 0.9587 - val_loss: 0.8225 - val_acc: 0.7842\n",
      "Epoch 108/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1187 - acc: 0.9603 - val_loss: 0.8230 - val_acc: 0.7920\n",
      "Epoch 109/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1210 - acc: 0.9586 - val_loss: 0.8059 - val_acc: 0.7879\n",
      "Epoch 110/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1256 - acc: 0.9577 - val_loss: 0.8058 - val_acc: 0.7954\n",
      "Epoch 111/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.1120 - acc: 0.9624 - val_loss: 0.8725 - val_acc: 0.7843\n",
      "Epoch 112/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1217 - acc: 0.9591 - val_loss: 0.9392 - val_acc: 0.7718\n",
      "Epoch 113/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1161 - acc: 0.9604 - val_loss: 0.9281 - val_acc: 0.7747\n",
      "Epoch 114/130\n",
      " 771/1093 [====================>.........] - ETA: 10s - loss: 0.1127 - acc: 0.9623"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.1032 - acc: 0.9644 - val_loss: 0.9380 - val_acc: 0.7843\n",
      "Epoch 127/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.0975 - acc: 0.9671 - val_loss: 0.8269 - val_acc: 0.7922\n",
      "Epoch 128/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.0987 - acc: 0.9664 - val_loss: 0.8456 - val_acc: 0.7935\n",
      "Epoch 129/130\n",
      "1093/1093 [==============================] - 41s 38ms/step - loss: 0.0982 - acc: 0.9686 - val_loss: 0.7546 - val_acc: 0.8123\n",
      "Epoch 130/130\n",
      "1093/1093 [==============================] - 41s 37ms/step - loss: 0.0945 - acc: 0.9688 - val_loss: 0.9988 - val_acc: 0.7662\n",
      "exe time:  5365.481000185013\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea16_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea16_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 99s 10ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7575436492246583, 0.8124]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del(test_model)\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea16_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_17 (Model)                multiple             14714688    input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_19 (Gl (None, 64)           0           model_17[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_20 (Gl (None, 128)          0           model_17[1][1]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_21 (Gl (None, 256)          0           model_17[1][2]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_22 (Gl (None, 512)          0           model_17[1][3]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_23 (Gl (None, 512)          0           model_17[1][4]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 10)           650         global_average_pooling2d_19[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 10)           1290        global_average_pooling2d_20[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 10)           2570        global_average_pooling2d_21[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 10)           5130        global_average_pooling2d_22[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 10)           5130        global_average_pooling2d_23[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 10)           40          dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 10)           40          dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 10)           40          dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 10)           40          dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 10)           40          dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 10)           0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 10)           0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 10)           0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 10)           0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 10)           0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_29 (Concatenate)    (None, 50)           0           activation_24[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "                                                                 activation_27[0][0]              \n",
      "                                                                 activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 10)           510         concatenate_29[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 10)           40          dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 10)           0           batch_normalization_29[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 14,730,208\n",
      "Trainable params: 15,400\n",
      "Non-trainable params: 14,714,808\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "del(model)\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "# vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "for layer in pretrain_vgg.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs)\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "\n",
    "concat_outputs = []\n",
    "\n",
    "for output_layer in vgg_outs:\n",
    "    partial_output = GlobalAveragePooling2D()(output_layer)\n",
    "    partial_output = Dense(10)(partial_output)\n",
    "    partial_output = BatchNormalization()(partial_output)\n",
    "    partial_output = Activation('softmax')(partial_output)\n",
    "    concat_outputs.append(partial_output)\n",
    "    \n",
    "final_output = concatenate(concat_outputs)\n",
    "final_output = Dense(10)(final_output)\n",
    "final_output = BatchNormalization()(final_output)\n",
    "final_output = Activation('softmax')(final_output)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=final_output)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(1e-4), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 19s 17ms/step - loss: 2.0892 - acc: 0.2649 - val_loss: 1.8107 - val_acc: 0.3631\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.7381 - acc: 0.3932 - val_loss: 1.6501 - val_acc: 0.4342\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.6406 - acc: 0.4336 - val_loss: 1.5801 - val_acc: 0.4621\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.5766 - acc: 0.4574 - val_loss: 1.5360 - val_acc: 0.4733\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.5296 - acc: 0.4762 - val_loss: 1.4981 - val_acc: 0.4848\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.4924 - acc: 0.4912 - val_loss: 1.4712 - val_acc: 0.4969\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.4630 - acc: 0.4959 - val_loss: 1.4163 - val_acc: 0.5172\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.4400 - acc: 0.5096 - val_loss: 1.3902 - val_acc: 0.5252\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.4147 - acc: 0.5135 - val_loss: 1.3756 - val_acc: 0.5287\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3959 - acc: 0.5213 - val_loss: 1.3577 - val_acc: 0.5344\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3788 - acc: 0.5263 - val_loss: 1.3693 - val_acc: 0.5299\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3598 - acc: 0.5331 - val_loss: 1.3519 - val_acc: 0.5355\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3506 - acc: 0.5354 - val_loss: 1.3289 - val_acc: 0.5389\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3420 - acc: 0.5351 - val_loss: 1.3014 - val_acc: 0.5541\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3194 - acc: 0.5441 - val_loss: 1.3003 - val_acc: 0.5548\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3113 - acc: 0.5469 - val_loss: 1.2703 - val_acc: 0.5597\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.3037 - acc: 0.5465 - val_loss: 1.3053 - val_acc: 0.5488\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.3018 - acc: 0.5469 - val_loss: 1.2894 - val_acc: 0.5593\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2911 - acc: 0.5507 - val_loss: 1.2570 - val_acc: 0.5683\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2765 - acc: 0.5598 - val_loss: 1.2696 - val_acc: 0.5606\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.2743 - acc: 0.5547 - val_loss: 1.2748 - val_acc: 0.5643\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2626 - acc: 0.5625 - val_loss: 1.2461 - val_acc: 0.5647\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2594 - acc: 0.5619 - val_loss: 1.2490 - val_acc: 0.5701\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2500 - acc: 0.5635 - val_loss: 1.2378 - val_acc: 0.5722\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2471 - acc: 0.5664 - val_loss: 1.2033 - val_acc: 0.5794\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2401 - acc: 0.5668 - val_loss: 1.2308 - val_acc: 0.5777\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2360 - acc: 0.5713 - val_loss: 1.2174 - val_acc: 0.5787\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2329 - acc: 0.5706 - val_loss: 1.2286 - val_acc: 0.5760\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.2325 - acc: 0.5697 - val_loss: 1.2458 - val_acc: 0.5685\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2201 - acc: 0.5763 - val_loss: 1.2331 - val_acc: 0.5753\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2203 - acc: 0.5728 - val_loss: 1.2065 - val_acc: 0.5817\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.2080 - acc: 0.5762 - val_loss: 1.1982 - val_acc: 0.5832\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2119 - acc: 0.5737 - val_loss: 1.1883 - val_acc: 0.5838\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2087 - acc: 0.5763 - val_loss: 1.1815 - val_acc: 0.5878\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2071 - acc: 0.5764 - val_loss: 1.2050 - val_acc: 0.5812\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.2006 - acc: 0.5777 - val_loss: 1.2064 - val_acc: 0.5866\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.2068 - acc: 0.5754 - val_loss: 1.2049 - val_acc: 0.5858\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1949 - acc: 0.5808 - val_loss: 1.1993 - val_acc: 0.5873\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1942 - acc: 0.5834 - val_loss: 1.1912 - val_acc: 0.5898\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1941 - acc: 0.5807 - val_loss: 1.1966 - val_acc: 0.5849\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1839 - acc: 0.5862 - val_loss: 1.2025 - val_acc: 0.5870\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1846 - acc: 0.5831 - val_loss: 1.1917 - val_acc: 0.5875\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1808 - acc: 0.5863 - val_loss: 1.1690 - val_acc: 0.5921\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1835 - acc: 0.5878 - val_loss: 1.1614 - val_acc: 0.5923\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1826 - acc: 0.5828 - val_loss: 1.1266 - val_acc: 0.6067\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1718 - acc: 0.5890 - val_loss: 1.1855 - val_acc: 0.5879\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1792 - acc: 0.5846 - val_loss: 1.1693 - val_acc: 0.5933\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1694 - acc: 0.5883 - val_loss: 1.1709 - val_acc: 0.5933\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1738 - acc: 0.5868 - val_loss: 1.1563 - val_acc: 0.5999\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1739 - acc: 0.5880 - val_loss: 1.1734 - val_acc: 0.5971\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1610 - acc: 0.5939 - val_loss: 1.1813 - val_acc: 0.5941\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1582 - acc: 0.5897 - val_loss: 1.1815 - val_acc: 0.5910\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1613 - acc: 0.5910 - val_loss: 1.1645 - val_acc: 0.5937\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.1647 - acc: 0.5891 - val_loss: 1.1674 - val_acc: 0.5979\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1577 - acc: 0.5937 - val_loss: 1.1874 - val_acc: 0.5909\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 16s 15ms/step - loss: 1.1592 - acc: 0.5919 - val_loss: 1.1719 - val_acc: 0.5950\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1589 - acc: 0.5901 - val_loss: 1.1723 - val_acc: 0.5931\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1467 - acc: 0.5959 - val_loss: 1.2128 - val_acc: 0.5834\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1522 - acc: 0.5927 - val_loss: 1.1417 - val_acc: 0.6034\n",
      "Epoch 60/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1538 - acc: 0.5953 - val_loss: 1.1277 - val_acc: 0.6047\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1497 - acc: 0.5945 - val_loss: 1.1817 - val_acc: 0.5895\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1507 - acc: 0.5907 - val_loss: 1.1328 - val_acc: 0.6037\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1534 - acc: 0.5970 - val_loss: 1.1520 - val_acc: 0.5965\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1365 - acc: 0.5976 - val_loss: 1.1624 - val_acc: 0.5943\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1443 - acc: 0.5983 - val_loss: 1.1799 - val_acc: 0.5913\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1447 - acc: 0.5963 - val_loss: 1.1733 - val_acc: 0.5946\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.1444 - acc: 0.5964 - val_loss: 1.1454 - val_acc: 0.6007\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1385 - acc: 0.5979 - val_loss: 1.1401 - val_acc: 0.5994\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1398 - acc: 0.5982 - val_loss: 1.1954 - val_acc: 0.5911\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1302 - acc: 0.6003 - val_loss: 1.1529 - val_acc: 0.5968\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1394 - acc: 0.5977 - val_loss: 1.1238 - val_acc: 0.6064\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1317 - acc: 0.6012 - val_loss: 1.1654 - val_acc: 0.5923\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1381 - acc: 0.5980 - val_loss: 1.1458 - val_acc: 0.6022\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1330 - acc: 0.6010 - val_loss: 1.1359 - val_acc: 0.6046\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1340 - acc: 0.5991 - val_loss: 1.1667 - val_acc: 0.5973\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1373 - acc: 0.5994 - val_loss: 1.1393 - val_acc: 0.6036\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1297 - acc: 0.6015 - val_loss: 1.1542 - val_acc: 0.6008\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1264 - acc: 0.6039 - val_loss: 1.1348 - val_acc: 0.6052\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1227 - acc: 0.6012 - val_loss: 1.1715 - val_acc: 0.5943\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1244 - acc: 0.6022 - val_loss: 1.1233 - val_acc: 0.6089\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1235 - acc: 0.6065 - val_loss: 1.1326 - val_acc: 0.6049\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1233 - acc: 0.6050 - val_loss: 1.1192 - val_acc: 0.6103\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1213 - acc: 0.6052 - val_loss: 1.1346 - val_acc: 0.6072\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1139 - acc: 0.6041 - val_loss: 1.1760 - val_acc: 0.5945\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.1217 - acc: 0.6038 - val_loss: 1.1139 - val_acc: 0.6138\n",
      "Epoch 86/130\n",
      " 406/1093 [==========>...................] - ETA: 7s - loss: 1.1061 - acc: 0.6114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0996 - acc: 0.6114 - val_loss: 1.1302 - val_acc: 0.6093\n",
      "Epoch 115/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0989 - acc: 0.6127 - val_loss: 1.1123 - val_acc: 0.6131\n",
      "Epoch 116/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0971 - acc: 0.6105 - val_loss: 1.1061 - val_acc: 0.6177\n",
      "Epoch 117/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.0971 - acc: 0.6130 - val_loss: 1.0900 - val_acc: 0.6203\n",
      "Epoch 118/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.1053 - acc: 0.6088 - val_loss: 1.1097 - val_acc: 0.6116\n",
      "Epoch 119/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.0925 - acc: 0.6157 - val_loss: 1.1436 - val_acc: 0.6037\n",
      "Epoch 120/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0983 - acc: 0.6108 - val_loss: 1.1228 - val_acc: 0.6106\n",
      "Epoch 121/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0967 - acc: 0.6098 - val_loss: 1.1337 - val_acc: 0.6086\n",
      "Epoch 122/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0939 - acc: 0.6126 - val_loss: 1.1534 - val_acc: 0.6041\n",
      "Epoch 123/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0960 - acc: 0.6122 - val_loss: 1.1118 - val_acc: 0.6141\n",
      "Epoch 124/130\n",
      "1093/1093 [==============================] - 15s 13ms/step - loss: 1.0927 - acc: 0.6152 - val_loss: 1.1179 - val_acc: 0.6128\n",
      "Epoch 125/130\n",
      "1093/1093 [==============================] - 15s 14ms/step - loss: 1.0917 - acc: 0.6143 - val_loss: 1.1130 - val_acc: 0.6130\n",
      "Epoch 126/130\n",
      "1093/1093 [==============================] - 14s 13ms/step - loss: 1.0928 - acc: 0.6149 - val_loss: 1.1155 - val_acc: 0.6133\n",
      "Epoch 127/130\n",
      " 855/1093 [======================>.......] - ETA: 2s - loss: 1.1000 - acc: 0.6109"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea17_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea17_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 91s 9ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0767300821033248, 0.6258]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del(test_model)\n",
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea17_model.hdf5')\n",
    "test_generator.reset()\n",
    "\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_3/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_3/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_3:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_2/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_3/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_3/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 multiple             260160      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_3[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    1475072     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_5 (Glo (None, 704)          0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           7050        global_average_pooling2d_5[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 6,803,506\n",
      "Trainable params: 6,799,646\n",
      "Non-trainable params: 3,860\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_74 (None, None, None, 3)\n",
      "block1_conv1 (None, None, None, 64)\n",
      "block1_conv2 (None, None, None, 64)\n",
      "block1_pool (None, None, None, 64)\n",
      "block2_conv1 (None, None, None, 128)\n",
      "block2_conv2 (None, None, None, 128)\n",
      "block2_pool (None, None, None, 128)\n",
      "block3_conv1 (None, None, None, 256)\n",
      "block3_conv2 (None, None, None, 256)\n",
      "block3_conv3 (None, None, None, 256)\n",
      "block3_pool (None, None, None, 256)\n",
      "block4_conv1 (None, None, None, 512)\n",
      "block4_conv2 (None, None, None, 512)\n",
      "block4_conv3 (None, None, None, 512)\n",
      "block4_pool (None, None, None, 512)\n",
      "block5_conv1 (None, None, None, 512)\n",
      "block5_conv2 (None, None, None, 512)\n",
      "block5_conv3 (None, None, None, 512)\n",
      "block5_pool (None, None, None, 512)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "    \n",
    "vgg_input, vgg_outputs = vgg_extend(input_layer, weights=[k.get_weights() for k in pretrain_vgg.layers], vgg_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = pretrain_vgg.layers[3]\n",
    "# b = Conv2D(128,(3,3), padding='same')(b)\n",
    "b.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_extend(input_layer, weights, num_blocks, extend_inputs):\n",
    "    neurons = [64,128,256,512]\n",
    "    counter = 1\n",
    "    b = input_layer\n",
    "    inputs = b\n",
    "    outputs = []\n",
    "    for block in num_blocks:\n",
    "        b =  Conv2D(neurons[block], kernel_size=(3,3), strides=1, padding='same', name='conv' + str(i) + '_block' + str(block))(b)\n",
    "        b.set_weights(weights[counter])\n",
    "        counter += 1\n",
    "        \n",
    "        b =  Conv2D(neurons[block], kernel_size=(3,3), strides=1, padding='same', name='conv' + str(i) + '_block' + str(block))(b)\n",
    "        b.set_weights(weights[counter])\n",
    "        counter += 1\n",
    "        \n",
    "        b = MaxPooling2D(pool_size=(2,2), name='maxpooling_block' + str(block))(b)\n",
    "        b.set_weights(weights[counter])\n",
    "        counter += 1\n",
    "        \n",
    "        outputs.append(b)\n",
    "        if exist(extend_inputs[block-1]):\n",
    "            b = concatenate([extend_inputs[block-1], b])\n",
    "        else:\n",
    "            print('Not exist input at ', block-1)\n",
    "        \n",
    "        \n",
    "    return inputs, outputs \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block3_conv1\n",
      "block3_conv2\n",
      "block3_conv3\n",
      "block3_pool\n",
      "block4_conv1\n",
      "block4_conv2\n",
      "block4_conv3\n",
      "block4_pool\n",
      "block5_conv1\n",
      "block5_conv2\n",
      "block5_conv3\n",
      "block5_pool\n"
     ]
    }
   ],
   "source": [
    "pretrain_layers = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "for layer in pretrain_vgg.layers[7:]:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def SkippedVGG_for_transfer_2(nb_blocks, input_layer, nb_layers, nb_neurons, extra_inputs, verbose=1):\n",
    "    concats = []\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        concats.append([])\n",
    "        \n",
    "    if type(nb_layers) is int:\n",
    "        temp = nb_layers\n",
    "        nb_layers = []\n",
    "        for i in range(nb_blocks):\n",
    "            nb_layers.append(temp)\n",
    "    \n",
    "    if verbose:\n",
    "        print('Create DensedVGG model with ' + str(nb_blocks) + ' blocks, input layer = ', input_layer)\n",
    "    \n",
    "    for i in range(nb_blocks):\n",
    "        if verbose:\n",
    "            print('Create block ' + str(i) + ':')\n",
    "        \n",
    "        if i == 0: # First block\n",
    "            b = input_layer\n",
    "            inputs = b\n",
    "        \n",
    "        # check all layers before\n",
    "        if len(concats[i-1]) > 1:\n",
    "            if (i-1) < len(extra_inputs):\n",
    "                print('Add extra inputs ', extra_inputs[i-1])\n",
    "                temp_concats = concats[i-1]\n",
    "                temp_concats.append(extra_inputs[i-1])\n",
    "                b = concatenate(temp_concats)\n",
    "                \n",
    "                # Go through 1x1 Conv2D\n",
    "                b = Conv2D(nb_neurons[i], (1,1), strides=1, padding='same')(b)\n",
    "                \n",
    "            else:   \n",
    "                print('No extra inputs at block ', i)\n",
    "                b = concatenate(concats[i-1])\n",
    "        elif len(concats[i-1]) == 1:\n",
    "            if verbose:\n",
    "                print('Get direct output from the previous block ', concats[i-1])\n",
    "#             b = concats[i-1][0]\n",
    "            if (i-1) < len(extra_inputs):\n",
    "                print('Add extra inputs ', extra_inputs[i-1])\n",
    "                temp_concats = concats[i-1]\n",
    "                temp_concats.append(extra_inputs[i-1])\n",
    "                b = concatenate(temp_concats)\n",
    "                b = Conv2D(nb_neurons[i], (1,1), strides=1, padding='same')(b)\n",
    "                \n",
    "            else:   \n",
    "                print('No extra inputs at block ', i)\n",
    "                b = concatenate(concats[i-1][0])\n",
    "            \n",
    "        # create main block\n",
    "        b = define_block(b, nb_layers[i], nb_neurons[i], index=i)\n",
    "        concats[i].append(b)\n",
    "        \n",
    "        \n",
    "        # create cropping layer\n",
    "        for j in range(i+1, nb_blocks):\n",
    "            if verbose:\n",
    "                print('-- create skipped connection from block '+ str(i) + ' to block ' + str(j+1) + ' ...')\n",
    "                \n",
    "            src_shape = b.get_shape()\n",
    "            src_shape = (int(src_shape[1]), int(src_shape[2]), int(src_shape[3]))\n",
    "            dst_shape = (src_shape[0]//2**(j-i), src_shape[1]//2**(j-i), src_shape[2])\n",
    "            \n",
    "            print(src_shape, dst_shape)\n",
    "            h = src_shape[0] - dst_shape[0]\n",
    "            w = src_shape[1] - dst_shape[1]\n",
    "            if h % 2 == 0:\n",
    "                h_0 = h // 2\n",
    "                h_1 = h // 2\n",
    "            else:\n",
    "                h_0 = h // 2\n",
    "                h_1 = (h // 2) + 1\n",
    "            \n",
    "            if w % 2 == 0:\n",
    "                w_0 = w // 2\n",
    "                w_1 = w // 2\n",
    "            else:\n",
    "                w_0 = w // 2\n",
    "                w_1 = (w // 2) + 1\n",
    "                \n",
    "            concat_layer = Cropping2D(((h_0,h_1),(w_0,w_1)), name='cropping_block' + str(i) + '_block' + str(j+1))(b)\n",
    "            concats[j].append(concat_layer)\n",
    "            \n",
    "    if len(concats[nb_blocks-1]) == 1:\n",
    "        outputs = concats[nb_blocks-1]\n",
    "    else:\n",
    "        outputs = concatenate(concats[nb_blocks-1])\n",
    "        \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'model_3/block1_pool/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>, <tf.Tensor 'model_3/block2_pool/MaxPool:0' shape=(?, 8, 8, 128) dtype=float32>]\n",
      "Create DensedVGG model with 3 blocks, input layer =  Tensor(\"input_3:0\", shape=(?, 32, 32, 3), dtype=float32)\n",
      "Create block 0:\n",
      "-- create skipped connection from block 0 to block 2 ...\n",
      "(16, 16, 64) (8, 8, 64)\n",
      "-- create skipped connection from block 0 to block 3 ...\n",
      "(16, 16, 64) (4, 4, 64)\n",
      "Create block 1:\n",
      "Get direct output from the previous block  [<tf.Tensor 'maxpooling_block0_1/MaxPool:0' shape=(?, 16, 16, 64) dtype=float32>]\n",
      "Add extra inputs  Tensor(\"model_3/block1_pool/MaxPool:0\", shape=(?, 16, 16, 64), dtype=float32)\n",
      "-- create skipped connection from block 1 to block 3 ...\n",
      "(8, 8, 128) (4, 4, 128)\n",
      "Create block 2:\n",
      "Add extra inputs  Tensor(\"model_3/block2_pool/MaxPool:0\", shape=(?, 8, 8, 128), dtype=float32)\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block0 (Conv2D)           (None, 32, 32, 64)   1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block0 (BatchNormali (None, 32, 32, 64)   256         conv0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm0_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block0 (Conv2D)           (None, 32, 32, 64)   36928       relu0_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block0 (BatchNormali (None, 32, 32, 64)   256         conv1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block0 (Activation)       (None, 32, 32, 64)   0           batchnorm1_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block0 (MaxPooling2D (None, 16, 16, 64)   0           relu1_block0[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "model_3 (Model)                 multiple             260160      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 16, 16, 128)  0           maxpooling_block0[0][0]          \n",
      "                                                                 model_3[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 16, 16, 128)  16512       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block1 (Conv2D)           (None, 16, 16, 128)  147584      conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block1 (BatchNormali (None, 16, 16, 128)  512         conv0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm0_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block1 (Conv2D)           (None, 16, 16, 128)  147584      relu0_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block1 (BatchNormali (None, 16, 16, 128)  512         conv1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block1 (Activation)       (None, 16, 16, 128)  0           batchnorm1_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block1 (MaxPooling2D (None, 8, 8, 128)    0           relu1_block1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block2 (Croppin (None, 8, 8, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 320)    0           cropping_block0_block2[0][0]     \n",
      "                                                                 maxpooling_block1[0][0]          \n",
      "                                                                 model_3[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 512)    164352      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv0_block2 (Conv2D)           (None, 8, 8, 512)    2359808     conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm0_block2 (BatchNormali (None, 8, 8, 512)    2048        conv0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu0_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm0_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu0_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm1_block2 (BatchNormali (None, 8, 8, 512)    2048        conv1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu1_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm1_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2 (Conv2D)           (None, 8, 8, 512)    2359808     relu1_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batchnorm2_block2 (BatchNormali (None, 8, 8, 512)    2048        conv2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "relu2_block2 (Activation)       (None, 8, 8, 512)    0           batchnorm2_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block0_block3 (Croppin (None, 4, 4, 64)     0           maxpooling_block0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "cropping_block1_block3 (Croppin (None, 4, 4, 128)    0           maxpooling_block1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "maxpooling_block2 (MaxPooling2D (None, 4, 4, 512)    0           relu2_block2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 4, 4, 704)    0           cropping_block0_block3[0][0]     \n",
      "                                                                 cropping_block1_block3[0][0]     \n",
      "                                                                 maxpooling_block2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 704)          0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           7050        global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           batch_normalization_2[0][0]      \n",
      "==================================================================================================\n",
      "Total params: 7,869,106\n",
      "Trainable params: 7,865,246\n",
      "Non-trainable params: 3,860\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "input_layer = Input((32,32,3))\n",
    "conv_indices=[3,6,10,14,18]\n",
    "vgg_gaps = []\n",
    "\n",
    "pretrain_vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "vgg_conv_outputs = []\n",
    "for i in conv_indices:\n",
    "    vgg_conv_outputs.append(pretrain_vgg.layers[i].output)\n",
    "\n",
    "vgg = Model(inputs=pretrain_vgg.input, outputs=vgg_conv_outputs[:2])\n",
    "\n",
    "vgg_outs = vgg(input_layer)\n",
    "print(vgg_outs)\n",
    "\n",
    "# for layer in vgg_outs:\n",
    "#     vgg_gaps.append(GlobalAveragePooling2D()(layer))\n",
    "\n",
    "skippedvgg_out = SkippedVGG_for_transfer_2(nb_blocks=3, input_layer=input_layer, \n",
    "                            nb_layers=[2,2,3], nb_neurons=[64,128,512], extra_inputs=vgg_outs)\n",
    "\n",
    "# vgg_concats = GlobalAveragePooling2D()(vgg_outs[3:])\n",
    "\n",
    "skippedvgg_out = GlobalAveragePooling2D()(skippedvgg_out)\n",
    "\n",
    "# outputs = concatenate([vgg_concats, skippedvgg_out])\n",
    "outputs = Dense(10)(skippedvgg_out)\n",
    "outputs = BatchNormalization()(outputs)\n",
    "outputs = Activation('softmax')(outputs)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=outputs)\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1016 09:54:47.163458 140057288775424 deprecation.py:323] From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/130\n",
      "1093/1093 [==============================] - 32s 29ms/step - loss: 1.6365 - acc: 0.4170 - val_loss: 1.7200 - val_acc: 0.4073\n",
      "Epoch 2/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.3994 - acc: 0.5093 - val_loss: 1.7279 - val_acc: 0.4278\n",
      "Epoch 3/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.2900 - acc: 0.5493 - val_loss: 1.3876 - val_acc: 0.5018\n",
      "Epoch 4/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.2172 - acc: 0.5731 - val_loss: 1.3081 - val_acc: 0.5485\n",
      "Epoch 5/130\n",
      "1093/1093 [==============================] - 28s 25ms/step - loss: 1.1510 - acc: 0.5963 - val_loss: 1.3132 - val_acc: 0.5494\n",
      "Epoch 6/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.1033 - acc: 0.6130 - val_loss: 1.2658 - val_acc: 0.5764\n",
      "Epoch 7/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.0544 - acc: 0.6309 - val_loss: 1.3161 - val_acc: 0.5596\n",
      "Epoch 8/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 1.0159 - acc: 0.6428 - val_loss: 1.0513 - val_acc: 0.6359\n",
      "Epoch 9/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.9785 - acc: 0.6635 - val_loss: 0.9926 - val_acc: 0.6585\n",
      "Epoch 10/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.9491 - acc: 0.6716 - val_loss: 1.2227 - val_acc: 0.6046\n",
      "Epoch 11/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.9164 - acc: 0.6781 - val_loss: 1.0988 - val_acc: 0.6266\n",
      "Epoch 12/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.8802 - acc: 0.6934 - val_loss: 0.9493 - val_acc: 0.6756\n",
      "Epoch 13/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.8609 - acc: 0.6998 - val_loss: 0.8946 - val_acc: 0.6954\n",
      "Epoch 14/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.8328 - acc: 0.7095 - val_loss: 0.9128 - val_acc: 0.6859\n",
      "Epoch 15/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.8126 - acc: 0.7167 - val_loss: 1.0444 - val_acc: 0.6552\n",
      "Epoch 16/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7989 - acc: 0.7214 - val_loss: 0.9986 - val_acc: 0.6633\n",
      "Epoch 17/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7677 - acc: 0.7335 - val_loss: 1.0179 - val_acc: 0.6575\n",
      "Epoch 18/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7556 - acc: 0.7389 - val_loss: 0.9325 - val_acc: 0.6899\n",
      "Epoch 19/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7363 - acc: 0.7449 - val_loss: 0.9847 - val_acc: 0.6581\n",
      "Epoch 20/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7177 - acc: 0.7531 - val_loss: 0.8707 - val_acc: 0.7064\n",
      "Epoch 21/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.7004 - acc: 0.7577 - val_loss: 0.7948 - val_acc: 0.7231\n",
      "Epoch 22/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6861 - acc: 0.7620 - val_loss: 0.8962 - val_acc: 0.6985\n",
      "Epoch 23/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6703 - acc: 0.7662 - val_loss: 0.8284 - val_acc: 0.7276\n",
      "Epoch 24/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6642 - acc: 0.7688 - val_loss: 0.7715 - val_acc: 0.7408\n",
      "Epoch 25/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6382 - acc: 0.7784 - val_loss: 0.7959 - val_acc: 0.7285\n",
      "Epoch 26/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6241 - acc: 0.7845 - val_loss: 0.7356 - val_acc: 0.7474\n",
      "Epoch 27/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.6116 - acc: 0.7873 - val_loss: 0.6869 - val_acc: 0.7682\n",
      "Epoch 28/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5879 - acc: 0.7951 - val_loss: 0.7746 - val_acc: 0.7425\n",
      "Epoch 29/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5782 - acc: 0.8002 - val_loss: 0.8602 - val_acc: 0.7100\n",
      "Epoch 30/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5697 - acc: 0.8038 - val_loss: 0.9660 - val_acc: 0.6783\n",
      "Epoch 31/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5577 - acc: 0.8067 - val_loss: 0.8440 - val_acc: 0.7282\n",
      "Epoch 32/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5457 - acc: 0.8121 - val_loss: 0.8795 - val_acc: 0.7156\n",
      "Epoch 33/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5327 - acc: 0.8179 - val_loss: 0.7604 - val_acc: 0.7521\n",
      "Epoch 34/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5168 - acc: 0.8212 - val_loss: 0.8491 - val_acc: 0.7250\n",
      "Epoch 35/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5090 - acc: 0.8257 - val_loss: 0.8330 - val_acc: 0.7274\n",
      "Epoch 36/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.5012 - acc: 0.8274 - val_loss: 0.7542 - val_acc: 0.7504\n",
      "Epoch 37/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4858 - acc: 0.8319 - val_loss: 0.6955 - val_acc: 0.7699\n",
      "Epoch 38/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4756 - acc: 0.8350 - val_loss: 0.6977 - val_acc: 0.7744\n",
      "Epoch 39/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4587 - acc: 0.8429 - val_loss: 0.7221 - val_acc: 0.7661\n",
      "Epoch 40/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4497 - acc: 0.8481 - val_loss: 0.7421 - val_acc: 0.7531\n",
      "Epoch 41/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4398 - acc: 0.8502 - val_loss: 0.7455 - val_acc: 0.7581\n",
      "Epoch 42/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4295 - acc: 0.8508 - val_loss: 0.6920 - val_acc: 0.7775\n",
      "Epoch 43/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4218 - acc: 0.8553 - val_loss: 0.6715 - val_acc: 0.7819\n",
      "Epoch 44/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.4114 - acc: 0.8582 - val_loss: 0.7021 - val_acc: 0.7720\n",
      "Epoch 45/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3944 - acc: 0.8643 - val_loss: 0.7359 - val_acc: 0.7639\n",
      "Epoch 46/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3955 - acc: 0.8619 - val_loss: 0.6892 - val_acc: 0.7751\n",
      "Epoch 47/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3845 - acc: 0.8677 - val_loss: 0.7322 - val_acc: 0.7656\n",
      "Epoch 48/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3786 - acc: 0.8708 - val_loss: 0.6508 - val_acc: 0.7909\n",
      "Epoch 49/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3661 - acc: 0.8731 - val_loss: 0.7518 - val_acc: 0.7642\n",
      "Epoch 50/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3617 - acc: 0.8751 - val_loss: 0.6229 - val_acc: 0.8004\n",
      "Epoch 51/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3441 - acc: 0.8823 - val_loss: 0.6522 - val_acc: 0.7936\n",
      "Epoch 52/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3449 - acc: 0.8809 - val_loss: 0.6506 - val_acc: 0.7877\n",
      "Epoch 53/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3391 - acc: 0.8821 - val_loss: 0.6813 - val_acc: 0.7799\n",
      "Epoch 54/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3288 - acc: 0.8874 - val_loss: 0.7303 - val_acc: 0.7775\n",
      "Epoch 55/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3289 - acc: 0.8867 - val_loss: 0.7256 - val_acc: 0.7747\n",
      "Epoch 56/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3171 - acc: 0.8905 - val_loss: 0.7302 - val_acc: 0.7700\n",
      "Epoch 57/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3068 - acc: 0.8956 - val_loss: 0.6637 - val_acc: 0.7936\n",
      "Epoch 58/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.3025 - acc: 0.8945 - val_loss: 0.6752 - val_acc: 0.7944\n",
      "Epoch 59/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2949 - acc: 0.8996 - val_loss: 0.7167 - val_acc: 0.7815\n",
      "Epoch 60/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2899 - acc: 0.9002 - val_loss: 0.7696 - val_acc: 0.7638\n",
      "Epoch 61/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2841 - acc: 0.9021 - val_loss: 0.7051 - val_acc: 0.7834\n",
      "Epoch 62/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2703 - acc: 0.9071 - val_loss: 0.6950 - val_acc: 0.7790\n",
      "Epoch 63/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2672 - acc: 0.9085 - val_loss: 0.7214 - val_acc: 0.7877\n",
      "Epoch 64/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2627 - acc: 0.9101 - val_loss: 0.7160 - val_acc: 0.7871\n",
      "Epoch 65/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2618 - acc: 0.9102 - val_loss: 0.6579 - val_acc: 0.7960\n",
      "Epoch 66/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2536 - acc: 0.9124 - val_loss: 0.7435 - val_acc: 0.7795\n",
      "Epoch 67/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2528 - acc: 0.9132 - val_loss: 0.8175 - val_acc: 0.7600\n",
      "Epoch 68/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2399 - acc: 0.9180 - val_loss: 0.6908 - val_acc: 0.7986\n",
      "Epoch 69/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2398 - acc: 0.9180 - val_loss: 0.7405 - val_acc: 0.7850\n",
      "Epoch 70/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2408 - acc: 0.9199 - val_loss: 0.9573 - val_acc: 0.7365\n",
      "Epoch 71/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2345 - acc: 0.9181 - val_loss: 0.8538 - val_acc: 0.7506\n",
      "Epoch 72/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2229 - acc: 0.9250 - val_loss: 0.7886 - val_acc: 0.7667\n",
      "Epoch 73/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2231 - acc: 0.9219 - val_loss: 0.7424 - val_acc: 0.7879\n",
      "Epoch 74/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2184 - acc: 0.9248 - val_loss: 0.6611 - val_acc: 0.8055\n",
      "Epoch 75/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2123 - acc: 0.9278 - val_loss: 0.8065 - val_acc: 0.7774\n",
      "Epoch 76/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2136 - acc: 0.9267 - val_loss: 0.7253 - val_acc: 0.7879\n",
      "Epoch 77/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2069 - acc: 0.9291 - val_loss: 0.7554 - val_acc: 0.7843\n",
      "Epoch 78/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.2016 - acc: 0.9309 - val_loss: 0.7339 - val_acc: 0.7870\n",
      "Epoch 79/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1994 - acc: 0.9329 - val_loss: 0.8023 - val_acc: 0.7763\n",
      "Epoch 80/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1958 - acc: 0.9346 - val_loss: 0.7187 - val_acc: 0.7945\n",
      "Epoch 81/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1976 - acc: 0.9326 - val_loss: 0.7347 - val_acc: 0.7948\n",
      "Epoch 82/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1925 - acc: 0.9347 - val_loss: 0.7642 - val_acc: 0.7839\n",
      "Epoch 83/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1886 - acc: 0.9352 - val_loss: 0.7668 - val_acc: 0.7856\n",
      "Epoch 84/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1804 - acc: 0.9396 - val_loss: 0.7755 - val_acc: 0.7811\n",
      "Epoch 85/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1770 - acc: 0.9398 - val_loss: 0.8218 - val_acc: 0.7773\n",
      "Epoch 86/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1845 - acc: 0.9383 - val_loss: 0.7004 - val_acc: 0.7995\n",
      "Epoch 87/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1764 - acc: 0.9393 - val_loss: 0.7545 - val_acc: 0.7913\n",
      "Epoch 88/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1738 - acc: 0.9399 - val_loss: 0.7669 - val_acc: 0.7808\n",
      "Epoch 89/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1723 - acc: 0.9410 - val_loss: 0.7928 - val_acc: 0.7788\n",
      "Epoch 90/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1659 - acc: 0.9440 - val_loss: 0.7588 - val_acc: 0.7941\n",
      "Epoch 91/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1651 - acc: 0.9448 - val_loss: 0.7291 - val_acc: 0.7986\n",
      "Epoch 92/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1646 - acc: 0.9433 - val_loss: 0.7203 - val_acc: 0.7950\n",
      "Epoch 93/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1591 - acc: 0.9464 - val_loss: 0.7860 - val_acc: 0.7819\n",
      "Epoch 94/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1591 - acc: 0.9454 - val_loss: 0.8080 - val_acc: 0.7787\n",
      "Epoch 95/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1557 - acc: 0.9477 - val_loss: 0.7323 - val_acc: 0.7916\n",
      "Epoch 96/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1537 - acc: 0.9478 - val_loss: 0.7392 - val_acc: 0.7950\n",
      "Epoch 97/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1582 - acc: 0.9458 - val_loss: 0.7342 - val_acc: 0.7958\n",
      "Epoch 98/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1510 - acc: 0.9489 - val_loss: 0.8132 - val_acc: 0.7799\n",
      "Epoch 99/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1508 - acc: 0.9489 - val_loss: 0.7301 - val_acc: 0.7994\n",
      "Epoch 100/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1435 - acc: 0.9518 - val_loss: 0.8453 - val_acc: 0.7789\n",
      "Epoch 101/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1423 - acc: 0.9524 - val_loss: 0.7736 - val_acc: 0.7960\n",
      "Epoch 102/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1445 - acc: 0.9504 - val_loss: 0.7719 - val_acc: 0.7892\n",
      "Epoch 103/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1385 - acc: 0.9529 - val_loss: 0.8073 - val_acc: 0.7885\n",
      "Epoch 104/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1428 - acc: 0.9532 - val_loss: 0.8006 - val_acc: 0.7843\n",
      "Epoch 105/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1383 - acc: 0.9529 - val_loss: 0.8372 - val_acc: 0.7861\n",
      "Epoch 106/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1324 - acc: 0.9547 - val_loss: 0.7591 - val_acc: 0.7997\n",
      "Epoch 107/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1322 - acc: 0.9553 - val_loss: 0.8271 - val_acc: 0.7835\n",
      "Epoch 108/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1316 - acc: 0.9557 - val_loss: 0.7409 - val_acc: 0.8004\n",
      "Epoch 109/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1300 - acc: 0.9556 - val_loss: 0.7468 - val_acc: 0.8039\n",
      "Epoch 110/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1304 - acc: 0.9555 - val_loss: 0.7821 - val_acc: 0.7930\n",
      "Epoch 111/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1246 - acc: 0.9568 - val_loss: 0.8060 - val_acc: 0.7907\n",
      "Epoch 112/130\n",
      "1093/1093 [==============================] - 29s 26ms/step - loss: 0.1269 - acc: 0.9570 - val_loss: 0.7531 - val_acc: 0.7974\n",
      "Epoch 113/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1253 - acc: 0.9572 - val_loss: 0.8126 - val_acc: 0.7877\n",
      "Epoch 114/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1198 - acc: 0.9581 - val_loss: 0.7700 - val_acc: 0.7981\n",
      "Epoch 115/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1221 - acc: 0.9588 - val_loss: 0.7741 - val_acc: 0.7978\n",
      "Epoch 116/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1234 - acc: 0.9568 - val_loss: 0.8708 - val_acc: 0.7814\n",
      "Epoch 117/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1166 - acc: 0.9614 - val_loss: 0.7720 - val_acc: 0.8003\n",
      "Epoch 118/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1196 - acc: 0.9589 - val_loss: 0.7933 - val_acc: 0.7936\n",
      "Epoch 119/130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1159 - acc: 0.9593 - val_loss: 0.7363 - val_acc: 0.8064\n",
      "Epoch 120/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1166 - acc: 0.9605 - val_loss: 0.7822 - val_acc: 0.7964\n",
      "Epoch 121/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1101 - acc: 0.9630 - val_loss: 0.8574 - val_acc: 0.7859\n",
      "Epoch 122/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1116 - acc: 0.9617 - val_loss: 0.7890 - val_acc: 0.7988\n",
      "Epoch 123/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1091 - acc: 0.9631 - val_loss: 0.9015 - val_acc: 0.7683\n",
      "Epoch 124/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1103 - acc: 0.9620 - val_loss: 0.8283 - val_acc: 0.7888\n",
      "Epoch 125/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1053 - acc: 0.9640 - val_loss: 0.8336 - val_acc: 0.7898\n",
      "Epoch 126/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1120 - acc: 0.9635 - val_loss: 0.8606 - val_acc: 0.7773\n",
      "Epoch 127/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1076 - acc: 0.9633 - val_loss: 0.7620 - val_acc: 0.8014\n",
      "Epoch 128/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1062 - acc: 0.9642 - val_loss: 0.8740 - val_acc: 0.7889\n",
      "Epoch 129/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1018 - acc: 0.9652 - val_loss: 0.8359 - val_acc: 0.7957\n",
      "Epoch 130/130\n",
      "1093/1093 [==============================] - 28s 26ms/step - loss: 0.1071 - acc: 0.9635 - val_loss: 0.8248 - val_acc: 0.7980\n",
      "exe time:  3663.901920080185\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "epochs = 130\n",
    "\n",
    "train_generator.reset()\n",
    "validation_generator.reset()\n",
    "test_generator.reset()\n",
    "\n",
    "mcp = ModelCheckpoint('/data/Quan/datasets/cifar10/transfer_idea18_model.hdf5', monitor='val_acc', \n",
    "                      save_best_only=True, save_weights_only=False)\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit_generator(train_generator, steps_per_epoch=len(train_images)//batch_size, \n",
    "                            epochs=epochs, verbose=1, validation_data=validation_generator, \n",
    "                              validation_steps=len(validation_images)//batch_size, callbacks=[mcp])\n",
    "\n",
    "exe_time = time.time() - start\n",
    "\n",
    "print('exe time: ', exe_time)\n",
    "with open('/data/Quan/datasets/cifar10/transfer_idea18_history.hdf5', 'wb') as dt:\n",
    "    pickle.dump(history.history, dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 70s 7ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7753325680182985, 0.8039]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = load_model('/data/Quan/datasets/cifar10/transfer_idea18_model.hdf5')\n",
    "score = test_model.evaluate_generator(test_generator, steps=len(test_images), verbose=1)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
